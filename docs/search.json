[
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "STA 9890 Project: Property Valuation",
    "section": "",
    "text": "Property assessment values directly influence individual tax obligations, urban development decisions, and housing affordability analyses. However, public datasets are often noisy and incomplete—featuring missing renovation records, outdated area measurements, or abrupt shifts in land valuation. Therefore, a high-performing ML model must not only minimize prediction error but also provide interpretable insights that help stakeholders understand, trust, and audit the predictions—especially when such predictions may inform public policy or fiscal planning."
  },
  {
    "objectID": "project3.html#why-prediction-accuracy-and-interpretability-matter",
    "href": "project3.html#why-prediction-accuracy-and-interpretability-matter",
    "title": "STA 9890 Project: Property Valuation",
    "section": "",
    "text": "Property assessment values directly influence individual tax obligations, urban development decisions, and housing affordability analyses. However, public datasets are often noisy and incomplete—featuring missing renovation records, outdated area measurements, or abrupt shifts in land valuation. Therefore, a high-performing ML model must not only minimize prediction error but also provide interpretable insights that help stakeholders understand, trust, and audit the predictions—especially when such predictions may inform public policy or fiscal planning."
  },
  {
    "objectID": "project3.html#logic-driven-missing-value-handling-and-imputation",
    "href": "project3.html#logic-driven-missing-value-handling-and-imputation",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.1 Logic-Driven Missing Value Handling and Imputation",
    "text": "3.1 Logic-Driven Missing Value Handling and Imputation\nBackfilling Year-Based Columns Across Feature Groups: To handle missing values in temporally structured features (e.g., building area, quality, full bath), we designed a consistent backfilling approach that uses older data to impute more recent years. Specifically, columns were ordered from newest to oldest (i.e., 2019, 2018, …, 2015), and we applied bfill(axis=1) across these columns. This setup causes older values (e.g., from 2015 or 2016) to be used to fill in newer year columns (e.g., 2018 or 2019), effectively implementing a forward fill in temporal logic. This approach assumes that older data reflects the property’s original state more accurately and helps prevent later-year anomalies or missing values from distorting long-term trends.\nThis logic was applied across multiple feature groups: - Residential count features: floors, full bath, half bath, bedrooms, total rooms - Area-based features: building area, land area - Valuation features: building value, land value, assessed - Categorical building attributes: foundation type, grade, building condition, quality, quality description, physical condition, exterior walls, has cooling, has heat\n\n\nCode\ndef backfill_categorical_year_features(df, features, years):\n    for feature in features:\n        year_cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n        if len(year_cols) &gt;= 2:\n            df[year_cols] = df[year_cols].bfill(axis=1)\n            print(f\" Backfilled: {feature} across {year_cols}\")\n        else:\n            print(f\" Skipped: {feature} — not enough year-based columns.\")\n    return df\n\n# Backfill from most recent year to oldest\nyears = ['2019', '2018', '2017', '2016', '2015']\nfeatures = ['building_condition', 'foundation_type', 'grade', 'has_cooling', \n            'has_heat', 'physical_condition', 'exterior_walls']\n\n# Apply to train and test\ntrain_merged = backfill_categorical_year_features(train_merged, features, years)\ntest_merged = backfill_categorical_year_features(test_merged, features, years)\n\n\nZero-Aware Property Filtering: In cases where floor_area_total_2019 = 0, we treated the property as non-residential or commercial and applied domain-specific logic to avoid inappropriate imputations or distortions in the modeling process: - All related residential building features—such as full bath, total rooms, garage area, and porch area—were set to zero. - We also zeroed out all building area variables and the corresponding building value variables to reflect the absence of a residential structure.\nThese records were retained in the dataset rather than dropped, as they likely represent a distinct class of properties where valuation is driven primarily by land characteristics. Explicitly identifying and treating these properties allowed the model to better separate residential and non-residential valuation patterns.\n\n\nCode\n# === STEP 0: Define base feature names ===\nnumeric_bases = [\n    'garage_area', 'porch_area', 'floors', 'half_bath', 'full_bath',\n    'total_rooms', 'bedrooms', 'fireplaces', 'building_area', 'building_value'\n]\n\ncategorical_fill_map = {\n    'quality': 'None',\n    'quality_description': 'None',\n    'building_condition': 'None',\n    'foundation_type': 'None',\n    'grade': 'None',\n    'has_cooling': False,\n    'has_heat': False,\n    'physical_condition': 'None',\n    'exterior_walls': 'None',\n    'protested': False\n}\n\n# Generate full list of columns (2015–2019 only, no final columns)\nnumeric_cols_to_zero = [\n    f'{base}_{year}' for base in numeric_bases for year in range(2015, 2020)\n] + ['building_value_growth']\n\ncategorical_cols_to_fill = {\n    f'{base}_{year}': val\n    for base, val in categorical_fill_map.items()\n    for year in range(2015, 2020)\n}\n\n# === STEP 1: Apply imputation if floor_area_total_2019 == 0 ===\nfor df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n    if 'floor_area_total_2019' in df.columns:\n        zero_floor_mask = df['floor_area_total_2019'] == 0\n\n        # Fill numeric columns with 0\n        for col in numeric_cols_to_zero:\n            if col in df.columns:\n                df.loc[zero_floor_mask, col] = df.loc[zero_floor_mask, col].fillna(0)\n\n        # Fill categorical/boolean columns\n        for col, fill_val in categorical_cols_to_fill.items():\n            if col in df.columns:\n                df.loc[zero_floor_mask, col] = df.loc[zero_floor_mask, col].fillna(fill_val)\n\n        print(f\" Filled structure-dependent missing values in {df_name} for {zero_floor_mask.sum()} rows\")\n    else:\n        print(f\" 'floor_area_total_2019' not found in {df_name}\")\n\n\nFeature Drop Based on Sparse Signals: For features that appeared largely irrelevant or unused across the dataset, we calculated the percentage of zero values in columns such as mobile home area, deck area, and porch area. If a column contained over 90% zeros, it was considered non-informative and dropped from the modeling pipeline to reduce dimensionality and noise.\n\n\nCode\ncols_to_drop = [col for col in train_merged.columns if col.startswith(\"mobile_home_area\")]\n\n# Drop from both sets\ntrain_merged.drop(columns=cols_to_drop, inplace=True)\ntest_merged.drop(columns=cols_to_drop, inplace=True)\n\nprint(f\" Dropped columns from train/test: {cols_to_drop}\")\n\n\nMulti-Level Median and Mode Imputation: After applying logic-based pruning, we used a three-level median imputation strategy for continuous features (e.g., assessed value 2017, building area 2017) based on the following hierarchy: - Level 1: neighborhood-level median - Level 2: region-level median - Level 3: global median (fallback)\n\n\nCode\n# List of all assessed columns to impute\nassessed_cols = ['assessed_2015', 'assessed_2016', 'assessed_2017', 'assessed_2018']\n\nfor col in assessed_cols:\n    if col not in train_merged.columns:\n        continue\n\n    # Step 1: Compute medians from training data only\n    neigh_medians = train_merged.groupby('neighborhood')[col].median()\n    region_medians = train_merged.groupby('region')[col].median()\n    global_median = train_merged[col].median()\n\n    # Step 2: Train set imputation\n    train_merged[col] = train_merged.apply(\n        lambda row: neigh_medians[row['neighborhood']]\n        if pd.isna(row[col]) and row['neighborhood'] in neigh_medians else\n        region_medians[row['region']]\n        if pd.isna(row[col]) and row['region'] in region_medians else\n        global_median\n        if pd.isna(row[col]) else\n        row[col],\n        axis=1\n    )\n\n    # Step 3: Test set imputation (using train medians only)\n    test_merged[col] = test_merged.apply(\n        lambda row: neigh_medians.get(row['neighborhood'], np.nan)\n        if pd.isna(row[col]) else row[col],\n        axis=1\n    )\n    test_merged[col] = test_merged.apply(\n        lambda row: region_medians.get(row['region'], np.nan)\n        if pd.isna(row[col]) else row[col],\n        axis=1\n    )\n    test_merged[col].fillna(global_median, inplace=True)\n\n    print(f\" Imputed '{col}' using neighborhood → region → global medians (from training data)\")\n\n\nFor categorical variables such as foundation type or building condition, we applied single-level mode imputation using the most frequent category within the training data. While this approach is less localized, it provided a simple and stable method for handling missing values in features with low cardinality. ## 3.2 Neighborhood and Region-Level Statistical Features\nTo capture localized pricing dynamics and identify anomalies in property assessments, we engineered a suite of statistical features using the 2018 assessed values as a proxy for prior valuation context. We first computed neighborhood-level metrics including the mean, median, standard deviation, and interquartile range (IQR) of assessed 2018, grouped by neighborhood. Similarly, region-level statistics were computed using the region variable.\nUsing the merged and imputed stats, we computed derived features such as: - assess minus neigh mean: the raw deviation of a property’s 2018 assessed value from its neighborhood mean - assess ratio neigh mean: a normalized ratio of a property’s value to its local average - z score assess neigh: a z-score based on neighborhood-level variation - Corresponding region-level counterparts: assess minus region mean, assess ratio region mean, and z score assess region\nThese features helped contextualize each property’s assessed value relative to other properties within the same neighborhood or region. They proved useful in capturing outliers and potentially undervalued homes that deviated from local valuation patterns.\n\n\nCode\n# === Step 1: Compute neighborhood-level stats ===\nneigh_stats = train_merged.groupby('neighborhood')['assessed_2018'].agg([\n    ('neigh_assess_mean', 'mean'),\n    ('neigh_assess_median', 'median'),\n    ('neigh_assess_std', 'std'),\n    ('neigh_assess_q1', lambda x: x.quantile(0.25)),\n    ('neigh_assess_q3', lambda x: x.quantile(0.75)),\n]).reset_index()\nneigh_stats['neigh_assess_iqr'] = neigh_stats['neigh_assess_q3'] - neigh_stats['neigh_assess_q1']\n\n# === Step 2: Compute region-level stats ===\nregion_stats = train_merged.groupby('region')['assessed_2018'].agg([\n    ('region_assess_mean', 'mean'),\n    ('region_assess_median', 'median'),\n    ('region_assess_std', 'std'),\n    ('region_assess_q1', lambda x: x.quantile(0.25)),\n    ('region_assess_q3', lambda x: x.quantile(0.75)),\n]).reset_index()\nregion_stats['region_assess_iqr'] = region_stats['region_assess_q3'] - region_stats['region_assess_q1']\n\n# === Step 3: Fallback std maps from training data ===\n# For neighborhood fallback, group region medians of neighborhood std\nneigh_std_by_region = neigh_stats.merge(train_merged[['neighborhood', 'region']], on='neighborhood', how='left') \\\n                                  .groupby('region')['neigh_assess_std'].median()\nglobal_neigh_std = neigh_stats['neigh_assess_std'].median()\n\nregion_std_by_neigh = region_stats.merge(train_merged[['neighborhood', 'region']], on='region', how='left') \\\n                                   .groupby('neighborhood')['region_assess_std'].median()\nglobal_region_std = region_stats['region_assess_std'].median()\n\n# === Step 4: Merge into train/test and compute features ===\nfor df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n    df = df.merge(neigh_stats, on='neighborhood', how='left')\n    df = df.merge(region_stats, on='region', how='left')\n\n    # Fill missing std values via fallback\n    df['neigh_assess_std'] = df['neigh_assess_std'].fillna(\n        df['region'].map(neigh_std_by_region)\n    ).fillna(global_neigh_std)\n\n    df['region_assess_std'] = df['region_assess_std'].fillna(\n        df['neighborhood'].map(region_std_by_neigh)\n    ).fillna(global_region_std)\n\n    # Compute derived features\n    df['assess_minus_neigh_mean'] = df['assessed_2018'] - df['neigh_assess_mean']\n    df['assess_ratio_neigh_mean'] = df['assessed_2018'] / (df['neigh_assess_mean'] + 1e-6)\n    df['z_score_assess_neigh'] = df['assess_minus_neigh_mean'] / (df['neigh_assess_std'] + 1e-6)\n\n    df['assess_minus_region_mean'] = df['assessed_2018'] - df['region_assess_mean']\n    df['assess_ratio_region_mean'] = df['assessed_2018'] / (df['region_assess_mean'] + 1e-6)\n    df['z_score_assess_region'] = df['assess_minus_region_mean'] / (df['region_assess_std'] + 1e-6)\n\n    # Save back\n    if df_name == 'train_merged':\n        train_merged = df\n    else:\n        test_merged = df\n\nprint(\" Completed: Stats merge + std fallback + z-score computation.\")"
  },
  {
    "objectID": "project3.html#frequency-encoding-of-high-cardinality-geographic-variables",
    "href": "project3.html#frequency-encoding-of-high-cardinality-geographic-variables",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.3 Frequency Encoding of High-Cardinality Geographic Variables",
    "text": "3.3 Frequency Encoding of High-Cardinality Geographic Variables\nTo convert high-cardinality categorical variables into numerical features while preserving signal strength, we applied frequency encoding to four key geographic identifiers: neighborhood, region, zone, and subneighborhood.\nThis encoding strategy served two purposes: 1. It allowed the model to retain information about how common or rare a spatial unit was. A frequently occurring neighborhood (i.e., one with high frequency) likely has more properties, which implies greater residential or commercial development in that area. 2. Areas with more properties are also likely to have more consistent and well-understood assessment patterns—the government has “seen” more properties there, which may reduce valuation volatility. These areas are more visible or prioritized in municipal processes.\nFinally, frequency encoding avoids the dimensional explosion caused by one-hot encoding, which is especially problematic for variables with high cardinality like neighborhood or subneighborhood.\n\n\nCode\nfor col in ['neighborhood', 'region','zone','subneighborhood']:\n    if col in train_merged.columns:\n        # Step 1: Compute frequency from training data\n        freq_map = train_merged[col].value_counts(normalize=True)\n\n        # Step 2: Apply to both datasets\n        train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n        test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n\n        print(f\" Frequency encoded: {col} → {col}_freq (based on training set)\")\n    else:\n        print(f\" Column '{col}' not found in training set\")"
  },
  {
    "objectID": "project3.html#boolean-and-ordinal-encoding",
    "href": "project3.html#boolean-and-ordinal-encoding",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.4 Boolean and Ordinal Encoding",
    "text": "3.4 Boolean and Ordinal Encoding\nBoolean Encoding: We focused on three boolean variables: has cooling, has heat, and protested. These features were encoded across all five years using binary values (0/1).\nOrdinal Encoding: For ordinal features such as quality, quality description, grade, building condition, and physical condition, we performed domain-informed cleaning and then applied ordinal encoding based on defined category hierarchies. Prior to encoding, raw values were standardized through column-specific replacements. For instance, extreme or ambiguous values like X, None, or overly granular subgrades (e.g., X-, E+) were either mapped to more interpretable categories or treated as missing. Some detailed conditions like Unsound and Very Poor were collapsed into broader categories such as Poor. Unknown values were handled gracefully by assigning an encoded fallback of -1. This process ensured that ordinal information was preserved in a numerically meaningful way, allowing models to leverage the ordered nature of these features without exploding dimensionality as one-hot encoding would.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import KFold\n\n# Clear specific variables\nfor var in ['ordinal_cols_all', 'bool_cols_all']:\n    if var in locals():\n        del globals()[var]\n\n# === STEP 1: Boolean Encoding (2015–2019 only) ===\nbool_bases = ['has_cooling', 'has_heat', 'protested']\nbool_cols_all = [f\"{base}_{year}\" for base in bool_bases for year in range(2015, 2020)]\n\nfor col in bool_cols_all:\n    if col in train_merged.columns:\n        mode_val = train_merged[col].mode(dropna=True)[0]\n        train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n        test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n\n# === STEP 2: Ordinal Cleaning and Encoding (2015–2019 only) ===\nordinal_bases = [\n    'quality', 'quality_description', 'grade',\n    'building_condition', 'physical_condition'\n]\n\nordinal_cols_all = [f\"{base}_{year}\" for base in ordinal_bases for year in range(2015, 2020)]\n\n# Column-specific replacements\nreplacement_maps = {\n    'quality': {'E': 'D', 'F': 'D', 'X': np.nan, 'None': np.nan},\n    'quality_description': {'Poor': 'Very Low', 'None': np.nan},\n    'grade': {'X': 'F', 'X-': 'F', 'X+': 'F', 'E': 'D', 'E-': 'D-', 'E+': 'D+', 'None': np.nan},\n    'building_condition': {'Very Poor': 'Poor', 'Unsound': 'Poor', 'None': np.nan},\n    'physical_condition': {'Very Poor': 'Poor', 'Unsound': 'Poor', 'None': np.nan}\n}\n\n# Ordinal category order\nord_categories = {\n    'quality': ['D', 'C', 'B', 'A'],\n    'quality_description': ['Very Low', 'Low', 'Average', 'Good', 'Excellent', 'Superior'],\n    'grade': ['F', 'D-', 'D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'],\n    'building_condition': ['Poor', 'Fair', 'Average', 'Good', 'Very Good', 'Excellent'],\n    'physical_condition': ['Poor', 'Fair', 'Average', 'Good', 'Very Good', 'Excellent']\n}\n\n# Clean and encode\nfor base in ordinal_bases:\n    for year in range(2015, 2020):\n        col = f\"{base}_{year}\"\n        if col in train_merged.columns:\n            replacements = replacement_maps.get(base, {})\n            train_merged[col] = train_merged[col].replace(replacements)\n            test_merged[col] = test_merged[col].replace(replacements)\n\n            mode_val = train_merged[col].mode(dropna=True)[0]\n            train_merged[col] = train_merged[col].fillna(mode_val)\n            test_merged[col] = test_merged[col].fillna(mode_val)\n\n            encoder = OrdinalEncoder(categories=[ord_categories[base]], handle_unknown='use_encoded_value', unknown_value=-1)\n            train_merged[[col]] = encoder.fit_transform(train_merged[[col]])\n            test_merged[[col]] = encoder.transform(test_merged[[col]])"
  },
  {
    "objectID": "project3.html#target-encoding-of-nominal-categorical-variables-20152019",
    "href": "project3.html#target-encoding-of-nominal-categorical-variables-20152019",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.5 Target Encoding of Nominal Categorical Variables (2015–2019)",
    "text": "3.5 Target Encoding of Nominal Categorical Variables (2015–2019)\nFor certain nominal features that lacked ordinal structure but exhibited high cardinality, such as foundation type and exterior walls, we applied target encoding across all years from 2015 to 2019. This encoding replaces each category with a smoothed version of the mean target value (assessed 2018) observed for that category.\nTo avoid overfitting and data leakage, we implemented a 5-fold cross-validated target encoding procedure. For each fold, the mean target value was computed from the training portion and mapped to the validation fold. We used a smoothing parameter of 10 to balance the influence of the global mean versus the category-specific mean, especially for infrequent categories.\nThis method enabled us to capture predictive signal from nominal features without creating high-dimensional one-hot encodings or imposing artificial ordinal structure.\n\n\nCode\n# === STEP 3: Target Encoding (2015–2019 only) ===\ndef group_and_target_encode_cv(train_df, test_df, target_name, column, rare_threshold=0.001, smoothing=10, n_splits=5):\n    freq = train_df[column].value_counts(normalize=True)\n    rare_cats = freq[freq &lt; rare_threshold].index\n    train_df[column] = train_df[column].replace(rare_cats, 'Other')\n    test_df[column] = test_df[column].replace(rare_cats, 'Other')\n\n    global_mean = train_df[target_name].mean()\n    oof_encoded = pd.Series(index=train_df.index, dtype='float64')\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for train_idx, val_idx in kf.split(train_df):\n        X_tr, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n        stats = X_tr.groupby(column)[target_name].agg(['mean', 'count'])\n        smooth = (stats['mean'] * stats['count'] + global_mean * smoothing) / (stats['count'] + smoothing)\n        oof_encoded.iloc[val_idx] = X_val[column].map(smooth).fillna(global_mean)\n\n    final_stats = train_df.groupby(column)[target_name].agg(['mean', 'count'])\n    final_smooth = (final_stats['mean'] * final_stats['count'] + global_mean * smoothing) / (final_stats['count'] + smoothing)\n    test_encoded = test_df[column].map(final_smooth).fillna(global_mean)\n\n    return oof_encoded, test_encoded\n\n# Target-encodable nominal columns\ntarget_encodable_bases = ['foundation_type', 'exterior_walls']\ntarget_encodable_cols_all = [f\"{base}_{year}\" for base in target_encodable_bases for year in range(2015, 2020)]\n\n# Apply target encoding\nfor col in target_encodable_cols_all:\n    if col in train_merged.columns:\n        mode_val = train_merged[col].mode(dropna=True)[0]\n        train_merged[col] = train_merged[col].fillna(mode_val)\n        test_merged[col] = test_merged[col].fillna(mode_val)\n\n        train_merged[f'{col}_te'], test_merged[f'{col}_te'] = group_and_target_encode_cv(\n            train_merged, test_merged, target_name='assessed_2018', column=col,\n            rare_threshold=0.001, smoothing=10, n_splits=5\n        )\n\n        train_merged.drop(columns=[col], inplace=True)\n        test_merged.drop(columns=[col], inplace=True)"
  },
  {
    "objectID": "project3.html#quantile-binning-of-features",
    "href": "project3.html#quantile-binning-of-features",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.6 Quantile Binning of Features",
    "text": "3.6 Quantile Binning of Features\nTo enhance robustness and reduce sensitivity to outliers, we converted few continuous features into categorical bins using quantile-based binning. Growth metrics such as land value growth, building value growth, and assessed growth were binned into four quantiles, with thresholds computed only on the training data to prevent information leakage. If quantile binning failed due to low cardinality (e.g., repeated values), we defaulted to equal-width binning. All binned variables were explicitly cast as categorical to ensure compatibility with tree-based models.\nAdditionally, we binned year built final into five quantiles to capture generational differences in construction periods. This replaced raw year values with interpretable ordinal categories. Original continuous features were removed after binning to avoid redundancy and reduce multicollinearity.\n\n\nCode\n# === Step 1: List your growth features ===\ngrowth_features = ['land_value_growth', 'building_value_growth', 'assessed_growth']\n\n# === Step 2: Binning Function (train-based binning) ===\ndef bin_growth_feature_safe(train_df, test_df, feature, bins=4):\n    try:\n        # Quantile binning on train only\n        train_df[f'{feature}_bin'], bin_edges = pd.qcut(train_df[feature], q=bins, labels=False, retbins=True, duplicates='drop')\n        test_df[f'{feature}_bin'] = pd.cut(test_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n    except ValueError:\n        # Fallback: Equal-width binning\n        min_val = train_df[feature].min()\n        max_val = train_df[feature].max()\n        bin_edges = np.linspace(min_val, max_val, bins + 1)\n        train_df[f'{feature}_bin'] = pd.cut(train_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n        test_df[f'{feature}_bin'] = pd.cut(test_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n\n    # Convert to category\n    train_df[f'{feature}_bin'] = train_df[f'{feature}_bin'].astype('category')\n    test_df[f'{feature}_bin'] = test_df[f'{feature}_bin'].astype('category')\n    return train_df, test_df\n\n# === Step 3: Apply to train_merged and test_merged ===\nfor feature in growth_features:\n    train_merged, test_merged = bin_growth_feature_safe(train_merged, test_merged, feature)\n\n# === Step 4: Bin year_built_final using train-based quantiles ===\ntrain_merged['year_built_bin'], bin_edges = pd.qcut(\n    train_merged['year_built_final'], q=5, retbins=True, labels=False, duplicates='drop'\n)\ntest_merged['year_built_bin'] = pd.cut(\n    test_merged['year_built_final'], bins=bin_edges, labels=False, include_lowest=True\n)\n\n# Convert to category\ntrain_merged['year_built_bin'] = train_merged['year_built_bin'].astype('category')\ntest_merged['year_built_bin'] = test_merged['year_built_bin'].astype('category')\n\n# === Step 5: Drop original continuous columns ===\ncols_to_drop = growth_features + ['year_built_final']\ntrain_merged.drop(columns=cols_to_drop, inplace=True)\ntest_merged.drop(columns=cols_to_drop, inplace=True)\n\nprint(\" Binned growth & year_built features safely with no leakage.\")"
  },
  {
    "objectID": "project3.html#rare-frequency-suppression-in-spatial-encodings",
    "href": "project3.html#rare-frequency-suppression-in-spatial-encodings",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.7 Rare Frequency Suppression in Spatial Encodings",
    "text": "3.7 Rare Frequency Suppression in Spatial Encodings\nFollowing frequency encoding of high-cardinality spatial variables (region, neighborhood, zone, subneighborhood), we applied a rare-value suppression step to mitigate the noise introduced by sparsely represented categories. For each frequency-encoded column, we identified values that occurred in less than 0.1% of the training data and replaced them with a neutral value of zero in both the training and test sets. This was done using thresholds derived solely from the training distribution to prevent data leakage.\nThe intuition behind this strategy is that extremely rare spatial groupings may not provide reliable or generalizable signals to the model. Treating them as a common fallback class (i.e., assigning them a frequency of zero) improves model stability and reduces overfitting to idiosyncratic, low-support locations. This transformation preserves the informativeness of frequent categories while smoothing out sparse tail behavior in the feature space.\n\n\nCode\n# Define frequency columns and threshold\nfreq_cols = ['region_freq', 'neighborhood_freq', 'zone_freq', 'subneighborhood_freq']\nrare_thresh = 0.001\n\n# Apply rare value replacement for each frequency column\nfor col in freq_cols:\n    if col in train_merged.columns:\n        rare_vals = train_merged[col].value_counts(normalize=True)[lambda x: x &lt; rare_thresh].index\n        train_merged[col] = train_merged[col].replace(rare_vals, 0)\n        test_merged[col] = test_merged[col].replace(rare_vals, 0)\n        print(f\" Replaced rare values in {col} using train_merged threshold &lt; {rare_thresh}\")\n    else:\n        print(f\" Column {col} not found in train_merged — skipping.\")"
  },
  {
    "objectID": "project3.html#log-transformation-and-distribution-smoothing-for-ridge-regression",
    "href": "project3.html#log-transformation-and-distribution-smoothing-for-ridge-regression",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.8 Log Transformation and Distribution Smoothing for Ridge Regression",
    "text": "3.8 Log Transformation and Distribution Smoothing for Ridge Regression\nTo satisfy linear model assumptions and reduce skew-related distortion in Ridge regression, we applied a targeted log transformation to select continuous features. Specifically, we identified variables related to building size, land area, and valuation (e.g., building value 2019, land area 2018, neigh assess mean) whose skewness exceeded a threshold of 2.0 in the training set. For these features, we applied a log1p transformation, which effectively stabilized variance, compressed long-tailed distributions, and improved linear fit potential.\nThis transformation was particularly useful for the Ridge regression model, which benefits from normally distributed inputs and is sensitive to extreme values. By selectively applying log1p only to features with high skew, we preserved model interpretability while enhancing numerical stability and predictive performance.\n\n\nCode\nimport numpy as np\n\n# === Step 1: Skew-based Log Transformation (2015–2019 only) ===\nlog_bases = [\n    'floor_area_total', 'porch_area', 'building_area', 'land_area',\n    'building_value', 'land_value', 'assessed'\n]\nneigh_stat_cols = [\n    'neigh_assess_mean', 'neigh_assess_std', 'neigh_assess_median',\n    'neigh_assess_q1', 'neigh_assess_q3'\n]\n\n# Collect log-transformable columns (2015–2019 + neighborhood stats)\nlog_transform_cols = [f\"{base}_{year}\" for base in log_bases for year in range(2015, 2020)]\nlog_transform_cols += neigh_stat_cols\n\n# Compute skewness on train and apply log1p only if skew &gt; 2\nfor col in log_transform_cols:\n    if col in train_merged.columns:\n        skew = train_merged[col].skew()\n        if skew &gt; 2:\n            for df in [train_merged, test_merged]:\n                df[f\"log_{col}\"] = np.log1p(df[col])\n            print(f\" Log-transformed: {col} (skew={skew:.2f})\")\n        else:\n            print(f\"ℹ Skipped: {col} (skew={skew:.2f})\")"
  },
  {
    "objectID": "project3.html#adaptive-quantile-clipping-for-tree-based-models",
    "href": "project3.html#adaptive-quantile-clipping-for-tree-based-models",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.9 Adaptive Quantile Clipping for Tree-Based Models",
    "text": "3.9 Adaptive Quantile Clipping for Tree-Based Models\nTo further control the influence of extreme values in tree-based models, we implemented an adaptive quantile clipping strategy informed by skewness severity. Using a precomputed skewness report, we categorized numeric features (excluding binary and target-encoded variables) into two groups: ultra-skewed (skewness &gt; 100) and moderately-skewed (2 &lt; skewness ≤ 100). Features were considered only if they had more than ten unique values and were not binary.\nFor ultra-skewed features, we applied clipping at the 0.5th and 99.5th percentiles. For moderately skewed features, we clipped at the 0.1st and 99.9th percentiles. All thresholds were derived solely from the training data and applied to both training and test sets to ensure leakage-free transformations. This clipping procedure helped suppress extreme values that might otherwise dominate decision paths or split criteria in tree-based learners.\nThese transformations were specifically designed for use with XGBoost and LightGBM, where reducing the influence of outliers improves model generalization and enhances interpretability in leaf-based decision structures.\n\n\nCode\nimport pandas as pd\n\n\n# === Step 1: Categorize features by skew level ===\nultra_skewed = []\nmoderately_skewed = []\n\nfor _, row in skew_df.iterrows():\n    feature = row['feature']\n    skew = row['skewness']\n    \n    if feature not in train_merged.columns:\n        continue\n\n    unique_vals = train_merged[feature].nunique()\n    is_binary = set(train_merged[feature].dropna().unique()).issubset({0, 1})\n\n    if unique_vals &gt; 10 and not is_binary and not feature.endswith('_te'):\n        if skew &gt; 100:\n            ultra_skewed.append(feature)\n        elif 2&lt; skew &lt;= 100:\n            moderately_skewed.append(feature)\n\nprint(f\" {len(ultra_skewed)} ultra-skewed features to clip at 0.995.\")\nprint(f\" {len(moderately_skewed)} moderately-skewed features to clip at 0.999.\")\n\n# === Step 2: Compute quantile clipping bounds ===\nclip_bounds = {}\n\nfor col in ultra_skewed:\n    clip_bounds[col] = (\n        train_merged[col].quantile(0.005),\n        train_merged[col].quantile(0.995)\n    )\n\nfor col in moderately_skewed:\n    clip_bounds[col] = (\n        train_merged[col].quantile(0.001),\n        train_merged[col].quantile(0.999)\n    )\n\n# === Step 3: Apply clipping to both train and test ===\nfor df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n    for col, (lower, upper) in clip_bounds.items():\n        if col in df.columns:\n            df[col] = df[col].clip(lower, upper)\n\nprint(\" Adaptive clipping applied: 0.995 for ultra-skewed, 0.999 for moderately-skewed features.\")"
  },
  {
    "objectID": "project3.html#interaction-features-for-linear-and-nonlinear-models",
    "href": "project3.html#interaction-features-for-linear-and-nonlinear-models",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.10 Interaction Features for Linear and Nonlinear Models",
    "text": "3.10 Interaction Features for Linear and Nonlinear Models\nTo enrich model expressiveness, we engineered a comprehensive set of interaction features used across both Ridge regression and tree-based models (XGBoost and LightGBM). These included multiplicative and ratio-based terms such as grade quality index, value per age, area x quality, and assess to neigh mean, capturing relationships between physical dimensions, valuation, quality, and neighborhood context. For Ridge regression, these features acted as implicit basis expansions—effectively enabling the linear model to capture non-additive effects by introducing new combinations of input features.\nAdditionally, we created a specialized set of log-transformed interaction terms—such as log area x grade, log assess x age, and log value diff—used exclusively in the Ridge pipeline. These features helped linearize multiplicative relationships and reduce skew, improving fit under Ridge’s sensitivity to input distribution. Log-based interactions were excluded from tree models, which are inherently robust to skew and insensitive to monotonic transformations like log, as they rely only on the relative ordering of feature values when making splits.\n\n\nCode\n# === Interaction Features for Ridge Regression ===\ndef add_features(df):\n    df = df.copy()\n\n    # === Ratio features ===\n    df['area_ratio'] = df['building_area_2019'] / (df['land_area_2019'] + 1)\n    df['porch_ratio'] = df['porch_area_2019'] / (df['building_area_2019'] + 1)\n    df['floor_density'] = df['floor_area_total_2019'] / (df['land_area_2019'] + 1)\n    df['log_build_density'] = df['log_building_area_2019'] - df['log_land_area_2019']\n    df['log_land_to_build_ratio'] = df['log_land_area_2019'] - df['log_building_area_2019']\n\n    df['value_ratio'] = df['building_value_2018'] / (df['land_value_2018'] + 1)\n    df['log_value_diff'] = df['log_building_value_2018'] - df['log_land_value_2018']\n    df['value_per_sqft'] = df['building_value_2018'] / (df['building_area_2019'] + 1)\n    df['price_per_sqft'] = df['assessed_2018'] / (df['building_area_2019'] + 1)\n\n    # === Bathroom & room structure ===\n    df['bathroom_score'] = df['full_bath_2019'] + 0.5 * df['half_bath_2019']\n    df['bathroom_density'] = df['bathroom_score'] / (df['total_rooms_2019'] + 1)\n    df['bedroom_ratio'] = df['bedrooms_2019'] / (df['total_rooms_2019'] + 1)\n    df['rooms_per_floor'] = df['total_rooms_2019'] / (df['floors_2019'] + 1)\n\n    # === Core interactions ===\n    df['bedrooms_x_floors'] = df['bedrooms_2019'] * df['floors_2019']\n    df['rooms_x_quality'] = df['total_rooms_2019'] * df['quality_2019']\n    df['log_area_x_grade'] = df['log_building_area_2019'] * df['grade_2019']\n    df['log_assess_x_age'] = df['log_assessed_2018'] * df['building_age']\n    df['assess_spread_neigh'] = df['log_neigh_assess_q3'] - df['log_neigh_assess_q1']\n    df['grade_quality_index'] = df['grade_2019'] * df['quality_2019']\n\n    # === Clean up ===\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    return df\n\n# === Apply to train and test ===\ntrain_merged = add_features(train_merged)\ntest_merged = add_features(test_merged)\n\n\n\n\nCode\n# === Interaction Features for Tree models ===\ndef add_features(df):\n    df = df.copy()\n    \n    # === Ratio features ===\n    df['area_ratio'] = df['building_area_2019'] / (df['land_area_2019'] + 1)\n    df['porch_ratio'] = df['porch_area_2019'] / (df['building_area_2019'] + 1)\n    df['floor_density'] = df['floor_area_total_2019'] / (df['land_area_2019'] + 1)\n    \n    df['value_ratio'] = df['building_value_2018'] / (df['land_value_2018'] + 1)\n    df['value_per_sqft'] = df['building_value_2018'] / (df['building_area_2019'] + 1)\n    df['price_per_sqft'] = df['assessed_2018'] / (df['building_area_2019'] + 1)\n\n    # === Bathroom & room structure ===\n    df['bathroom_score'] = df['full_bath_2019'] + 0.5 * df['half_bath_2019']\n    df['bathroom_density'] = df['bathroom_score'] / (df['total_rooms_2019'] + 1)\n    df['bedroom_ratio'] = df['bedrooms_2019'] / (df['total_rooms_2019'] + 1)\n    df['rooms_per_floor'] = df['total_rooms_2019'] / (df['floors_2019'] + 1)\n\n    # === Core interactions ===\n    df['bedrooms_x_floors'] = df['bedrooms_2019'] * df['floors_2019']\n    df['rooms_x_quality'] = df['total_rooms_2019'] * df['quality_2019']\n    df['assess_x_age'] = df['assessed_2018'] * df['building_age']\n    df['grade_quality_index'] = df['grade_2019'] * df['quality_2019']\n\n    # === Selected high-signal interactions ===\n    df['area_x_quality'] = df['building_area_2019'] * df['quality_2019']\n    df['floor_area_x_grade'] = df['floor_area_total_2019'] * df['grade_2019']\n    df['value_to_neigh_median'] = df['building_value_2018'] / (df['neigh_assess_median'] + 1)\n    df['assess_to_neigh_mean'] = df['assessed_2018'] / (df['neigh_assess_mean'] + 1)\n    df['value_per_age'] = df['building_value_2018'] / (df['building_age'] + 1)\n\n    # === Clean up ===\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    \n    return df\n\n# === Apply to train and test sets ===\ntrain_merged = add_features(train_merged)\ntest_merged = add_features(test_merged)"
  },
  {
    "objectID": "project3.html#ridge-regression-with-cross-validation",
    "href": "project3.html#ridge-regression-with-cross-validation",
    "title": "STA 9890 Project: Property Valuation",
    "section": "4.1 Ridge Regression with Cross-Validation",
    "text": "4.1 Ridge Regression with Cross-Validation\nWe implemented a Ridge regression model using RidgeCV to automatically select the regularization strength α through nested cross-validation. A 3-fold outer loop was used for estimating out-of-fold (OOF) performance, while each inner fold evaluated a grid of α values ranging from 10⁻³ to 10² on a logarithmic scale. Input features were standardized within a Pipeline using StandardScaler to ensure scale-invariant regression coefficients. The model selected a different optimal α for each fold, reflecting local variance in validation behavior:\n\nFold 1 RMSE: 42,050.33—Best α: 2.1544\nFold 2 RMSE: 41,036.52—Best α: 27.8256\nFold 3 RMSE: 40,619.40—Best α: 0.5995\n\nThe final out-of-fold RMSE across all folds was 41,239.79, with an average best α of approximately 10.1932. This indicates that moderate regularization consistently improved generalization across different training splits.\nWe saved both OOF predictions and test forecasts as NumPy arrays—ridgecv_oof_preds.npy and ridgecv_test_preds.npy—for later use in model ensembling. These stored outputs served as reliable building blocks for downstream blending and stacking strategies.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\n# === STEP 2: Prepare training/test matrices ===\nX = train_merged.copy()\nX_test = test_merged.copy()\ny = pd.Series(y_train).values # use raw target (not log)\n\n# === STEP 3: RidgeCV pipeline ===\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nalphas = np.logspace(-3, 2, 10)\n\nridge_oof = np.zeros(len(X))\nridge_test_preds = np.zeros(len(X_test))\nbest_alphas = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n    print(f\"\\n Fold {fold+1}/5\")\n\n    X_train, y_train_fold = X.iloc[train_idx], y[train_idx]\n    X_val, y_val = X.iloc[val_idx], y[val_idx]\n\n    model = make_pipeline(\n        StandardScaler(),\n        RidgeCV(alphas=alphas, cv=3, scoring='neg_root_mean_squared_error')\n    )\n\n    model.fit(X_train, y_train_fold)\n    ridge_oof[val_idx] = model.predict(X_val)\n    ridge_test_preds += model.predict(X_test) / kf.get_n_splits()\n\n    best_alpha = model.named_steps['ridgecv'].alpha_\n    best_alphas.append(best_alpha)\n    \n    fold_rmse = root_mean_squared_error(y_val, ridge_oof[val_idx])\n    print(f\"Fold {fold+1} RMSE: {fold_rmse:,.2f} | Best alpha: {best_alpha:.4f}\")\n\n# === STEP 4: Final RMSE ===\nfinal_rmse = root_mean_squared_error(y, ridge_oof)\nprint(f\"\\n Final OOF RMSE (RidgeCV): {final_rmse:,.2f}\")\nprint(f\" Average best alpha across folds: {np.mean(best_alphas):.4f}\")\n\n# === STEP 5: Save predictions ===\nsubmission = pd.DataFrame({\n    \"ACCOUNT\": acct_test.values.ravel(),\n    \"TARGET\": ridge_test_preds\n})\nsubmission.to_csv(\"submission_ridgecv_pipeline.csv\", index=False)\nprint(\"\\n Saved: submission_ridgecv_pipeline.csv\")\n\n# === Optional: Save OOF & test preds for stacking or analysis ===\nnp.save(\"ridgecv_oof_preds.npy\", ridge_oof)\nnp.save(\"ridgecv_test_preds.npy\", ridge_test_preds)\nprint(\" Saved: ridgecv_oof_preds.npy and ridgecv_test_preds.npy\")"
  },
  {
    "objectID": "project3.html#tree-based-models-with-optuna-and-shap-gain-feature-selection",
    "href": "project3.html#tree-based-models-with-optuna-and-shap-gain-feature-selection",
    "title": "STA 9890 Project: Property Valuation",
    "section": "4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection",
    "text": "4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection\nTo capture nonlinear interactions and leverage automatic handling of missing values and categorical splits, we trained two gradient boosting models: LightGBM and XGBoost. Both models followed a structured pipeline consisting of hyperparameter optimization using Optuna, followed by SHAP- and gain-based feature selection, and a final retraining on the selected features.\nStep 1: Hyperparameter Tuning with Optuna. For each model, we defined an Optuna objective that trained 3-fold cross-validated models using early stopping. We explored hyperparameter ranges tailored to each algorithm, with LightGBM using a native pruning callback and XGBoost leveraging XGBoostPruningCallback. During tuning, we stored the best out-of-fold (OOF) predictions across all trials to later use in ensembling.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport lightgbm as lgb\nimport shap\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import root_mean_squared_error\nfrom optuna.integration import LightGBMPruningCallback\nfrom optuna.pruners import SuccessiveHalvingPruner\nfrom lightgbm import log_evaluation, early_stopping\n\n# === STEP 0: Setup Data ===\nX_full = train_merged.copy()\ny_full = pd.Series(y_train)\nX_test = test_merged.copy()\n\n# Detect categorical columns\ncat_cols = X_full.select_dtypes(include=['category', 'object']).columns.tolist()\nfor col in cat_cols:\n    X_full[col] = X_full[col].astype(\"category\")\n    X_test[col] = X_test[col].astype(\"category\")\n\nglobal_oof_preds = np.zeros(len(X_full))\nbest_score = float('inf')\n\n# === STEP 1: Define Optuna Objective ===\ndef objective(trial):\n    global global_oof_preds, best_score\n\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.025, 0.04, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 160, 220),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 7, 11),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 18, 30),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.65, 0.88),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.75),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1.0, 5.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 4.0, log=True),\n        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.15, 0.25),\n        \"verbose\": -1,\n        \"n_jobs\": -1,\n    }\n\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    val_rmse = []\n    oof_preds = np.zeros(len(X_full))\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n        X_train, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n        y_train_fold, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n\n        dtrain = lgb.Dataset(X_train, label=y_train_fold, categorical_feature=cat_cols)\n        dvalid = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_cols)\n\n        model = lgb.train(\n            params,\n            dtrain,\n            valid_sets=[dvalid],\n            num_boost_round=1000,\n            callbacks=[\n                early_stopping(stopping_rounds=100),\n                log_evaluation(period=100),\n                LightGBMPruningCallback(trial, \"rmse\")\n            ]\n        )\n\n        val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n        oof_preds[val_idx] = val_pred\n        val_rmse.append(root_mean_squared_error(y_val, val_pred))\n\n    mean_rmse = np.mean(val_rmse)\n    trial.set_user_attr(\"cv_rmse\", mean_rmse)\n\n    if mean_rmse &lt; best_score:\n        best_score = mean_rmse\n        global_oof_preds[:] = oof_preds\n\n    print(f\" Trial {trial.number} | CV RMSE: {mean_rmse:,.2f}\")\n    return mean_rmse\n\n# === STEP 2: Run Optuna ===\nstudy = optuna.create_study(\n    direction='minimize',\n    study_name='lgbm_study_final_with_shap',\n    storage='sqlite:///lgbm_study_final_with_shap.db',\n    load_if_exists=True,\n    pruner=SuccessiveHalvingPruner(min_resource=100, reduction_factor=2)\n)\nstudy.optimize(objective, n_trials=25, show_progress_bar=True)\n\nprint(\" Best RMSE:\", study.best_value)\nprint(\" Best Parameters:\", study.best_params)\nnp.save(\"oof_preds_lgbm.npy\", global_oof_preds)\nprint(\" Saved: oof_preds_lgbm.npy\")\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import root_mean_squared_error\nfrom optuna.integration import XGBoostPruningCallback\nfrom shap import TreeExplainer\n\n# === STEP 0: Prepare Data ===\nX_full = train_merged.copy()\ny_full = pd.Series(y_train)\nX_test = test_merged.copy()\n\nbin_cols = [\n    'building_value_growth_bin',\n    'assessed_growth_bin',\n    'land_value_growth_bin','year_built_bin'\n]\n\nfor col in bin_cols:\n    X_full[col] = X_full[col].cat.codes\n    X_test[col] = X_test[col].cat.codes\n\ncategorical_cols = X_full.select_dtypes(include='object').columns.tolist()\nX_full[categorical_cols] = X_full[categorical_cols].astype('category')\nX_test[categorical_cols] = X_test[categorical_cols].astype('category')\n\n# === Global OOF Tracker ===\nglobal_oof_preds = np.zeros(len(X_full))\nbest_score = float(\"inf\")\n\n# === STEP 1: Optuna Objective Function (No SHAP during tuning) ===\ndef objective(trial):\n    global global_oof_preds, best_score\n\n    params = {\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"tree_method\": \"hist\",\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.047, 0.05, log=True),\n        \"max_depth\": 6,\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 11, 12),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.87, 0.89),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.74),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.30, 0.56, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 0.11, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 1.1, 4.3),\n        \"n_estimators\": 1000,\n        \"n_jobs\": -1,\n        \"enable_categorical\": True,\n    }\n\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(X_full))\n    fold_rmse = []\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n        X_train, y_train_fold = X_full.iloc[train_idx], y_full.iloc[train_idx]\n        X_val, y_val = X_full.iloc[val_idx], y_full.iloc[val_idx]\n\n        model = XGBRegressor(\n            **params,\n            early_stopping_rounds=100,\n            callbacks=[XGBoostPruningCallback(trial, \"validation_0-rmse\"),\n                       ]\n        )\n        model.fit(X_train, y_train_fold, eval_set=[(X_val, y_val)], verbose=100)\n\n        val_pred = model.predict(X_val)\n        oof_preds[val_idx] = val_pred\n        fold_rmse.append(root_mean_squared_error(y_val, val_pred))\n\n    mean_rmse = np.mean(fold_rmse)\n    trial.set_user_attr(\"cv_rmse\", mean_rmse)\n\n    if mean_rmse &lt; best_score:\n        best_score = mean_rmse\n        global_oof_preds[:] = oof_preds\n\n    print(f\" Trial {trial.number} | CV RMSE: {mean_rmse:,.2f}\")\n    return mean_rmse\n\n# === STEP 2: Run Optuna ===\nstudy = optuna.create_study(\n    direction='minimize',\n    study_name='xgbreg_optuna_final_no_shap',\n    pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=100, reduction_factor=2)\n)\nstudy.optimize(objective, n_trials=25, show_progress_bar=True)\n\nprint(\" Best RMSE:\", study.best_value)\nprint(\" Best Parameters:\", study.best_params)\nnp.save(\"oof_preds_xgbreg.npy\", global_oof_preds)\nprint(\" Saved: oof_preds_xgbreg.npy\")\n\n\nStep 2: SHAP and Gain-Based Feature Selection. After tuning, we trained new LightGBM and XGBoost models using the best parameters on each fold of the training data. For each fold, we computed SHAP importance values and LightGBM/XGBoost gain importances. We retained features that collectively accounted for 95% of total importance in either SHAP or gain, and constructed a union of these high-signal features across all folds. This union was used to define the final reduced feature space for retraining.\n\n\nCode\n# === STEP 3: SHAP + GAIN Feature Selection for LGBM ===\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nselected_feature_sets = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n    X_train_raw, y_train_fold = X_full.iloc[train_idx], y_full.iloc[train_idx]\n\n    train_dataset = lgb.Dataset(X_train_raw, label=y_train_fold, categorical_feature=cat_cols)\n    model_temp = lgb.train(\n        study.best_params,\n        train_dataset,\n        num_boost_round=200,\n        valid_sets=[train_dataset],\n        callbacks=[log_evaluation(period=100)] \n    )\n\n    # SHAP importance\n    explainer = shap.TreeExplainer(model_temp)\n    shap_values = explainer.shap_values(X_train_raw)\n    shap_df = pd.DataFrame(np.abs(shap_values), columns=X_train_raw.columns)\n    shap_importance = shap_df.mean().sort_values(ascending=False)\n    shap_cumsum = shap_importance.cumsum() / shap_importance.sum()\n    top_shap = shap_cumsum[shap_cumsum &lt;= 0.95].index.tolist()\n\n    # Gain importance\n    gain_importance = pd.Series(model_temp.feature_importance(importance_type='gain'), index=X_train_raw.columns)\n    gain_sorted = gain_importance.sort_values(ascending=False)\n    gain_cumsum = gain_sorted.cumsum() / gain_sorted.sum()\n    top_gain = gain_cumsum[gain_cumsum &lt;= 0.95].index.tolist()\n\n    selected_features = list(set(top_shap).union(set(top_gain)))\n    selected_feature_sets.append(selected_features)\n\n# === STEP 4: Final Feature Union ===\nfinal_union_features = list(set().union(*selected_feature_sets))\nprint(\" Final Union Feature Count:\", len(final_union_features))\n\n# Filter only those categorical columns that are in final features\nfiltered_cat_cols = [col for col in cat_cols if col in final_union_features]\n\n\n\n\nCode\n# === STEP 3: Post-Optuna SHAP + Gain Feature Selection for XGBoost ===\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nselected_feature_sets = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n    X_train_raw, y_train_fold = X_full.iloc[train_idx], y_full.iloc[train_idx]\n\n    model_temp = XGBRegressor(**study.best_params, n_estimators=200)\n    model_temp.fit(X_train_raw, y_train_fold)\n\n    # === SHAP Importance ===\n    explainer = TreeExplainer(model_temp)\n    shap_values = explainer.shap_values(X_train_raw)\n    shap_df = pd.DataFrame(np.abs(shap_values), columns=X_train_raw.columns)\n    shap_importance = shap_df.mean().sort_values(ascending=False)\n    shap_cumsum = shap_importance.cumsum() / shap_importance.sum()\n    top_shap = shap_cumsum[shap_cumsum &lt;= 0.95].index.tolist()\n\n    # === Gain Importance ===\n    gain_importance = pd.Series(model_temp.feature_importances_, index=X_train_raw.columns)\n    gain_sorted = gain_importance.sort_values(ascending=False)\n    gain_cumsum = gain_sorted.cumsum() / gain_sorted.sum()\n    top_gain = gain_cumsum[gain_cumsum &lt;= 0.95].index.tolist()\n\n    selected_features = list(set(top_shap).union(set(top_gain)))\n    selected_feature_sets.append(selected_features)\n\n# === STEP 4: Final Feature Union ===\nfinal_union_features = list(set().union(*selected_feature_sets))\nprint(\" Final Union Feature Count:\", len(final_union_features))\n\n\nStep 3: Final Model Training and Inference. Each final model was retrained on the full training set using only the selected features, with early stopping enabled to prevent overfitting. Predictions on the test set were generated using the best iteration count. All model outputs—including OOF predictions and test forecasts—were saved for submission and later use in ensemble blending.\n\n\nCode\n# === STEP 5: Final Model on Selected Features for LGBM ===\nX_full_selected = X_full[final_union_features]\nX_test_selected = X_test[final_union_features]\n\n\nfinal_dataset = lgb.Dataset(X_full_selected, label=y_full, categorical_feature=filtered_cat_cols)\nfinal_model = lgb.train(\n    study.best_params,\n    final_dataset,\n    num_boost_round=1000,\n    valid_sets=[final_dataset],\n    valid_names=[\"train\"],\n    callbacks=[log_evaluation(period=100)]\n)\n\n# === STEP 6: Predict on Test Set ===\ntest_preds = final_model.predict(X_test_selected, num_iteration=final_model.best_iteration)\nnp.save(\"test_preds_lgbm_shap.npy\", test_preds)\nprint(\" Saved: test_preds_lgbm_shap.npy\")\n\n# === STEP 7: Save Submission ===\nsubmission = pd.DataFrame({\n    'ACCOUNT': acct_test.values.ravel(),  # Replace with your ID col\n    'TARGET': test_preds\n})\nsubmission.to_csv(\"submission_lgbm_shap.csv\", index=False)\nprint(\" Submission saved: submission_lgbm_shap.csv\")\n\n\n\n\nCode\n# === STEP 5: Final Model on Selected Features for XGBoost ===\nX_full_selected = X_full[final_union_features]\nX_test_selected = X_test[final_union_features]\n\nfinal_model = XGBRegressor(**study.best_params)\nfinal_model.set_params(n_estimators=1000, verbosity=1, early_stopping_rounds=100)\nfinal_model.fit(X_full_selected, y_full, eval_set=[(X_full_selected, y_full)], verbose=100)\n\n# === STEP 6: Predict on Test Set ===\ntest_preds = final_model.predict(X_test_selected)\nnp.save(\"test_preds_xgbreg.npy\", test_preds)\nprint(\" Saved: test_preds_xgbreg.npy\")\n\n# === STEP 7: Create Submission File ===\naccount_ids = acct_test.values.ravel()  # Replace with actual ID column\nsubmission = pd.DataFrame({\n    'ACCOUNT': account_ids,\n    'TARGET': test_preds\n})\nsubmission.to_csv(\"submission_xgbreg.csv\", index=False)\nprint(\" Submission saved: submission_xgbreg.csv\")\n\n\nThis hybrid approach—combining Optuna-based tuning with SHAP-driven interpretability—allowed us to retain only high-impact features, thereby improving generalization and reducing overfitting without sacrificing performance. The best out-of-fold RMSE achieved was 40,925.29 with XGBoost and 41,641.42 with LightGBM, confirming the robustness of both pipelines."
  },
  {
    "objectID": "project3.html#weighted-model-blending-with-optuna",
    "href": "project3.html#weighted-model-blending-with-optuna",
    "title": "STA 9890 Project: Property Valuation",
    "section": "5.1 Weighted Model Blending with Optuna",
    "text": "5.1 Weighted Model Blending with Optuna\nTo consolidate the strengths of our top-performing base models—XGBoost, RidgeCV, and LightGBM—we employed a weighted blending strategy optimized using Optuna. This approach directly searched for the optimal linear combination of model predictions that minimized RMSE on a holdout set.\nWe first constructed a meta-training set consisting of out-of-fold (OOF) predictions from each base model. A corresponding test matrix was constructed from each model’s final test predictions. The blending weights were constrained to be non-negative and normalized to sum to one.\nAn Optuna study was run for 100 trials, where each trial proposed a new set of blending weights and evaluated their performance via RMSE on the holdout split. The final optimized weights were:\n\nXGBoost: w₀ = 25.98%\nRidgeCV: w₁ = 33.53%\nLightGBM: w₂ = 40.49%\n\nThese weights were then used to produce a final blended prediction for the test set. The resulting predictions achieved an RMSE of 36,239.91 on the holdout set—outperforming all individual base models and demonstrating the value of combining linear and tree-based perspectives.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport logging\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# === Setup Logging ===\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(\"OptunaBlender\")\noptuna.logging.set_verbosity(optuna.logging.INFO)\n\n# === Load base model predictions ===\noof_xgb = np.load(\"oof_preds_xgbreg.npy\")\nridge_oof = np.load(\"ridgecv_oof_preds.npy\")\noof_lgb=np.load(\"oof_preds_lgbm.npy\")\ntest_xgb = np.load(\"test_preds_xgbreg.npy\")\nridge_test_preds = np.load(\"ridgecv_test_preds.npy\")\ntest_lgb=np.load(\"test_preds_lgbm_shap.npy\")\n# === Targets and prediction stack ===\ny_meta = train['TARGET'].values\nX_base = np.vstack([oof_xgb, ridge_oof,oof_lgb]).T\nX_test_base = np.vstack([test_xgb, ridge_test_preds,test_lgb]).T\n\n# === Holdout split ===\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_base, y_meta, test_size=0.2, random_state=42)\n\n# === Objective Function ===\ndef objective(trial):\n    weights = [trial.suggest_float(f\"w{i}\", 0, 1) for i in range(X_train.shape[1])]\n    weights = np.array(weights)\n    weights /= weights.sum()  # normalize\n\n    preds = X_holdout @ weights\n    rmse = root_mean_squared_error(y_holdout, preds)\n\n    logger.info(f\"Trial {trial.number} | Weights: {np.round(weights, 3).tolist()} | RMSE: {rmse:,.4f}\")\n    return rmse\n\n# === Run Study ===\nlogger.info(\" Starting Optuna optimization for weighted blending...\")\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\n\n# === Best weights ===\nbest_weights = np.array([study.best_trial.params[f\"w{i}\"] for i in range(X_base.shape[1])])\nbest_weights /= best_weights.sum()\nlogger.info(f\" Best weights: {np.round(best_weights, 4)}\")\nlogger.info(f\" Best RMSE: {study.best_value:.4f}\")\n\n# === Final test prediction ===\nmeta_preds = X_test_base @ best_weights\n\n# === Save predictions ===\nnp.save(\"test_preds_optuna_blended.npy\", meta_preds)\naccount_ids = acct_test.values.ravel()\nsubmission = pd.DataFrame({\n    \"ACCOUNT\": account_ids,\n    \"TARGET\": meta_preds\n})\nsubmission.to_csv(\"submission_optuna_blended.csv\", index=False)\nlogger.info(\" Saved: test_preds_optuna_blended.npy and submission_optuna_blended.csv\")"
  },
  {
    "objectID": "project3.html#stacked-ensembling-with-elasticnetcv",
    "href": "project3.html#stacked-ensembling-with-elasticnetcv",
    "title": "STA 9890 Project: Property Valuation",
    "section": "5.2 Stacked Ensembling with ElasticNetCV",
    "text": "5.2 Stacked Ensembling with ElasticNetCV\nTo complement our Optuna-based weighted average ensemble, we implemented a stacked generalization approach using ElasticNetCV as a meta-learner. This method treats out-of-fold (OOF) predictions from the base models—XGBoost, RidgeCV, and LightGBM—as features in a second-level regression model. By learning how to optimally combine base predictions, the meta-model can capture nonlinear inter-model relationships while applying regularization to prevent overfitting.\nMeta-Model Training. We concatenated the OOF predictions into a 3-column meta-feature matrix and used it to fit an ElasticNetCV model wrapped in a StandardScaler pipeline. The meta-model searched over a grid of l1_ratio and alpha values, using 3-fold cross-validation to identify the optimal regularization configuration.\nHoldout Evaluation. For evaluation, we trained the meta-learner on an 80% split and evaluated on a 20% holdout. The resulting RMSE was 36,344.64, closely aligned with the Optuna-weighted blend. The selected alpha was 0.01, indicating strong regularization and robust coefficient shrinkage.\nFinal Test Predictions. The final model was retrained on the full meta-feature set and used to predict the test set. This stacked approach provided a robust and regularized alternative to linear averaging, automatically downweighting weaker models while maintaining interpretability and reproducibility.\nOn the competition leaderboard, the ElasticNetCV ensemble—combining Ridge, XGBoost, and LightGBM predictions—secured the top rank with a private leaderboard RMSE of 36,021, just 2 points above the public leaderboard score of 36,019. Interestingly, while an Optuna-weighted linear blend achieved a lower public RMSE, it ranked below the ElasticNet ensemble on the final evaluation, underscoring the latter’s generalization strength.\n\n\nCode\nimport numpy as np\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import root_mean_squared_error\n\n# === Load OOF + Test Predictions ===\noof_xgb = np.load(\"oof_preds_xgbreg.npy\")\ntest_xgb = np.load(\"test_preds_xgbreg.npy\")\nridge_oof=np.load(\"ridgecv_oof_preds.npy\")\nridge_test_preds=np.load(\"ridgecv_test_preds.npy\")\noof_lgb=np.load(\"oof_preds_lgbm.npy\")\ntest_lgb=np.load(\"test_preds_lgbm_shap.npy\")\n\n# === 3. Combine full meta-input feature set ===\nX_meta = np.hstack([\n    oof_xgb.reshape(-1, 1),\n    ridge_oof.reshape(-1, 1),\n    oof_lgb.reshape(-1,1)\n])\ny_meta = train['TARGET'].values\n\nX_meta_test = np.hstack([\n    test_xgb.reshape(-1, 1),\n    ridge_test_preds.reshape(-1, 1),\n    test_lgb.reshape(-1,1)\n])\n\n# === 4. Train ElasticNetCV meta-learner ===\nmeta_model = make_pipeline(\n    StandardScaler(),\n    ElasticNetCV(\n        l1_ratio=[0.1, 0.5, 0.9, 1],\n        alphas=np.logspace(-4, 2, 100),\n        cv=3,\n        max_iter=5000,\n        n_jobs=-1\n    )\n)\nmeta_model.fit(X_meta, y_meta)\n\n# === 5. Predict and evaluate (optional holdout split) ===\n# You can skip this section if you're blending on full train\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)\nmeta_model.fit(X_train, y_train)\nholdout_preds = meta_model.predict(X_holdout)\nrmse = root_mean_squared_error(y_holdout, holdout_preds)\nprint(f\"ElasticNetCV Blended Meta Holdout RMSE: {rmse:,.2f}\")\nbest_alpha = meta_model.named_steps['elasticnetcv'].alpha_\nprint(f\" Best alpha selected: {best_alpha}\")\n# === 6. Final predictions for test set ===\nmeta_preds = meta_model.predict(X_meta_test)\n\n# === 7. Save blended test predictions ===\nnp.save(\"test_preds_elasticnet_blended.npy\", meta_preds)\naccount_ids = acct_test.values.ravel() \nsubmission = pd.DataFrame({\n    \"ACCOUNT\": account_ids,  # Replace with your actual ID column\n    \"TARGET\": meta_preds\n})\nsubmission.to_csv(\"submission_elasticnet_blended.csv\", index=False)\nprint(\" ElasticNetCV blended stacking submission saved.\")"
  },
  {
    "objectID": "project3.html#top-predictive-features",
    "href": "project3.html#top-predictive-features",
    "title": "STA 9890 Project: Property Valuation",
    "section": "Top Predictive Features",
    "text": "Top Predictive Features\nSHAP analysis revealed that features like assessed 2018, building value 2018, and land value 2018 were primary drivers of the model’s predictions. Structural attributes such as building area 2019, floor area x grade, and grade 2019 also carried strong explanatory power. These results confirmed the value of engineering ratio and interaction terms that encode economic density, build quality, and age-adjusted valuation. Neighborhood-level variables, especially neigh assess std and frequency encodings, further demonstrated the importance of local context in real estate assessment."
  },
  {
    "objectID": "project3.html#low-impact-features",
    "href": "project3.html#low-impact-features",
    "title": "STA 9890 Project: Property Valuation",
    "section": "Low-Impact Features",
    "text": "Low-Impact Features\nAlthough SHAP-ranked bottom 30 features showed limited average contribution, they were retained via the 95% SHAP + Gain union due to their potential complementary value. Examples include early-year indicators like quality description 2015, fireplaces 2016, and spatial ratios like porch ratio. Their inclusion likely enhanced generalization by supporting edge cases, and their low impact helped confirm that more aggressive feature pruning would have offered little gain.\n\n\n\n\nBottom 30 SHAP Features\n\n\n\n\nTop 30 SHAP Features"
  },
  {
    "objectID": "project3.html#what-worked-well",
    "href": "project3.html#what-worked-well",
    "title": "STA 9890 Project: Property Valuation",
    "section": "What Worked Well",
    "text": "What Worked Well\n\nThe SHAP + Gain union was highly effective in denoising the feature space and avoiding overfitting, retaining only high-impact predictors across folds.\nThe ElasticNet ensemble combined linear and non-linear model strengths to capture nuanced patterns in both well-represented and sparse regions.\nOptuna tuning reduced manual trial-and-error and consistently improved generalization across Ridge, LGBM, and XGBoost pipelines."
  },
  {
    "objectID": "project3.html#what-could-improve",
    "href": "project3.html#what-could-improve",
    "title": "STA 9890 Project: Property Valuation",
    "section": "What Could Improve",
    "text": "What Could Improve\n\nWe did not model temporal dependencies across yearly features—treating them as time-series sequences or using transformer architectures may better account for evolving valuation dynamics over time.\nGeographical variation was only partially captured using frequency encoding. Future work could explore residual stacking or blended stacking techniques with subgroup-aware features (e.g., neighborhood frequency, protest history) to correct localized error patterns more explicitly.\n\nOverall, the final ElasticNet ensemble—integrating Ridge, SHAP-informed LGBM, and Optuna-tuned XGBoost—delivered a strong performance on the private leaderboard. Future extensions may benefit from incorporating time-aware modeling, subgroup-specific residual correction, or spatially informed representations to further improve accuracy and fairness in municipal assessment systems."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "As machine learning (ML) systems become more integrated into our daily lives—from influencing who is hired to determining who qualifies for medical care—questions about fairness become impossible to ignore.\n\n\nML systems learn from historical data, and history is not free of bias. If we are not careful, these models can magnify societal inequalities embedded in that data. Fairness in ML is about trying to prevent that. However, fairness is not a one-size-fits-all concept. Different fields define fairness in different ways:\n\nLaw: Fairness means protecting people from discrimination (e.g., based on race or gender).\nSocial sciences: Focus on systems of power and inequality—who gets advantages and who does not.\nComputer science and statistics: Treat fairness as a math problem—something we can define and measure.\nPhilosophy and political theory: See fairness as a question of justice and moral reasoning.\n\n\n\n\nThe COMPAS Algorithm Used in the US to predict which defendants are likely to reoffend, the COMPAS algorithm was designed to support bail and sentencing decisions. On the surface, it appeared fair—it was equally accurate for black and white defendants. However, an analysis by ProPublica revealed the following:\n\nBlack defendants who would not reoffend were twice as likely to be labeled “high risk.”\nWhite defendants who did reoffend were often mislabeled “low risk.” Although the developers claimed that the model was fair (based on accuracy), it violated fairness in terms of racial bias, reinforcing systemic disparities.\n\nBiased Healthcare Predictions In U.S. hospitals, an algorithm used to prioritize patients for preventative care relied on healthcare spending as a proxy for health needs. However, due to systemic disparities, black patients historically incurred lower medical costs—not due to better health, but due to under-treatment. As a result, the algorithm underestimated their needs, leading to biased care allocation.\n\n\n\nFairness in machine learning is typically categorized into two main types:\n\nGroup Fairness\nIndividual Fairness\n\n\n\nGroup fairness ensures that different demographic groups (e.g., races or genders) are treated similarly by the model. A common metric used to quantify this is Demographic Parity, which aims for equal rates of positive outcomes across groups.\nDemographic Parity Formula\nGiven two groups \\(G_1\\) and \\(G_2\\), and a binary classifier \\(f : \\mathbb{R}^p \\to \\{0, 1\\}\\), the deviation from demographic parity (DDP) is defined as:\n\\[\n\\text{DDP}(f) = \\left| \\mathbb{E}_{x \\sim G_1}[f(x)] - \\mathbb{E}_{x \\sim G_2}[f(x)] \\right|\n\\]\nA DDP of \\(0\\) indicates perfect demographic parity, meaning both groups receive positive outcomes at equal rates.\n\n\n\nIndividual fairness is based on the principle that similar individuals should receive similar outcomes. The main challenge lies in defining what it means for individuals to be “similar.” For example, should two loan applicants with identical incomes be considered similar if they differ in zip code or education level?\nThis definition is highly context-dependent and can be subjective, often requiring domain-specific knowledge to determine appropriate fairness constraints.\n\n\n\n\nTo evaluate fairness in NBA draft selections, we use college basketball data from 2009 to 2021.\n\n\n\nPlayer Performance Data: Points per game (PPG), assists (APG), shooting efficiency, etc.\nDemographics: Height, school name, conference.\nDraft Data: Draft round and pick number for selected players.\n\nWe merge the datasets by player name and year to construct a binary classification task:\nWas this player drafted (1) or not (0)?\n\n\n\nIn a world where physical attributes and college brand often outweigh actual performance, we adopt Equal Opportunity as our fairness metric. In this context:\nPlayers with comparable game statistics should have equal chances of being drafted, regardless of:\n\nThe prestige of their college (e.g., Davidson vs. Duke)\nThe conference in which they played (e.g., Southern vs. ACC)\nTheir physical traits, especially height (e.g., undersized guards often being overlooked)\n\n\n\n\nThe NBA draft process is deeply influenced by legacy scouting practices:\n\nSize Bias: Overemphasis on height and athleticism can overlook highly skilled but smaller players.\nConference Prestige Bias: Players from smaller programs often receive less exposure and fewer opportunities.\n\nAthletes like Steph Curry and Fred VanVleet exemplify players who may be unfairly penalized by traditional models. Our goal is to mitigate this through fairness-aware machine learning that recognizes talent irrespective of institutional pedigree.\n\n\n\nWe begin by merging two datasets: one containing college player statistics (2009–2021) and another listing NBA-drafted players during the same period. To ensure consistency, player names are standardized before matching. A binary ’Drafted‘ label is assigned based on presence in the official draft list.\nTo capture structural disparities across programs, we compute each school’s draft rate (number of players drafted divided by total players). Schools are then assigned to one of three tiers based on their draft rate quantiles:\n\nTier 1: Top 10% schools by draft rate\n\nTier 2: 60th to 90th percentile\n\nTier 3: Bottom 60%\n\nThis tier system enables group fairness evaluation across schools with varying institutional prestige.\n\n\nCode\nimport pandas as pd\nimport re\nimport numpy as np\n# Load data\ncollege_df = pd.read_csv(\"CollegeBasketballPlayers2009-2021.csv\")\ndrafted_df = pd.read_excel(\"DraftedPlayers2009-2021.xlsx\")\n\n# Standardize names for matching\ncollege_df['player_name'] = college_df['player_name'].astype(str).str.strip().str.lower()\ndrafted_df['PLAYER'] = drafted_df['PLAYER'].astype(str).str.strip().str.lower()\n\n# Add Drafted column: 1 if player in drafted list, 0 otherwise\ncollege_df['Drafted'] = college_df['player_name'].isin(drafted_df['PLAYER']).astype(int)\n\ndef convert_excel_date_height(ht_str):\n    # Convert month abbreviations to numbers\n    month_map = {\n        'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04',\n        'may': '05', 'jun': '06', 'jul': '07', 'aug': '08',\n        'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'\n    }\n\n    try:\n        ht_str = str(ht_str).lower()\n        for month, number in month_map.items():\n            ht_str = ht_str.replace(month, number)\n        parts = re.findall(r'\\d+', ht_str)\n        if len(parts) == 2:\n            inches, feet = map(int, parts)\n            return feet * 12 + inches\n    except:\n        pass\n    return np.nan\n\n# Apply to create the height_in column\ncollege_df['height_in'] = college_df['ht'].apply(convert_excel_date_height)\n\n\n\n\nCode\n# --- TIER ASSIGNMENT BASED ON DRAFT RATE ---\nschool_draft_stats = college_df.groupby('team')['Drafted'].agg(['sum', 'count'])\nschool_draft_stats['draft_rate'] = school_draft_stats['sum'] / school_draft_stats['count']\nq90 = school_draft_stats['draft_rate'].quantile(0.90)\nq60 = school_draft_stats['draft_rate'].quantile(0.60)\nschool_draft_stats['school_tier'] = pd.cut(\n    school_draft_stats['draft_rate'],\n    bins=[-1, q60, q90, 1.0],\n    labels=['Tier 3', 'Tier 2', 'Tier 1']\n)\ncollege_df = college_df.merge(school_draft_stats['school_tier'], left_on='team', right_index=True, how='left')\n\n\n\n\n\n\nThe CVX optimization framework was employed to formulate and solve convex models underlying fairness-aware NBA draft prediction, including logistic regression variants (plain, ridge, lasso), support vector machines, and the FairStacks ensemble. By expressing these models as convex optimization problems, CVX enabled rapid prototyping without the need for custom algorithmic implementation.\nAlthough Lasso regression is theoretically designed to drive some coefficients exactly to zero, solutions obtained via general-purpose solvers like CVX often yield near-zero coefficients due to numerical precision limitations. As a result, strict sparsity may require additional post-processing. Nevertheless, the framework provided sufficient flexibility to explore regularization effects and fairness constraints in the context of evaluating how players from less prestigious programs could be assessed more equitably based on game performance rather than institutional pedigree.\n\n\n\n\nThis implementation assumes conditional independence of features and fits Gaussian distributions separately for each class. The model computes:\n\nClass-wise means (\\(\\mu_0\\), \\(\\mu_1\\)) and pooled variance \\(\\sigma^2\\)\nPrior class probabilities (\\(\\pi_0\\), \\(\\pi_1\\))\nLog-likelihoods using the Gaussian log-density function\n\nFor each input vector, the model compares log-posterior probabilities and assigns a label of 1 if the class 1 posterior is greater, and 0 otherwise. This lightweight model is not only computationally simple but also provides useful diversity in the ensemble. Because Naive Bayes is so simple, it acts as a benchmark. If your complex models (like SVM or Lasso) don’t clearly outperform it, you might need to reassess their added complexity or feature use.\n\n\nCode\n# --- MANUAL NAIVE BAYES ---\ndef manual_gaussian_nb(X_train, y_train, X_test):\n    X_0 = X_train[y_train== 0]\n    X_1 = X_train[y_train== 1]\n    mu_0, mu_1 = np.mean(X_0, axis=0), np.mean(X_1, axis=0)\n    pi_0, pi_1 = X_0.shape[0] / len(X_train), X_1.shape[0] / len(X_train)\n    pooled_var = (np.sum((X_0 - mu_0) ** 2, axis=0) + np.sum((X_1 - mu_1) ** 2, axis=0)) / len(X_train)\n    pooled_var += 1e-6\n    def log_gaussian(x, mu, var):\n        return -0.5 * np.sum(np.log(2 * np.pi * var)) - 0.5 * np.sum(((x - mu) ** 2) / var)\n    return np.array([\n        1 if np.log(pi_1) + log_gaussian(x, mu_1, pooled_var) &gt; np.log(pi_0) + log_gaussian(x, mu_0, pooled_var) else 0\n        for x in X_test\n    ])\n\n\n\n\n\nThis implementation assumes Gaussian class-conditional distributions with a shared covariance matrix. LDA serves as a linear classifier that projects data to maximize class separability under these assumptions. The model performs the following:\n\nCalculates class-wise means (\\(\\mu_0\\), \\(\\mu_1\\))\nComputes the shared covariance matrix \\(\\Sigma\\) across both classes\nSolves the linear discriminant function (delta) for each class\n\nEach data point is assigned to the class with the higher discriminant score. LDA offers a more flexible alternative to Naive Bayes by relaxing the independence assumption while still providing a computationally efficient, interpretable classifier.\n\n\nCode\n# --- MANUAL LDA ---\ndef manual_lda(X_train, y_train, X_test):\n    X_0 = X_train[y_train == 0]\n    X_1 = X_train[y_train == 1]\n    mu_0, mu_1 = np.mean(X_0, axis=0), np.mean(X_1, axis=0)\n    pi_0, pi_1 = X_0.shape[0] / len(X_train), X_1.shape[0] / len(X_train)\n\n    # Regularized shared covariance matrix\n    Sigma = ((X_0 - mu_0).T @ (X_0 - mu_0) + (X_1 - mu_1).T @ (X_1 - mu_1)) / (len(X_train) - 2)\n    Sigma += 1e-6 * np.eye(Sigma.shape[0])  # small regularization to ensure invertibility\n\n    def delta(x, mu, pi):\n        a = np.linalg.solve(Sigma, mu)\n        return x @ a - 0.5 * mu.T @ a + np.log(pi)\n\n    return np.array([\n        1 if delta(x, mu_1, pi_1) &gt; delta(x, mu_0, pi_0) else 0\n        for x in X_test\n    ])\n\n\n\n\n\nTo model the probability of being drafted as a logistic function of player statistics, logistic regression was implemented using convex optimization. Three variants were considered: plain logistic regression, ridge-regularization, and lasso-regularization. The model minimizes the average logistic loss, with optional \\(\\ell_2\\) or \\(\\ell_1\\) penalty:\nPlain: Minimizes standard logistic loss\nRidge: Adds \\(\\ell_2\\) norm penalty to control weight magnitude\nLasso: Adds \\(\\ell_1\\) norm penalty to encourage sparsity\nThreshold tuning (e.g., using precision-recall curves) was applied to improve model calibration, especially when the dataset is imbalanced. A tuned threshold (e.g., 0.6) helps balance precision and recall based on evaluation metrics.\nA regularization strength of \\(\\lambda = 0.1\\) was selected as a reasonable value to balance overfitting and underfitting. It is strong enough to stabilize the solution but not so aggressive that it suppresses informative features.\n\n\nCode\ndef logistic_cvx_preds_binary(X, y, X_eval, reg_type=None, lambda_=0.1, thresh=0.6):\n    n, p = X.shape\n    beta = cp.Variable((p, 1))\n    y_ = y.reshape(-1, 1)\n    loss = cp.sum(cp.multiply(-y_, X @ beta) + cp.logistic(X @ beta))/n\n    if reg_type == 'ridge':\n        penalty = lambda_ * cp.norm2(beta)**2\n    elif reg_type == 'lasso':\n        penalty = lambda_ * cp.norm1(beta)\n    else:\n        penalty = 0\n    cp.Problem(cp.Minimize(loss + penalty)).solve()\n    probs = 1 / (1 + np.exp(-X_eval @ beta.value))\n    return (probs &gt;= thresh).astype(int).flatten()\n\n\n\n\n\nROC and Precision-Recall Curves for Plain, Ridge, and Lasso Logistic Regression(Best Threshold = 0.60)\n\n\n\n\n\nThis implementation was developed using convex optimization with hinge loss and \\(\\ell_2\\) regularization. This approach seeks to maximize the margin between classes while penalizing misclassifications near the decision boundary. The model performs the following:\n\nComputes hinge loss \\(\\max(0, 1 - y_i x_i^\\top \\beta)\\) for each point\nAdds \\(\\ell_2\\) regularization to prevent overfitting\nConverts predictions from raw margin scores to 0/1 labels via thresholding\n\nA decision threshold of 1.52 was selected empirically based on precision-recall and ROC curves to optimize performance. This allowed the classifier to maintain conservative positive predictions, especially useful in a fairness-aware pipeline where over-selection of dominant group members must be mitigated. A regularization parameter of \\(\\lambda = 0.1\\) was chosen to softly penalize large weights while preserving flexibility in the decision boundary. This value was found to offer a stable trade-off between underfitting and overfitting in the fairness setting.\n\n\nCode\ndef manual_svm_hinge(X_train, y_train, X_eval, lambda_=0.1):\n    \"\"\"\n    Train a linear SVM using hinge loss via CVXPY.\n    - y_train should be in {0, 1} → converted internally to {-1, +1}\n    - Returns 0/1 predictions for X_eval\n    \"\"\"\n    y_transformed = 2 * y_train - 1  # convert {0,1} to {-1,+1}\n    n, p = X_train.shape\n    beta = cp.Variable((p, 1))\n    margins = cp.multiply(y_transformed.reshape(-1, 1), X_train @ beta)\n    hinge_loss = cp.sum(cp.pos(1 - margins)) / n\n    reg = lambda_ * cp.norm2(beta)**2\n    problem = cp.Problem(cp.Minimize(hinge_loss + reg))\n    problem.solve()\n    raw_preds = X_eval @ beta.value\n    return (raw_preds &gt;= 1.52).astype(int).flatten()\n\n\n\n\n\nROC and Precision-Recall Curves for SVM(Best Threshold = 1.52)\n\n\n\n\n\nA decision tree classifier was included as a base learner using the built-in DecisionTreeClassifier from scikit-learn. This model makes hierarchical splits in the feature space to arrive at a decision path for each input. The implementation used the class_weight=\"balanced\" argument to address class imbalance, ensuring the tree did not overly favor the majority class.\n\n\nCode\n# --- DECISION TREE (BUILT-IN ALLOWED) ---\ntree_model = DecisionTreeClassifier(class_weight='balanced', random_state=0).fit(X_train, y_train)\ntree_preds = tree_model.predict(X_test)\n\n\n\n\n\n\n\nAs we shift toward a fairness-aware ensemble strategy, it becomes critical to structure the data pipeline in a way that supports robust model selection and unbiased performance evaluation. Before learning ensemble weights or assessing bias mitigation, a principled strategy for dividing our dataset into training, validation, and test splits is essential.\nThis is especially important in the context of our Steph Curry fairness model, where the number of drafted players is relatively small and the fairness metric—True Positive Rate (TPR) gap between players from high-exposure programs (Tier 1) and lesser-known schools (Tier 3)—requires meaningful group-level comparisons. A careful partitioning ensures that the evaluation of how fairly we treat underdog players is accurate and unbiased.\n\n\nCode\nfeatures = ['pts', 'ast', 'stl', 'blk', 'treb', 'height_in', 'TS_per', 'eFG', 'player_name', 'Drafted','school_tier']\ndf = college_df[features].dropna()\n\n# --- SPLIT ---\nX = df.drop(columns=['Drafted', 'player_name'])\ny = df['Drafted']\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42)\n\n\nTo build a model that balances predictive power with fairness, we selected a combination of performance-based and context-aware features. Traditional box score statistics such as points per game, assists, steals, blocks, total rebounds, and shooting efficiency (eFG, TS per) were included to capture a player’s tangible on-court contributions. Additionally, height was included as a proxy for physical attributes that are often overvalued in the draft process.\nMost importantly, we incorporated school tier—a categorical variable based on draft success rates of a player’s college program. This feature serves as the foundation for our fairness assessment, as we aim to detect and mitigate biases that favor players from elite programs (e.g., Duke, Kentucky) over similarly talented players from lesser-known schools (e.g., Davidson). Since our fairness metric is based on TPR disparities across school tiers, this feature is critical for evaluating whether players like Steph Curry, who came from a mid-major program, are systematically undervalued.\nBase Learner Outputs and Stacking Matrix: Each of the seven base learners—Naive Bayes, Linear Discriminant Analysis (LDA), Decision Tree, three logistic regression variants (plain, ridge, lasso), and Support Vector Machine (SVM)—was trained on the training set and evaluated on the validation set. The binary predictions (0/1) of each learner on the validation set were vertically stacked to construct the matrix \\(H_{\\text{val}} \\in \\mathbb{R}^{n \\times m}\\), where \\(n\\) is the number of validation examples and \\(m = 7\\) is the number of base models. This matrix serves as input to the FairStacks optimization routine, allowing us to compute an ensemble prediction as a weighted combination of individual model decisions.\n\n\nCode\n# Apply your functions\nnb_val = manual_gaussian_nb_binary(X_train_raw, y_train.values, X_val_scaled)\nlda_val = manual_lda_binary(X_train_scaled, y_train.values, X_val_scaled)\ntree_val = DecisionTreeClassifier(max_depth=5, class_weight=\"balanced\", random_state=0).fit(X_train_scaled, y_train).predict(X_val_scaled)\nlog_plain_val = logistic_cvx_preds_binary(X_train_scaled, y_train.values, X_val_scaled)\nlog_ridge_val = logistic_cvx_preds_binary(X_train_scaled, y_train.values, X_val_scaled, reg_type='ridge')\nlog_lasso_val = logistic_cvx_preds_binary(X_train_scaled, y_train.values, X_val_scaled, reg_type='lasso')\nsvm_val = manual_svm_hinge(X_train_scaled, y_train.values, X_val_scaled)\n# Build your H matrix using 0/1 predictions\nH_val = np.vstack([nb_val, lda_val, tree_val, log_plain_val, log_ridge_val, log_lasso_val,svm_val]).T\n\n\nFairness Vector Calculation: To estimate the bias of each base learner, we compute the True Positive Rate (TPR) separately for players from Tier 1 schools and those from lower-tier programs. The difference between these TPRs defines the bias for each learner:\n\n\nCode\n# --- Fairness Vector ---\ntier_group = (X_val['school_tier'] == 'Tier 1').astype(int)\ndef compute_tpr_gap(y_true, y_scores, group, threshold=0.5):\n    preds = (y_scores &gt;= threshold).astype(int)\n    mask_1 = group == 1\n    mask_0 = group == 0\n    tpr_1 = np.sum((preds[mask_1] == 1) & (y_true[mask_1] == 1)) / np.sum(y_true[mask_1] == 1)\n    tpr_0 = np.sum((preds[mask_0] == 1) & (y_true[mask_0] == 1)) / np.sum(y_true[mask_0] == 1)\n    return tpr_1 - tpr_0\nb_hat = np.array([\n    compute_tpr_gap(y_val.values, nb_val, tier_group),\n    compute_tpr_gap(y_val.values, lda_val, tier_group),\n    compute_tpr_gap(y_val.values, tree_val, tier_group),\n    compute_tpr_gap(y_val.values, log_plain_val, tier_group),\n    compute_tpr_gap(y_val.values, log_ridge_val, tier_group),\n    compute_tpr_gap(y_val.values, log_lasso_val, tier_group),\n    compute_tpr_gap(y_val.values, svm_val, tier_group)\n])\n\n\nBinary predictions from each model on the validation set are compared against ground truth labels, and group masks are applied to measure how fairly each learner treats players based on their school’s prestige. The resulting vector \\(\\hat{b}_j\\) quantifies these group-level disparities and is used in the fairness penalty of the FairStacks objective.\nEnsemble Weight Optimization via FairStacks:\nWith predictions and corresponding bias scores for each base learner in hand, the FairStacks optimization problem is solved to determine optimal ensemble weights. CVXPY is used to minimize a regularized logistic loss objective, where the penalty term is the absolute weighted sum of TPR gaps across base learners. The constraints ensure that all weights are non-negative and sum to one.\n\\[\n\\min_{\\mathbf{w} \\geq 0} \\ \\frac{1}{n} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-y_i \\cdot \\hat{y}_i) \\right) + \\lambda \\left| \\sum_j w_j \\hat{b}_j \\right|,\n\\quad \\text{subject to} \\quad \\sum_j w_j = 1\n\\]\n\nLogistic Loss Term:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-y_i \\cdot \\hat{y}_i) \\right)\n\\]\nmeasures how well the ensemble predicts the true outcome \\(y_i\\) (drafted or not). Here, \\(\\hat{y}_i = \\sum_j w_j \\hat{f}_j(x_i)\\) is the prediction from a weighted combination of base learners. The logistic loss is smooth and convex, making it ideal for binary classification tasks like draft prediction.\nFairness Penalty:\n\\[\n\\lambda \\left| \\sum_j w_j \\hat{b}_j \\right|\n\\]\npenalizes disparity in treatment between groups—specifically, the absolute ensemble-weighted true positive rate (TPR) gap. Each \\(\\hat{b}_j\\) represents the TPR gap of base learner \\(j\\), and minimizing this term encourages fairness across school tiers.\nConstraints:\nThe weights \\(\\mathbf{w}\\) form a convex combination \\((w_j \\geq 0, \\sum_j w_j = 1)\\), ensuring interpretability and stability.\nHere, \\(\\lambda = 10\\) emphasizes fairness by reducing group disparities between players from high-exposure and lower-tier programs. This balance is crucial in the Steph Curry fairness model, where the goal is to counteract systemic undervaluation of talented players from less visible schools.\n\n\n\nCode\n# --- FairStacks Optimization ---\nw = cp.Variable(H_val.shape[1])\ny_val_bin = 2 * y_val.values - 1\nlog_loss = cp.sum(cp.logistic(-cp.multiply(y_val_bin, H_val @ w))) / len(y_val)\nfairness_penalty = cp.abs(cp.sum(cp.multiply(w, b_hat)))\nlambda_ = 10\nobjective = cp.Minimize(log_loss + lambda_ * fairness_penalty)\nconstraints = [w &gt;= 0, cp.sum(w) == 1]\nprob = cp.Problem(objective, constraints)\nprob.solve()\nfinal_weights = w.value\nprint(\"Final FairStacks Weights:\", final_weights)\n\n#--- Ensemble TPR Gap ---\ntier_group_test = (X_test['school_tier'] == 'Tier 1').astype(int)\nensemble_tpr_gap = compute_tpr_gap(y_test.values, ensemble_test_preds, tier_group_test)\nprint(\"Ensemble Test TPR Gap (Tier 1 vs Others):\", ensemble_tpr_gap)\n\n# --- Accuracy ---\nensemble_accuracy = accuracy_score(y_test, ensemble_test_preds)\nprint(ensemble_accuracy)\n\n\nFinal FairStacks Weights: [1.15064014e-10 1.02497679e-11 3.06854668e-13 5.65404680e-12\n 6.72544450e-11 1.00000000e+00 8.35876761e-12]\nEnsemble Test TPR Gap (Tier 1 vs Others): 0.0\n0.9756941583587377\nThe final ensemble placed nearly all its weight on the Lasso-regularized logistic regression model. This outcome reflects the model’s ability to balance strong individual accuracy with fairness across school tiers. Despite the availability of other learners, including Decision Trees and SVMs, FairStacks effectively zeroed out their contributions to reduce potential TPR gaps. In terms of performance, the FairStacks ensemble matched the highest observed accuracy of 97.57% and achieved a TPR gap of 0.000. This is on par with Lasso regression alone, but with the added assurance that the model was selected through a fairness-aware optimization pipeline. Meanwhile, base learners such as Naive Bayes and LDA, despite respectable accuracy, exhibited substantial TPR gaps (e.g., -0.0256 and -0.0171), suggesting bias toward non-Tier 1 players. FairStacks thus succeeds in its dual objective: it preserves predictive performance while correcting for institutional bias, a key goal in fairness modeling for the NBA draft.\n\n\n\nTPR Gap of Base Learners and FairStacks (Tier 1 vs Others)\n\n\n\n\n\n\nAccuracy of Base Learners and FairStacks (Higher is Better)\n\n\n\n\n\n\nFairStacks Ensemble Weights\n\n\n\n\n\n\nHaas School of Business. What is Fairness?. University of California, Berkeley. Available at: https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf (Accessed April 2025).\nMichael Weylandt. STA9890 Course Notes. Available at: https://michael-weylandt.com/STA9890/notes.html (Accessed April 2025).\nAditya K. College Basketball Players 2009–2021. Kaggle. Available at: https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021 (Accessed April 2025)."
  },
  {
    "objectID": "project2.html#why-fairness-matters",
    "href": "project2.html#why-fairness-matters",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "ML systems learn from historical data, and history is not free of bias. If we are not careful, these models can magnify societal inequalities embedded in that data. Fairness in ML is about trying to prevent that. However, fairness is not a one-size-fits-all concept. Different fields define fairness in different ways:\n\nLaw: Fairness means protecting people from discrimination (e.g., based on race or gender).\nSocial sciences: Focus on systems of power and inequality—who gets advantages and who does not.\nComputer science and statistics: Treat fairness as a math problem—something we can define and measure.\nPhilosophy and political theory: See fairness as a question of justice and moral reasoning."
  },
  {
    "objectID": "project2.html#real-world-examples-of-biased-algorithms",
    "href": "project2.html#real-world-examples-of-biased-algorithms",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "The COMPAS Algorithm Used in the US to predict which defendants are likely to reoffend, the COMPAS algorithm was designed to support bail and sentencing decisions. On the surface, it appeared fair—it was equally accurate for black and white defendants. However, an analysis by ProPublica revealed the following:\n\nBlack defendants who would not reoffend were twice as likely to be labeled “high risk.”\nWhite defendants who did reoffend were often mislabeled “low risk.” Although the developers claimed that the model was fair (based on accuracy), it violated fairness in terms of racial bias, reinforcing systemic disparities.\n\nBiased Healthcare Predictions In U.S. hospitals, an algorithm used to prioritize patients for preventative care relied on healthcare spending as a proxy for health needs. However, due to systemic disparities, black patients historically incurred lower medical costs—not due to better health, but due to under-treatment. As a result, the algorithm underestimated their needs, leading to biased care allocation."
  },
  {
    "objectID": "project2.html#how-do-we-measure-fairness-in-ml",
    "href": "project2.html#how-do-we-measure-fairness-in-ml",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "Fairness in machine learning is typically categorized into two main types:\n\nGroup Fairness\nIndividual Fairness\n\n\n\nGroup fairness ensures that different demographic groups (e.g., races or genders) are treated similarly by the model. A common metric used to quantify this is Demographic Parity, which aims for equal rates of positive outcomes across groups.\nDemographic Parity Formula\nGiven two groups \\(G_1\\) and \\(G_2\\), and a binary classifier \\(f : \\mathbb{R}^p \\to \\{0, 1\\}\\), the deviation from demographic parity (DDP) is defined as:\n\\[\n\\text{DDP}(f) = \\left| \\mathbb{E}_{x \\sim G_1}[f(x)] - \\mathbb{E}_{x \\sim G_2}[f(x)] \\right|\n\\]\nA DDP of \\(0\\) indicates perfect demographic parity, meaning both groups receive positive outcomes at equal rates.\n\n\n\nIndividual fairness is based on the principle that similar individuals should receive similar outcomes. The main challenge lies in defining what it means for individuals to be “similar.” For example, should two loan applicants with identical incomes be considered similar if they differ in zip code or education level?\nThis definition is highly context-dependent and can be subjective, often requiring domain-specific knowledge to determine appropriate fairness constraints."
  },
  {
    "objectID": "project2.html#raw-data-and-fairness-metric-for-nba-draft-selection-using-ml",
    "href": "project2.html#raw-data-and-fairness-metric-for-nba-draft-selection-using-ml",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "To evaluate fairness in NBA draft selections, we use college basketball data from 2009 to 2021.\n\n\n\nPlayer Performance Data: Points per game (PPG), assists (APG), shooting efficiency, etc.\nDemographics: Height, school name, conference.\nDraft Data: Draft round and pick number for selected players.\n\nWe merge the datasets by player name and year to construct a binary classification task:\nWas this player drafted (1) or not (0)?\n\n\n\nIn a world where physical attributes and college brand often outweigh actual performance, we adopt Equal Opportunity as our fairness metric. In this context:\nPlayers with comparable game statistics should have equal chances of being drafted, regardless of:\n\nThe prestige of their college (e.g., Davidson vs. Duke)\nThe conference in which they played (e.g., Southern vs. ACC)\nTheir physical traits, especially height (e.g., undersized guards often being overlooked)\n\n\n\n\nThe NBA draft process is deeply influenced by legacy scouting practices:\n\nSize Bias: Overemphasis on height and athleticism can overlook highly skilled but smaller players.\nConference Prestige Bias: Players from smaller programs often receive less exposure and fewer opportunities.\n\nAthletes like Steph Curry and Fred VanVleet exemplify players who may be unfairly penalized by traditional models. Our goal is to mitigate this through fairness-aware machine learning that recognizes talent irrespective of institutional pedigree.\n\n\n\nWe begin by merging two datasets: one containing college player statistics (2009–2021) and another listing NBA-drafted players during the same period. To ensure consistency, player names are standardized before matching. A binary ’Drafted‘ label is assigned based on presence in the official draft list.\nTo capture structural disparities across programs, we compute each school’s draft rate (number of players drafted divided by total players). Schools are then assigned to one of three tiers based on their draft rate quantiles:\n\nTier 1: Top 10% schools by draft rate\n\nTier 2: 60th to 90th percentile\n\nTier 3: Bottom 60%\n\nThis tier system enables group fairness evaluation across schools with varying institutional prestige.\n\n\nCode\nimport pandas as pd\nimport re\nimport numpy as np\n# Load data\ncollege_df = pd.read_csv(\"CollegeBasketballPlayers2009-2021.csv\")\ndrafted_df = pd.read_excel(\"DraftedPlayers2009-2021.xlsx\")\n\n# Standardize names for matching\ncollege_df['player_name'] = college_df['player_name'].astype(str).str.strip().str.lower()\ndrafted_df['PLAYER'] = drafted_df['PLAYER'].astype(str).str.strip().str.lower()\n\n# Add Drafted column: 1 if player in drafted list, 0 otherwise\ncollege_df['Drafted'] = college_df['player_name'].isin(drafted_df['PLAYER']).astype(int)\n\ndef convert_excel_date_height(ht_str):\n    # Convert month abbreviations to numbers\n    month_map = {\n        'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04',\n        'may': '05', 'jun': '06', 'jul': '07', 'aug': '08',\n        'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'\n    }\n\n    try:\n        ht_str = str(ht_str).lower()\n        for month, number in month_map.items():\n            ht_str = ht_str.replace(month, number)\n        parts = re.findall(r'\\d+', ht_str)\n        if len(parts) == 2:\n            inches, feet = map(int, parts)\n            return feet * 12 + inches\n    except:\n        pass\n    return np.nan\n\n# Apply to create the height_in column\ncollege_df['height_in'] = college_df['ht'].apply(convert_excel_date_height)\n\n\n\n\nCode\n# --- TIER ASSIGNMENT BASED ON DRAFT RATE ---\nschool_draft_stats = college_df.groupby('team')['Drafted'].agg(['sum', 'count'])\nschool_draft_stats['draft_rate'] = school_draft_stats['sum'] / school_draft_stats['count']\nq90 = school_draft_stats['draft_rate'].quantile(0.90)\nq60 = school_draft_stats['draft_rate'].quantile(0.60)\nschool_draft_stats['school_tier'] = pd.cut(\n    school_draft_stats['draft_rate'],\n    bins=[-1, q60, q90, 1.0],\n    labels=['Tier 3', 'Tier 2', 'Tier 1']\n)\ncollege_df = college_df.merge(school_draft_stats['school_tier'], left_on='team', right_index=True, how='left')"
  },
  {
    "objectID": "project2.html#computation---cvx-optimization",
    "href": "project2.html#computation---cvx-optimization",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "The CVX optimization framework was employed to formulate and solve convex models underlying fairness-aware NBA draft prediction, including logistic regression variants (plain, ridge, lasso), support vector machines, and the FairStacks ensemble. By expressing these models as convex optimization problems, CVX enabled rapid prototyping without the need for custom algorithmic implementation.\nAlthough Lasso regression is theoretically designed to drive some coefficients exactly to zero, solutions obtained via general-purpose solvers like CVX often yield near-zero coefficients due to numerical precision limitations. As a result, strict sparsity may require additional post-processing. Nevertheless, the framework provided sufficient flexibility to explore regularization effects and fairness constraints in the context of evaluating how players from less prestigious programs could be assessed more equitably based on game performance rather than institutional pedigree.\n\n\n\n\nThis implementation assumes conditional independence of features and fits Gaussian distributions separately for each class. The model computes:\n\nClass-wise means (\\(\\mu_0\\), \\(\\mu_1\\)) and pooled variance \\(\\sigma^2\\)\nPrior class probabilities (\\(\\pi_0\\), \\(\\pi_1\\))\nLog-likelihoods using the Gaussian log-density function\n\nFor each input vector, the model compares log-posterior probabilities and assigns a label of 1 if the class 1 posterior is greater, and 0 otherwise. This lightweight model is not only computationally simple but also provides useful diversity in the ensemble. Because Naive Bayes is so simple, it acts as a benchmark. If your complex models (like SVM or Lasso) don’t clearly outperform it, you might need to reassess their added complexity or feature use.\n\n\nCode\n# --- MANUAL NAIVE BAYES ---\ndef manual_gaussian_nb(X_train, y_train, X_test):\n    X_0 = X_train[y_train== 0]\n    X_1 = X_train[y_train== 1]\n    mu_0, mu_1 = np.mean(X_0, axis=0), np.mean(X_1, axis=0)\n    pi_0, pi_1 = X_0.shape[0] / len(X_train), X_1.shape[0] / len(X_train)\n    pooled_var = (np.sum((X_0 - mu_0) ** 2, axis=0) + np.sum((X_1 - mu_1) ** 2, axis=0)) / len(X_train)\n    pooled_var += 1e-6\n    def log_gaussian(x, mu, var):\n        return -0.5 * np.sum(np.log(2 * np.pi * var)) - 0.5 * np.sum(((x - mu) ** 2) / var)\n    return np.array([\n        1 if np.log(pi_1) + log_gaussian(x, mu_1, pooled_var) &gt; np.log(pi_0) + log_gaussian(x, mu_0, pooled_var) else 0\n        for x in X_test\n    ])\n\n\n\n\n\nThis implementation assumes Gaussian class-conditional distributions with a shared covariance matrix. LDA serves as a linear classifier that projects data to maximize class separability under these assumptions. The model performs the following:\n\nCalculates class-wise means (\\(\\mu_0\\), \\(\\mu_1\\))\nComputes the shared covariance matrix \\(\\Sigma\\) across both classes\nSolves the linear discriminant function (delta) for each class\n\nEach data point is assigned to the class with the higher discriminant score. LDA offers a more flexible alternative to Naive Bayes by relaxing the independence assumption while still providing a computationally efficient, interpretable classifier.\n\n\nCode\n# --- MANUAL LDA ---\ndef manual_lda(X_train, y_train, X_test):\n    X_0 = X_train[y_train == 0]\n    X_1 = X_train[y_train == 1]\n    mu_0, mu_1 = np.mean(X_0, axis=0), np.mean(X_1, axis=0)\n    pi_0, pi_1 = X_0.shape[0] / len(X_train), X_1.shape[0] / len(X_train)\n\n    # Regularized shared covariance matrix\n    Sigma = ((X_0 - mu_0).T @ (X_0 - mu_0) + (X_1 - mu_1).T @ (X_1 - mu_1)) / (len(X_train) - 2)\n    Sigma += 1e-6 * np.eye(Sigma.shape[0])  # small regularization to ensure invertibility\n\n    def delta(x, mu, pi):\n        a = np.linalg.solve(Sigma, mu)\n        return x @ a - 0.5 * mu.T @ a + np.log(pi)\n\n    return np.array([\n        1 if delta(x, mu_1, pi_1) &gt; delta(x, mu_0, pi_0) else 0\n        for x in X_test\n    ])\n\n\n\n\n\nTo model the probability of being drafted as a logistic function of player statistics, logistic regression was implemented using convex optimization. Three variants were considered: plain logistic regression, ridge-regularization, and lasso-regularization. The model minimizes the average logistic loss, with optional \\(\\ell_2\\) or \\(\\ell_1\\) penalty:\nPlain: Minimizes standard logistic loss\nRidge: Adds \\(\\ell_2\\) norm penalty to control weight magnitude\nLasso: Adds \\(\\ell_1\\) norm penalty to encourage sparsity\nThreshold tuning (e.g., using precision-recall curves) was applied to improve model calibration, especially when the dataset is imbalanced. A tuned threshold (e.g., 0.6) helps balance precision and recall based on evaluation metrics.\nA regularization strength of \\(\\lambda = 0.1\\) was selected as a reasonable value to balance overfitting and underfitting. It is strong enough to stabilize the solution but not so aggressive that it suppresses informative features.\n\n\nCode\ndef logistic_cvx_preds_binary(X, y, X_eval, reg_type=None, lambda_=0.1, thresh=0.6):\n    n, p = X.shape\n    beta = cp.Variable((p, 1))\n    y_ = y.reshape(-1, 1)\n    loss = cp.sum(cp.multiply(-y_, X @ beta) + cp.logistic(X @ beta))/n\n    if reg_type == 'ridge':\n        penalty = lambda_ * cp.norm2(beta)**2\n    elif reg_type == 'lasso':\n        penalty = lambda_ * cp.norm1(beta)\n    else:\n        penalty = 0\n    cp.Problem(cp.Minimize(loss + penalty)).solve()\n    probs = 1 / (1 + np.exp(-X_eval @ beta.value))\n    return (probs &gt;= thresh).astype(int).flatten()\n\n\n\n\n\nROC and Precision-Recall Curves for Plain, Ridge, and Lasso Logistic Regression(Best Threshold = 0.60)\n\n\n\n\n\nThis implementation was developed using convex optimization with hinge loss and \\(\\ell_2\\) regularization. This approach seeks to maximize the margin between classes while penalizing misclassifications near the decision boundary. The model performs the following:\n\nComputes hinge loss \\(\\max(0, 1 - y_i x_i^\\top \\beta)\\) for each point\nAdds \\(\\ell_2\\) regularization to prevent overfitting\nConverts predictions from raw margin scores to 0/1 labels via thresholding\n\nA decision threshold of 1.52 was selected empirically based on precision-recall and ROC curves to optimize performance. This allowed the classifier to maintain conservative positive predictions, especially useful in a fairness-aware pipeline where over-selection of dominant group members must be mitigated. A regularization parameter of \\(\\lambda = 0.1\\) was chosen to softly penalize large weights while preserving flexibility in the decision boundary. This value was found to offer a stable trade-off between underfitting and overfitting in the fairness setting.\n\n\nCode\ndef manual_svm_hinge(X_train, y_train, X_eval, lambda_=0.1):\n    \"\"\"\n    Train a linear SVM using hinge loss via CVXPY.\n    - y_train should be in {0, 1} → converted internally to {-1, +1}\n    - Returns 0/1 predictions for X_eval\n    \"\"\"\n    y_transformed = 2 * y_train - 1  # convert {0,1} to {-1,+1}\n    n, p = X_train.shape\n    beta = cp.Variable((p, 1))\n    margins = cp.multiply(y_transformed.reshape(-1, 1), X_train @ beta)\n    hinge_loss = cp.sum(cp.pos(1 - margins)) / n\n    reg = lambda_ * cp.norm2(beta)**2\n    problem = cp.Problem(cp.Minimize(hinge_loss + reg))\n    problem.solve()\n    raw_preds = X_eval @ beta.value\n    return (raw_preds &gt;= 1.52).astype(int).flatten()\n\n\n\n\n\nROC and Precision-Recall Curves for SVM(Best Threshold = 1.52)\n\n\n\n\n\nA decision tree classifier was included as a base learner using the built-in DecisionTreeClassifier from scikit-learn. This model makes hierarchical splits in the feature space to arrive at a decision path for each input. The implementation used the class_weight=\"balanced\" argument to address class imbalance, ensuring the tree did not overly favor the majority class.\n\n\nCode\n# --- DECISION TREE (BUILT-IN ALLOWED) ---\ntree_model = DecisionTreeClassifier(class_weight='balanced', random_state=0).fit(X_train, y_train)\ntree_preds = tree_model.predict(X_test)"
  },
  {
    "objectID": "project2.html#implementation-and-assessment-of-fairstacks",
    "href": "project2.html#implementation-and-assessment-of-fairstacks",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "As we shift toward a fairness-aware ensemble strategy, it becomes critical to structure the data pipeline in a way that supports robust model selection and unbiased performance evaluation. Before learning ensemble weights or assessing bias mitigation, a principled strategy for dividing our dataset into training, validation, and test splits is essential.\nThis is especially important in the context of our Steph Curry fairness model, where the number of drafted players is relatively small and the fairness metric—True Positive Rate (TPR) gap between players from high-exposure programs (Tier 1) and lesser-known schools (Tier 3)—requires meaningful group-level comparisons. A careful partitioning ensures that the evaluation of how fairly we treat underdog players is accurate and unbiased.\n\n\nCode\nfeatures = ['pts', 'ast', 'stl', 'blk', 'treb', 'height_in', 'TS_per', 'eFG', 'player_name', 'Drafted','school_tier']\ndf = college_df[features].dropna()\n\n# --- SPLIT ---\nX = df.drop(columns=['Drafted', 'player_name'])\ny = df['Drafted']\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, stratify=y_trainval, random_state=42)\n\n\nTo build a model that balances predictive power with fairness, we selected a combination of performance-based and context-aware features. Traditional box score statistics such as points per game, assists, steals, blocks, total rebounds, and shooting efficiency (eFG, TS per) were included to capture a player’s tangible on-court contributions. Additionally, height was included as a proxy for physical attributes that are often overvalued in the draft process.\nMost importantly, we incorporated school tier—a categorical variable based on draft success rates of a player’s college program. This feature serves as the foundation for our fairness assessment, as we aim to detect and mitigate biases that favor players from elite programs (e.g., Duke, Kentucky) over similarly talented players from lesser-known schools (e.g., Davidson). Since our fairness metric is based on TPR disparities across school tiers, this feature is critical for evaluating whether players like Steph Curry, who came from a mid-major program, are systematically undervalued.\nBase Learner Outputs and Stacking Matrix: Each of the seven base learners—Naive Bayes, Linear Discriminant Analysis (LDA), Decision Tree, three logistic regression variants (plain, ridge, lasso), and Support Vector Machine (SVM)—was trained on the training set and evaluated on the validation set. The binary predictions (0/1) of each learner on the validation set were vertically stacked to construct the matrix \\(H_{\\text{val}} \\in \\mathbb{R}^{n \\times m}\\), where \\(n\\) is the number of validation examples and \\(m = 7\\) is the number of base models. This matrix serves as input to the FairStacks optimization routine, allowing us to compute an ensemble prediction as a weighted combination of individual model decisions.\n\n\nCode\n# Apply your functions\nnb_val = manual_gaussian_nb_binary(X_train_raw, y_train.values, X_val_scaled)\nlda_val = manual_lda_binary(X_train_scaled, y_train.values, X_val_scaled)\ntree_val = DecisionTreeClassifier(max_depth=5, class_weight=\"balanced\", random_state=0).fit(X_train_scaled, y_train).predict(X_val_scaled)\nlog_plain_val = logistic_cvx_preds_binary(X_train_scaled, y_train.values, X_val_scaled)\nlog_ridge_val = logistic_cvx_preds_binary(X_train_scaled, y_train.values, X_val_scaled, reg_type='ridge')\nlog_lasso_val = logistic_cvx_preds_binary(X_train_scaled, y_train.values, X_val_scaled, reg_type='lasso')\nsvm_val = manual_svm_hinge(X_train_scaled, y_train.values, X_val_scaled)\n# Build your H matrix using 0/1 predictions\nH_val = np.vstack([nb_val, lda_val, tree_val, log_plain_val, log_ridge_val, log_lasso_val,svm_val]).T\n\n\nFairness Vector Calculation: To estimate the bias of each base learner, we compute the True Positive Rate (TPR) separately for players from Tier 1 schools and those from lower-tier programs. The difference between these TPRs defines the bias for each learner:\n\n\nCode\n# --- Fairness Vector ---\ntier_group = (X_val['school_tier'] == 'Tier 1').astype(int)\ndef compute_tpr_gap(y_true, y_scores, group, threshold=0.5):\n    preds = (y_scores &gt;= threshold).astype(int)\n    mask_1 = group == 1\n    mask_0 = group == 0\n    tpr_1 = np.sum((preds[mask_1] == 1) & (y_true[mask_1] == 1)) / np.sum(y_true[mask_1] == 1)\n    tpr_0 = np.sum((preds[mask_0] == 1) & (y_true[mask_0] == 1)) / np.sum(y_true[mask_0] == 1)\n    return tpr_1 - tpr_0\nb_hat = np.array([\n    compute_tpr_gap(y_val.values, nb_val, tier_group),\n    compute_tpr_gap(y_val.values, lda_val, tier_group),\n    compute_tpr_gap(y_val.values, tree_val, tier_group),\n    compute_tpr_gap(y_val.values, log_plain_val, tier_group),\n    compute_tpr_gap(y_val.values, log_ridge_val, tier_group),\n    compute_tpr_gap(y_val.values, log_lasso_val, tier_group),\n    compute_tpr_gap(y_val.values, svm_val, tier_group)\n])\n\n\nBinary predictions from each model on the validation set are compared against ground truth labels, and group masks are applied to measure how fairly each learner treats players based on their school’s prestige. The resulting vector \\(\\hat{b}_j\\) quantifies these group-level disparities and is used in the fairness penalty of the FairStacks objective.\nEnsemble Weight Optimization via FairStacks:\nWith predictions and corresponding bias scores for each base learner in hand, the FairStacks optimization problem is solved to determine optimal ensemble weights. CVXPY is used to minimize a regularized logistic loss objective, where the penalty term is the absolute weighted sum of TPR gaps across base learners. The constraints ensure that all weights are non-negative and sum to one.\n\\[\n\\min_{\\mathbf{w} \\geq 0} \\ \\frac{1}{n} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-y_i \\cdot \\hat{y}_i) \\right) + \\lambda \\left| \\sum_j w_j \\hat{b}_j \\right|,\n\\quad \\text{subject to} \\quad \\sum_j w_j = 1\n\\]\n\nLogistic Loss Term:\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left( 1 + \\exp(-y_i \\cdot \\hat{y}_i) \\right)\n\\]\nmeasures how well the ensemble predicts the true outcome \\(y_i\\) (drafted or not). Here, \\(\\hat{y}_i = \\sum_j w_j \\hat{f}_j(x_i)\\) is the prediction from a weighted combination of base learners. The logistic loss is smooth and convex, making it ideal for binary classification tasks like draft prediction.\nFairness Penalty:\n\\[\n\\lambda \\left| \\sum_j w_j \\hat{b}_j \\right|\n\\]\npenalizes disparity in treatment between groups—specifically, the absolute ensemble-weighted true positive rate (TPR) gap. Each \\(\\hat{b}_j\\) represents the TPR gap of base learner \\(j\\), and minimizing this term encourages fairness across school tiers.\nConstraints:\nThe weights \\(\\mathbf{w}\\) form a convex combination \\((w_j \\geq 0, \\sum_j w_j = 1)\\), ensuring interpretability and stability.\nHere, \\(\\lambda = 10\\) emphasizes fairness by reducing group disparities between players from high-exposure and lower-tier programs. This balance is crucial in the Steph Curry fairness model, where the goal is to counteract systemic undervaluation of talented players from less visible schools.\n\n\n\nCode\n# --- FairStacks Optimization ---\nw = cp.Variable(H_val.shape[1])\ny_val_bin = 2 * y_val.values - 1\nlog_loss = cp.sum(cp.logistic(-cp.multiply(y_val_bin, H_val @ w))) / len(y_val)\nfairness_penalty = cp.abs(cp.sum(cp.multiply(w, b_hat)))\nlambda_ = 10\nobjective = cp.Minimize(log_loss + lambda_ * fairness_penalty)\nconstraints = [w &gt;= 0, cp.sum(w) == 1]\nprob = cp.Problem(objective, constraints)\nprob.solve()\nfinal_weights = w.value\nprint(\"Final FairStacks Weights:\", final_weights)\n\n#--- Ensemble TPR Gap ---\ntier_group_test = (X_test['school_tier'] == 'Tier 1').astype(int)\nensemble_tpr_gap = compute_tpr_gap(y_test.values, ensemble_test_preds, tier_group_test)\nprint(\"Ensemble Test TPR Gap (Tier 1 vs Others):\", ensemble_tpr_gap)\n\n# --- Accuracy ---\nensemble_accuracy = accuracy_score(y_test, ensemble_test_preds)\nprint(ensemble_accuracy)\n\n\nFinal FairStacks Weights: [1.15064014e-10 1.02497679e-11 3.06854668e-13 5.65404680e-12\n 6.72544450e-11 1.00000000e+00 8.35876761e-12]\nEnsemble Test TPR Gap (Tier 1 vs Others): 0.0\n0.9756941583587377\nThe final ensemble placed nearly all its weight on the Lasso-regularized logistic regression model. This outcome reflects the model’s ability to balance strong individual accuracy with fairness across school tiers. Despite the availability of other learners, including Decision Trees and SVMs, FairStacks effectively zeroed out their contributions to reduce potential TPR gaps. In terms of performance, the FairStacks ensemble matched the highest observed accuracy of 97.57% and achieved a TPR gap of 0.000. This is on par with Lasso regression alone, but with the added assurance that the model was selected through a fairness-aware optimization pipeline. Meanwhile, base learners such as Naive Bayes and LDA, despite respectable accuracy, exhibited substantial TPR gaps (e.g., -0.0256 and -0.0171), suggesting bias toward non-Tier 1 players. FairStacks thus succeeds in its dual objective: it preserves predictive performance while correcting for institutional bias, a key goal in fairness modeling for the NBA draft.\n\n\n\nTPR Gap of Base Learners and FairStacks (Tier 1 vs Others)\n\n\n\n\n\n\nAccuracy of Base Learners and FairStacks (Higher is Better)\n\n\n\n\n\n\nFairStacks Ensemble Weights"
  },
  {
    "objectID": "project2.html#references",
    "href": "project2.html#references",
    "title": "STA 9890 Project: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "Haas School of Business. What is Fairness?. University of California, Berkeley. Available at: https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf (Accessed April 2025).\nMichael Weylandt. STA9890 Course Notes. Available at: https://michael-weylandt.com/STA9890/notes.html (Accessed April 2025).\nAditya K. College Basketball Players 2009–2021. Kaggle. Available at: https://www.kaggle.com/datasets/adityak2003/college-basketball-players-20092021 (Accessed April 2025)."
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "",
    "text": "The bias-variance trade-off is an important concept used to optimize performance out of machine learning models. The total error of a model can be decomposed into Reducible error and Irreducible error respectively. Bias and variance are reducible errors. The irreducible error, often known as noise, is the error that can’t be reduced. The best models will always have an error which can’t be removed. Therefore, there is a trade-off between bias and variance to decrease the reducible error which essentially minimizes the total error. The total error can be expressed as the following:\n\\[\n\\text{Err}(x) = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n\\]\n\nOR\n\n\\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\nBefore dissecting the equation, lets delve deeper into bias and variance. Bias is the inability of the model to capture the true relationship. Models with high bias are too simple and fail to capture the complexity of the data, leading to higher training and test error. Models with low bias corresponds to a good fit to the training dataset. Variance refers to the amount an estimate would change on using a different training set. Models with high variance implies that the model doesn’t perform well on previously unseen data(testing data) even if it fits the training data well. In contrast, low variance implies that the model performs well on the testing set. The models that have low bias and high variance are known as overfitted models. The models that have high bias and low variance are known as underfitted models. The above equation suggests we need to find a model that simultaneosly achieves low bias and low variance. Variance is a non-negative term and bias squared is non-negative term, which implies that the total error can never go below irreducible error.\nThis graph suggests as model complexity increases, bias decreases faster than variance increases, reducing total error. Beyond a point, bias stabilizes while variance grows significantly, increasing total error and that point is the optimal point for minimum total error.\n\n\nWe have independent variables x that affects the value of a dependent variable y. Function f captures the true relationship between x and y. This is mathemetically expressed as: \\[\ny = f(x) + \\epsilon\n\\] 𝜖 represents the random noise in the relationship between X and Y. 𝜖 has the following properties: \\[\n\\mathbb{E}[\\epsilon] = 0, \\quad \\text{var}(\\epsilon) = \\mathbb{E}[\\epsilon^2] = \\sigma_{\\epsilon}^2\n\\] The goal here is to bring the prediction as close as possible to the actual value(y~f(x)) to minimise the error. Coming to the bias-variance decomposition equation; \\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\n\\(\\mathbb{E}[(y - \\hat{f}(x))^2]\\): Mean Squared Error(MSE). The average squared difference of a prediction f̂(x) from its true value y.\n\\(\\text{bias}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - f(x)\\): The difference of the average value of prediction from the true relationship function f(x)\n\\(\\text{var}(\\hat{f}(x)) = \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]\\): The expectation of the squared deviation of f̂(x) from its expected value 𝔼[f̂(x)]\nStarting from the LHS,\n\\[\n\\begin{align}\n\\mathbb{E}[(y - \\hat{f}(x))^2] &= \\mathbb{E}[(f(x) + \\epsilon - \\hat{f}(x))^2] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\mathbb{E}[\\epsilon^2] + 2\\mathbb{E}[(f(x) - \\hat{f}(x))\\epsilon] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\underbrace{\\mathbb{E}[\\epsilon^2]}_{\\sigma^2_{\\epsilon}} + 2\\mathbb{E}[(f(x) - \\hat{f}(x))] \\underbrace{\\mathbb{E}[\\epsilon]}_{=0} \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\sigma^2_{\\epsilon}\n\\end{align}\n\\]\nContinuing on the RHS, 𝔼[(f(x) −f̂(x))²];\n\\[\n\\begin{align}\n\\mathbb{E}[(f(x) - \\hat{f}(x))^2] &= \\mathbb{E} \\left[ \\left( (f(x) - \\mathbb{E}[\\hat{f}(x)]) - (\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]) \\right)^2 \\right] \\\\\n&= \\mathbb{E} \\left[ \\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2 \\right] + \\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] \\right)^2 \\right] \\\\\n&\\quad - 2 \\mathbb{E} \\left[ (f(x) - \\mathbb{E}[\\hat{f}(x)]) (\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]) \\right] \\\\\n&= \\underbrace{(\\mathbb{E}[\\hat{f}(x)] - f(x))^2}_{\\text{bias}^2[\\hat{f}(x)]} + \\underbrace{\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]}_{\\text{var}(\\hat{f}(x))} \\\\\n&\\quad - 2 (f(x) - \\mathbb{E}[\\hat{f}(x)]) \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])] \\\\\n&= \\text{bias}^2[\\hat{f}(x)] + \\text{var}(\\hat{f}(x)) \\\\\n&\\quad - 2 (f(x) - \\mathbb{E}[\\hat{f}(x)]) (\\mathbb{E}[\\hat{f}(x)] - \\mathbb{E}[\\hat{f}(x)]) \\\\\n&= \\text{bias}^2[\\hat{f}(x)] + \\text{var}(\\hat{f}(x))\n\\end{align}\n\\]\nplugging this back into the equation for 𝔼[(f(x) −f̂(x))²], we arrive at our bias variance decomposition equation\n\\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\] Let’s move on and discuss Bias-Variance decomposition in two contexts: Parameter Estimation & Model Misspecification.\nParameter Estimation: Here we’re looking at a Linear Data Generating Process(DGP) \\[\ny = f(x) + \\epsilon\n\\] where f(x) is a linear function\nIf we now fit a linear model(using OLS) to the above linear DGP, our estimator is unbiased\n\\[\n\\text{Bias}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - f(x)\n\\]\n\\[\n\\mathbb{E}[\\hat{f}(x)] = f(x) \\quad \\Rightarrow \\quad \\text{Bias}[\\hat{f}(x)] = 0\n\\]\nThus, the total error would now just be the variance and irreducible error respectively. Like I mentioned earlier, the irreducible error will always exist and can’t be eliminated. \\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] =\\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\nMisspecified Model: Here we’re looking at a nonlinear DGP \\[\ny = f(x) + \\epsilon\n\\] where f(x) is a nonlinear function\nIf we now fit a linear model to the above nonlinear DGP, our estimator is biased.\n\\[\\text{Bias}[f(x)]^2 = (f(x) - E[f(x)])^2 \\neq 0\\]\nThus, the total error would now include bias, variance and irreducible error respectively. Again , the irreducible error can’t be eliminated. \\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\nBlue Property\nThere are a few key assumptions in the OLS regression and if these assumptions are met, according to the Markov Theorem, the OLS estimates are BLUE-Best Linear Unbiased Estimator. The assumptions are:\n1-Linearity:The relationship between the explanatory variable and the target variable is linear \\[ y = \\beta_0 + \\beta_1 x + u \\] y:dependent variable\nx:independent variable\n𝛽:Parameter\n𝑢:error\n2-Independence of errors:There is no relationship between the residuals and the y variable\n3-Normality of errors: The error terms will follow a normal distribution,called multivariate normality.\n4-Homoscedasticity: The error terms have Equal variances.\n5-The successive error terms are uncorrelated with each other, meaning the covariance is zero.If this assumption is violated, then it causes Auto-correlation.\n6-The error term is uncorrelated with the independent variable\n7-No Multicollinearity:The explanatory variables are uncorrelated.\n\n\n\nThe OLS estimator is given by:\n\\[\n\\hat{\\beta}_{\\text{OLS}} = (X^T X)^{-1} X^T Y\n\\]\nTaking expectations:\n\\[\n\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = \\mathbb{E}[(X^T X)^{-1} X^T Y]\n\\]\nSubstituting \\(Y = X \\beta + \\epsilon\\):\n\\[\n\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T \\mathbb{E}[X \\beta + \\epsilon]\n\\]\nUsing \\(\\mathbb{E}[\\epsilon] = 0\\):\n\\[\n\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T X \\beta = \\beta\n\\]\nThus, \\(\\hat{\\beta}_{\\text{OLS}}\\) is an unbiased estimator of \\(\\beta\\).\n\n\n\n\nThe variance of \\(\\hat{\\beta}_{\\text{OLS}}\\) is:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = \\text{Var}[(X^T X)^{-1} X^T Y]\n\\]\nUsing \\(Y = X \\beta + \\epsilon\\):\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = \\text{Var}[(X^T X)^{-1} X^T (X \\beta + \\epsilon)]\n\\]\nSince \\(\\beta\\) is a constant:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T \\text{Var}[\\epsilon] X (X^T X)^{-1}\n\\]\nAssuming \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\), we substitute \\(\\text{Var}[\\epsilon] = \\sigma^2 I\\):\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T (\\sigma^2 I) X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1}\n\\]\n\n\n\n\nFor any other unbiased linear estimator:\n\\[\n\\hat{\\beta}_{\\text{other}} = C Y\n\\]\nwhere \\(C\\) is a matrix such that \\(\\mathbb{E}[\\hat{\\beta}_{\\text{other}}] = \\beta\\).\nExpanding:\n\\[\n\\hat{\\beta}_{\\text{other}} = C (X \\beta + \\epsilon) = C X \\beta + C \\epsilon\n\\]\nFor unbiasedness, we require:\n\\[\nC X = I\n\\]\nSo, \\(C\\) can be written as:\n\\[\nC = (X^T X)^{-1} X^T + D\n\\]\nwhere \\(D\\) is a matrix such that \\(D X = 0\\).\nNow compute the variance:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{other}}] = \\text{Var}[(X^T X)^{-1} X^T Y + D Y]\n\\]\nSubstituting \\(Y = X \\beta + \\epsilon\\) and using \\(\\text{Var}[\\epsilon] = \\sigma^2 I\\):\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{other}}] = \\sigma^2 (X^T X)^{-1} + \\sigma^2 D D^T\n\\]\nSince \\(D D^T\\) is positive semi-definite:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{other}}] \\geq \\text{Var}[\\hat{\\beta}_{\\text{OLS}}]\n\\]\nWhich implies:\n\\[\n\\mathbb{E}[(\\hat{\\beta}_{\\text{other}} - \\beta)^2] \\geq \\mathbb{E}[(\\hat{\\beta}_{\\text{OLS}} - \\beta)^2]\n\\]\n\n\n\n\nSince OLS is: 1. Unbiased: \\(\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = \\beta\\)\n2. Efficient: \\(\\text{Var}[\\hat{\\beta}_{\\text{OLS}}]\\) is the lowest among all unbiased linear estimators\nBy the Gauss-Markov Theorem:\n\\[\n\\Rightarrow \\text{OLS is the Best Linear Unbiased Estimator (BLUE).}\n\\]"
  },
  {
    "objectID": "project1.html#derivation-of-the-equation",
    "href": "project1.html#derivation-of-the-equation",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "",
    "text": "We have independent variables x that affects the value of a dependent variable y. Function f captures the true relationship between x and y. This is mathemetically expressed as: \\[\ny = f(x) + \\epsilon\n\\] 𝜖 represents the random noise in the relationship between X and Y. 𝜖 has the following properties: \\[\n\\mathbb{E}[\\epsilon] = 0, \\quad \\text{var}(\\epsilon) = \\mathbb{E}[\\epsilon^2] = \\sigma_{\\epsilon}^2\n\\] The goal here is to bring the prediction as close as possible to the actual value(y~f(x)) to minimise the error. Coming to the bias-variance decomposition equation; \\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\n\\(\\mathbb{E}[(y - \\hat{f}(x))^2]\\): Mean Squared Error(MSE). The average squared difference of a prediction f̂(x) from its true value y.\n\\(\\text{bias}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - f(x)\\): The difference of the average value of prediction from the true relationship function f(x)\n\\(\\text{var}(\\hat{f}(x)) = \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]\\): The expectation of the squared deviation of f̂(x) from its expected value 𝔼[f̂(x)]\nStarting from the LHS,\n\\[\n\\begin{align}\n\\mathbb{E}[(y - \\hat{f}(x))^2] &= \\mathbb{E}[(f(x) + \\epsilon - \\hat{f}(x))^2] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\mathbb{E}[\\epsilon^2] + 2\\mathbb{E}[(f(x) - \\hat{f}(x))\\epsilon] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\underbrace{\\mathbb{E}[\\epsilon^2]}_{\\sigma^2_{\\epsilon}} + 2\\mathbb{E}[(f(x) - \\hat{f}(x))] \\underbrace{\\mathbb{E}[\\epsilon]}_{=0} \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}(x))^2] + \\sigma^2_{\\epsilon}\n\\end{align}\n\\]\nContinuing on the RHS, 𝔼[(f(x) −f̂(x))²];\n\\[\n\\begin{align}\n\\mathbb{E}[(f(x) - \\hat{f}(x))^2] &= \\mathbb{E} \\left[ \\left( (f(x) - \\mathbb{E}[\\hat{f}(x)]) - (\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]) \\right)^2 \\right] \\\\\n&= \\mathbb{E} \\left[ \\left( \\mathbb{E}[\\hat{f}(x)] - f(x) \\right)^2 \\right] + \\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)] \\right)^2 \\right] \\\\\n&\\quad - 2 \\mathbb{E} \\left[ (f(x) - \\mathbb{E}[\\hat{f}(x)]) (\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]) \\right] \\\\\n&= \\underbrace{(\\mathbb{E}[\\hat{f}(x)] - f(x))^2}_{\\text{bias}^2[\\hat{f}(x)]} + \\underbrace{\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]}_{\\text{var}(\\hat{f}(x))} \\\\\n&\\quad - 2 (f(x) - \\mathbb{E}[\\hat{f}(x)]) \\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])] \\\\\n&= \\text{bias}^2[\\hat{f}(x)] + \\text{var}(\\hat{f}(x)) \\\\\n&\\quad - 2 (f(x) - \\mathbb{E}[\\hat{f}(x)]) (\\mathbb{E}[\\hat{f}(x)] - \\mathbb{E}[\\hat{f}(x)]) \\\\\n&= \\text{bias}^2[\\hat{f}(x)] + \\text{var}(\\hat{f}(x))\n\\end{align}\n\\]\nplugging this back into the equation for 𝔼[(f(x) −f̂(x))²], we arrive at our bias variance decomposition equation\n\\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\] Let’s move on and discuss Bias-Variance decomposition in two contexts: Parameter Estimation & Model Misspecification.\nParameter Estimation: Here we’re looking at a Linear Data Generating Process(DGP) \\[\ny = f(x) + \\epsilon\n\\] where f(x) is a linear function\nIf we now fit a linear model(using OLS) to the above linear DGP, our estimator is unbiased\n\\[\n\\text{Bias}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - f(x)\n\\]\n\\[\n\\mathbb{E}[\\hat{f}(x)] = f(x) \\quad \\Rightarrow \\quad \\text{Bias}[\\hat{f}(x)] = 0\n\\]\nThus, the total error would now just be the variance and irreducible error respectively. Like I mentioned earlier, the irreducible error will always exist and can’t be eliminated. \\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] =\\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\nMisspecified Model: Here we’re looking at a nonlinear DGP \\[\ny = f(x) + \\epsilon\n\\] where f(x) is a nonlinear function\nIf we now fit a linear model to the above nonlinear DGP, our estimator is biased.\n\\[\\text{Bias}[f(x)]^2 = (f(x) - E[f(x)])^2 \\neq 0\\]\nThus, the total error would now include bias, variance and irreducible error respectively. Again , the irreducible error can’t be eliminated. \\[\n\\mathbb{E}[(y - \\hat{f}(x))^2] = \\text{bias}[\\hat{f}(x)]^2 + \\text{var}(\\hat{f}(x)) + \\sigma_\\epsilon^2\n\\]\nBlue Property\nThere are a few key assumptions in the OLS regression and if these assumptions are met, according to the Markov Theorem, the OLS estimates are BLUE-Best Linear Unbiased Estimator. The assumptions are:\n1-Linearity:The relationship between the explanatory variable and the target variable is linear \\[ y = \\beta_0 + \\beta_1 x + u \\] y:dependent variable\nx:independent variable\n𝛽:Parameter\n𝑢:error\n2-Independence of errors:There is no relationship between the residuals and the y variable\n3-Normality of errors: The error terms will follow a normal distribution,called multivariate normality.\n4-Homoscedasticity: The error terms have Equal variances.\n5-The successive error terms are uncorrelated with each other, meaning the covariance is zero.If this assumption is violated, then it causes Auto-correlation.\n6-The error term is uncorrelated with the independent variable\n7-No Multicollinearity:The explanatory variables are uncorrelated."
  },
  {
    "objectID": "project1.html#unbiased-property",
    "href": "project1.html#unbiased-property",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "",
    "text": "The OLS estimator is given by:\n\\[\n\\hat{\\beta}_{\\text{OLS}} = (X^T X)^{-1} X^T Y\n\\]\nTaking expectations:\n\\[\n\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = \\mathbb{E}[(X^T X)^{-1} X^T Y]\n\\]\nSubstituting \\(Y = X \\beta + \\epsilon\\):\n\\[\n\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T \\mathbb{E}[X \\beta + \\epsilon]\n\\]\nUsing \\(\\mathbb{E}[\\epsilon] = 0\\):\n\\[\n\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T X \\beta = \\beta\n\\]\nThus, \\(\\hat{\\beta}_{\\text{OLS}}\\) is an unbiased estimator of \\(\\beta\\)."
  },
  {
    "objectID": "project1.html#best-property",
    "href": "project1.html#best-property",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "",
    "text": "The variance of \\(\\hat{\\beta}_{\\text{OLS}}\\) is:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = \\text{Var}[(X^T X)^{-1} X^T Y]\n\\]\nUsing \\(Y = X \\beta + \\epsilon\\):\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = \\text{Var}[(X^T X)^{-1} X^T (X \\beta + \\epsilon)]\n\\]\nSince \\(\\beta\\) is a constant:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T \\text{Var}[\\epsilon] X (X^T X)^{-1}\n\\]\nAssuming \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\), we substitute \\(\\text{Var}[\\epsilon] = \\sigma^2 I\\):\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{OLS}}] = (X^T X)^{-1} X^T (\\sigma^2 I) X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1}\n\\]"
  },
  {
    "objectID": "project1.html#minimum-variance-property",
    "href": "project1.html#minimum-variance-property",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "",
    "text": "For any other unbiased linear estimator:\n\\[\n\\hat{\\beta}_{\\text{other}} = C Y\n\\]\nwhere \\(C\\) is a matrix such that \\(\\mathbb{E}[\\hat{\\beta}_{\\text{other}}] = \\beta\\).\nExpanding:\n\\[\n\\hat{\\beta}_{\\text{other}} = C (X \\beta + \\epsilon) = C X \\beta + C \\epsilon\n\\]\nFor unbiasedness, we require:\n\\[\nC X = I\n\\]\nSo, \\(C\\) can be written as:\n\\[\nC = (X^T X)^{-1} X^T + D\n\\]\nwhere \\(D\\) is a matrix such that \\(D X = 0\\).\nNow compute the variance:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{other}}] = \\text{Var}[(X^T X)^{-1} X^T Y + D Y]\n\\]\nSubstituting \\(Y = X \\beta + \\epsilon\\) and using \\(\\text{Var}[\\epsilon] = \\sigma^2 I\\):\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{other}}] = \\sigma^2 (X^T X)^{-1} + \\sigma^2 D D^T\n\\]\nSince \\(D D^T\\) is positive semi-definite:\n\\[\n\\text{Var}[\\hat{\\beta}_{\\text{other}}] \\geq \\text{Var}[\\hat{\\beta}_{\\text{OLS}}]\n\\]\nWhich implies:\n\\[\n\\mathbb{E}[(\\hat{\\beta}_{\\text{other}} - \\beta)^2] \\geq \\mathbb{E}[(\\hat{\\beta}_{\\text{OLS}} - \\beta)^2]\n\\]"
  },
  {
    "objectID": "project1.html#conclusion-gauss-markov-theorem",
    "href": "project1.html#conclusion-gauss-markov-theorem",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "",
    "text": "Since OLS is: 1. Unbiased: \\(\\mathbb{E}[\\hat{\\beta}_{\\text{OLS}}] = \\beta\\)\n2. Efficient: \\(\\text{Var}[\\hat{\\beta}_{\\text{OLS}}]\\) is the lowest among all unbiased linear estimators\nBy the Gauss-Markov Theorem:\n\\[\n\\Rightarrow \\text{OLS is the Best Linear Unbiased Estimator (BLUE).}\n\\]"
  },
  {
    "objectID": "project1.html#references",
    "href": "project1.html#references",
    "title": "STA 9890 Project: Bias and Variance in Linear Regression",
    "section": "References",
    "text": "References\n\nSNU AI. (n.d.). The bias-variance trade-off: A mathematical view. Medium. https://medium.com/snu-ai/the-bias-variance-trade-off-a-mathematical-view-14ff9dfe5a3c\nShubham, S. (n.d.). All about Gauss-Markov theorem for ordinary least squares regression (OLS) & BLUE properties of OLS estimators. Medium. https://medium.com/@shubhamsd100/all-about-gauss-markov-theorem-for-ordinary-least-squares-regression-ols-blue-properties-of-e1e1864fe087\nWeylandt, M. (n.d.). STA9890 Notes. Retrieved from https://michael-weylandt.com/STA9890/notes.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Srijith Reddy | Portfolio",
    "section": "",
    "text": "Hi, I’m Srijith Reddy, a data science enthusiast currently exploring the intersection of statistics, optimization, and machine learning. This portfolio showcases three major projects from my Statistical Learning for Data Mining course—each grounded in rigorous modeling, evaluation, and interpretation. These projects reflect not just applied techniques, but a careful balance between theory and performance.\nYou can also check out my resume at: srijith-reddy.github.io/resume\n\n\n\n\n\nUsed Ridge Regression, LightGBM, and XGBoost to predict 2019 assessed property values from structured historical real estate data. Extensive feature engineering (including SHAP-based selection) and ensemble stacking yielded an RMSE of ~36K on the test set.\n\n\n\nDeveloped a fairness-aware ensemble model (FairStacks) to predict NBA draft selection probabilities. Trained base models like Naive Bayes, LDA, SVM, and penalized logistic regressions, while optimizing a fairness-constrained loss to reduce TPR gap across school tiers.\n\n\n\nSimulated performance of Ridge vs OLS across linear and nonlinear data-generating processes. Illustrated the bias-variance decomposition with clean visualizations to explain when and why Ridge outperforms OLS under regularization.\n\n\n\n\n\nThese projects were completed as part of STA 9890: Statistical Learning for Data Mining—a graduate-level course emphasizing predictive modeling, generalization error, model interpretability, and hands-on experimentation. The coursework combined theoretical insights with real-world data challenges.\n\nThanks for visiting!"
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Srijith Reddy | Portfolio",
    "section": "",
    "text": "Used Ridge Regression, LightGBM, and XGBoost to predict 2019 assessed property values from structured historical real estate data. Extensive feature engineering (including SHAP-based selection) and ensemble stacking yielded an RMSE of ~36K on the test set.\n\n\n\nDeveloped a fairness-aware ensemble model (FairStacks) to predict NBA draft selection probabilities. Trained base models like Naive Bayes, LDA, SVM, and penalized logistic regressions, while optimizing a fairness-constrained loss to reduce TPR gap across school tiers.\n\n\n\nSimulated performance of Ridge vs OLS across linear and nonlinear data-generating processes. Illustrated the bias-variance decomposition with clean visualizations to explain when and why Ridge outperforms OLS under regularization."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "Srijith Reddy | Portfolio",
    "section": "",
    "text": "These projects were completed as part of STA 9890: Statistical Learning for Data Mining—a graduate-level course emphasizing predictive modeling, generalization error, model interpretability, and hands-on experimentation. The coursework combined theoretical insights with real-world data challenges.\n\nThanks for visiting!"
  }
]