<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STA 9890 Project: Bias and Variance in Linear Regression – Statisitical Learning for Data Mining Project Portfolio</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Statisitical Learning for Data Mining Project Portfolio</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./project1.html" aria-current="page"> 
<span class="menu-text">Bias and Variance in Linear Regression</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./project2.html"> 
<span class="menu-text">Ensemble Learning Techniques for Fair Classification</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./project3.html"> 
<span class="menu-text">Forecasting Property Valuations in a Mid-Sized U.S. City:A SHAP-Gain Feature Selection and ElasticNet-Ensembled Approach with Optuna-Tuned XGBoost</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-bias-variance-tradeoff" id="toc-the-bias-variance-tradeoff" class="nav-link active" data-scroll-target="#the-bias-variance-tradeoff">1. The Bias &amp; Variance Tradeoff</a>
  <ul class="collapse">
  <li><a href="#derivation-of-the-equation" id="toc-derivation-of-the-equation" class="nav-link" data-scroll-target="#derivation-of-the-equation">1.1 Derivation of the Equation</a></li>
  </ul></li>
  <li><a href="#computation" id="toc-computation" class="nav-link" data-scroll-target="#computation">2. Computation</a>
  <ul class="collapse">
  <li><a href="#derivation-of-the-ols-closed-form-solution" id="toc-derivation-of-the-ols-closed-form-solution" class="nav-link" data-scroll-target="#derivation-of-the-ols-closed-form-solution">2.1 Derivation of the OLS Closed-Form Solution**</a></li>
  <li><a href="#derivation-of-the-ridge-regression-closed-form-solution" id="toc-derivation-of-the-ridge-regression-closed-form-solution" class="nav-link" data-scroll-target="#derivation-of-the-ridge-regression-closed-form-solution">2.2 Derivation of the Ridge Regression Closed form Solution**</a></li>
  <li><a href="#derivation-of-gradient-descent-for-ols" id="toc-derivation-of-gradient-descent-for-ols" class="nav-link" data-scroll-target="#derivation-of-gradient-descent-for-ols">2.3 Derivation of Gradient Descent for OLS</a></li>
  <li><a href="#ols-gradient-descent-with-weight-decay" id="toc-ols-gradient-descent-with-weight-decay" class="nav-link" data-scroll-target="#ols-gradient-descent-with-weight-decay">2.4 OLS Gradient Descent with Weight Decay</a></li>
  <li><a href="#empirical-demonstration-of-the-equivalence-of-olsweight-decay-with-ridge-regression" id="toc-empirical-demonstration-of-the-equivalence-of-olsweight-decay-with-ridge-regression" class="nav-link" data-scroll-target="#empirical-demonstration-of-the-equivalence-of-olsweight-decay-with-ridge-regression">2.5 Empirical Demonstration of the equivalence of (OLS+Weight Decay) with Ridge Regression</a></li>
  </ul></li>
  <li><a href="#bias-and-variance-under-linear-dgp" id="toc-bias-and-variance-under-linear-dgp" class="nav-link" data-scroll-target="#bias-and-variance-under-linear-dgp">3. Bias and Variance under Linear DGP</a></li>
  <li><a href="#bias-and-variance-under-non-linear-dgp" id="toc-bias-and-variance-under-non-linear-dgp" class="nav-link" data-scroll-target="#bias-and-variance-under-non-linear-dgp">4. Bias and Variance under Non-Linear DGP</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">STA 9890 Project: Bias and Variance in Linear Regression</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="the-bias-variance-tradeoff" class="level1">
<h1>1. The Bias &amp; Variance Tradeoff</h1>
<p>The bias-variance trade-off is an important concept used to optimize performance out of machine learning models. The total error of a model can be decomposed into Reducible error and Irreducible error respectively. Bias and variance are reducible errors. The irreducible error, often known as noise, is the error that can’t be reduced. The best models will always have an error which can’t be removed. Therefore, there is a trade-off between bias and variance to decrease the reducible error which essentially minimizes the total error. The total error can be expressed as the following:</p>
<p><span class="math display">\[
\text{Err}(x) = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
\]</span></p>
<p style="text-align: center;">
<strong>OR</strong>
</p>
<p><span class="math display">\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{bias}[\hat{f}(x)]^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2
\]</span></p>
<p>Before dissecting the equation, lets delve deeper into bias and variance. Bias is the inability of the model to capture the true relationship. Models with high bias are too simple and fail to capture the complexity of the data, leading to higher training and test error. Models with low bias corresponds to a good fit to the training dataset. Variance refers to the amount an estimate would change on using a different training set. Models with high variance implies that the model doesn’t perform well on previously unseen data(testing data) even if it fits the training data well. In contrast, low variance implies that the model performs well on the testing set. The models that have low bias and high variance are known as overfitted models. The models that have high bias and low variance are known as underfitted models. The above equation suggests we need to find a model that simultaneosly achieves low bias and low variance. Variance is a non-negative term and bias squared is non-negative term, which implies that the total error can never go below irreducible error.</p>
<p>This graph suggests as model complexity increases, bias decreases faster than variance increases, reducing total error. Beyond a point, bias stabilizes while variance grows significantly, increasing total error and that point is the optimal point for minimum total error.</p>
<section id="derivation-of-the-equation" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-the-equation">1.1 Derivation of the Equation</h2>
<p>We have independent variables x that affects the value of a dependent variable y. Function f captures the true relationship between x and y. This is mathemetically expressed as: <span class="math display">\[
y = f(x) + \epsilon
\]</span> 𝜖 represents the random noise in the relationship between X and Y. 𝜖 has the following properties: <span class="math display">\[
\mathbb{E}[\epsilon] = 0, \quad \text{var}(\epsilon) = \mathbb{E}[\epsilon^2] = \sigma_{\epsilon}^2
\]</span> The goal here is to bring the prediction as close as possible to the actual value(y~f(x)) to minimise the error. Coming to the bias-variance decomposition equation; <span class="math display">\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{bias}[\hat{f}(x)]^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2
\]</span></p>
<p><span class="math inline">\(\mathbb{E}[(y - \hat{f}(x))^2]\)</span>: Mean Squared Error(MSE). The average squared difference of a prediction f̂(x) from its true value y.</p>
<p><span class="math inline">\(\text{bias}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)] - f(x)\)</span>: The difference of the average value of prediction from the true relationship function f(x)</p>
<p><span class="math inline">\(\text{var}(\hat{f}(x)) = \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]\)</span>: The expectation of the squared deviation of f̂(x) from its expected value 𝔼[f̂(x)]</p>
<p>Starting from the LHS,</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}[(y - \hat{f}(x))^2] &amp;= \mathbb{E}[(f(x) + \epsilon - \hat{f}(x))^2] \\
&amp;= \mathbb{E}[(f(x) - \hat{f}(x))^2] + \mathbb{E}[\epsilon^2] + 2\mathbb{E}[(f(x) - \hat{f}(x))\epsilon] \\
&amp;= \mathbb{E}[(f(x) - \hat{f}(x))^2] + \underbrace{\mathbb{E}[\epsilon^2]}_{\sigma^2_{\epsilon}} + 2\mathbb{E}[(f(x) - \hat{f}(x))] \underbrace{\mathbb{E}[\epsilon]}_{=0} \\
&amp;= \mathbb{E}[(f(x) - \hat{f}(x))^2] + \sigma^2_{\epsilon}
\end{align}
\]</span></p>
<p>Continuing on the RHS, 𝔼[(f(x) −f̂(x))²];</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}[(f(x) - \hat{f}(x))^2] &amp;= \mathbb{E} \left[ \left( (f(x) - \mathbb{E}[\hat{f}(x)]) - (\hat{f}(x) - \mathbb{E}[\hat{f}(x)]) \right)^2 \right] \\
&amp;= \mathbb{E} \left[ \left( \mathbb{E}[\hat{f}(x)] - f(x) \right)^2 \right] + \mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E}[\hat{f}(x)] \right)^2 \right] \\
&amp;\quad - 2 \mathbb{E} \left[ (f(x) - \mathbb{E}[\hat{f}(x)]) (\hat{f}(x) - \mathbb{E}[\hat{f}(x)]) \right] \\
&amp;= \underbrace{(\mathbb{E}[\hat{f}(x)] - f(x))^2}_{\text{bias}^2[\hat{f}(x)]} + \underbrace{\mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]}_{\text{var}(\hat{f}(x))} \\
&amp;\quad - 2 (f(x) - \mathbb{E}[\hat{f}(x)]) \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])] \\
&amp;= \text{bias}^2[\hat{f}(x)] + \text{var}(\hat{f}(x)) \\
&amp;\quad - 2 (f(x) - \mathbb{E}[\hat{f}(x)]) (\mathbb{E}[\hat{f}(x)] - \mathbb{E}[\hat{f}(x)]) \\
&amp;= \text{bias}^2[\hat{f}(x)] + \text{var}(\hat{f}(x))
\end{align}
\]</span></p>
<p>plugging this back into the equation for 𝔼[(f(x) −f̂(x))²], we arrive at our bias variance decomposition equation</p>
<p><span class="math display">\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{bias}[\hat{f}(x)]^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2
\]</span> <strong>Let’s move on and discuss Bias-Variance decomposition in two contexts: Parameter Estimation &amp; Model Misspecification.</strong></p>
<p>Parameter Estimation: Here we’re looking at a Linear Data Generating Process(DGP) <span class="math display">\[
y = f(x) + \epsilon
\]</span> where f(x) is a linear function</p>
<p>If we now fit a linear model(using OLS) to the above linear DGP, our estimator is unbiased</p>
<p><span class="math display">\[
\text{Bias}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)] - f(x)
\]</span></p>
<p><span class="math display">\[
\mathbb{E}[\hat{f}(x)] = f(x) \quad \Rightarrow \quad \text{Bias}[\hat{f}(x)] = 0
\]</span></p>
<p>Thus, the total error would now just be the variance and irreducible error respectively. Like I mentioned earlier, the irreducible error will always exist and can’t be eliminated. <span class="math display">\[
\mathbb{E}[(y - \hat{f}(x))^2] =\text{var}(\hat{f}(x)) + \sigma_\epsilon^2
\]</span></p>
<p>Misspecified Model: Here we’re looking at a nonlinear DGP <span class="math display">\[
y = f(x) + \epsilon
\]</span> where f(x) is a nonlinear function</p>
<p>If we now fit a linear model to the above nonlinear DGP, our estimator is biased.</p>
<p><span class="math display">\[\text{Bias}[f(x)]^2 = (f(x) - E[f(x)])^2 \neq 0\]</span></p>
<p>Thus, the total error would now include bias, variance and irreducible error respectively. Again , the irreducible error can’t be eliminated. <span class="math display">\[
\mathbb{E}[(y - \hat{f}(x))^2] = \text{bias}[\hat{f}(x)]^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2
\]</span></p>
<p><strong>Blue Property</strong></p>
<p>There are a few key assumptions in the OLS regression and if these assumptions are met, according to the Markov Theorem, the OLS estimates are BLUE-Best Linear Unbiased Estimator. The assumptions are:</p>
<p>1-Linearity:The relationship between the explanatory variable and the target variable is linear <span class="math display">\[ y = \beta_0 + \beta_1 x + u \]</span> y:dependent variable</p>
<p>x:independent variable</p>
<p>𝛽:Parameter</p>
<p>𝑢:error</p>
<p>2-Independence of errors:There is no relationship between the residuals and the y variable</p>
<p>3-Normality of errors: The error terms will follow a normal distribution,called multivariate normality.</p>
<p>4-Homoscedasticity: The error terms have Equal variances.</p>
<p>5-The successive error terms are uncorrelated with each other, meaning the covariance is zero.If this assumption is violated, then it causes Auto-correlation.</p>
<p>6-The error term is uncorrelated with the independent variable</p>
<p>7-No Multicollinearity:The explanatory variables are uncorrelated.</p>
<p><strong>Unbiased Property</strong></p>
<p>The OLS estimator is given by:</p>
<p><span class="math display">\[ \hat{\beta}_{OLS} = (X^T X)^{-1} X^T Y \]</span></p>
<p>Taking expectations:</p>
<p><span class="math display">\[ E[\hat{\beta}_{OLS}] = E[(X^T X)^{-1} X^T Y] \]</span></p>
<p>Substituting $ Y = X + $:</p>
<p><span class="math display">\[ E[\hat{\beta}_{OLS}] = (X^T X)^{-1} X^T E[X \beta + \epsilon] \]</span></p>
<p>By OLS $ E[] = 0 $:</p>
<p><span class="math display">\[ E[\hat{\beta}_{OLS}] = (X^T X)^{-1} X^T (X \beta) \]</span></p>
<p><span class="math display">\[ = (X^T X)^{-1} X^T X \beta \]</span></p>
<p><span class="math display">\[ = \beta \]</span></p>
<p>Thus, $ _{OLS} $ is an unbiased estimator of $ $.</p>
<p><strong>Best Property</strong></p>
<p>The variance of $ _{OLS} $ is:</p>
<p><span class="math display">\[ V[\hat{\beta}_{OLS}] = V[(X^T X)^{-1} X^T Y] \]</span></p>
<p>Using $ Y = X+ $:</p>
<p><span class="math display">\[ V[\hat{\beta}_{OLS}] = V[(X^T X)^{-1} X^T (X\beta + \epsilon)] \]</span></p>
<p>Since $ $ is a constant:</p>
<p><span class="math display">\[ V[\hat{\beta}_{OLS}] = (X^T X)^{-1} X^T V[\epsilon] X (X^T X)^{-1} \]</span></p>
<p>Assuming $ N(0, ^2 I) $, we substitute $ V[] = ^2 I $:</p>
<p><span class="math display">\[ V[\hat{\beta}_{OLS}] = (X^T X)^{-1} X^T (\sigma^2 I) X (X^T X)^{-1} \]</span></p>
<p><span class="math display">\[ = \sigma^2 (X^T X)^{-1} \]</span></p>
<p>Thus, the variance of the OLS estimator is:</p>
<p><span class="math display">\[ V[\hat{\beta}_{OLS}] = \sigma^2 (X^T X)^{-1} \]</span></p>
<p><strong>Minimum Variance Property</strong></p>
<p>For any other unbiased linear estimator:</p>
<p><span class="math display">\[ \hat{\beta}_{other} = C Y \]</span></p>
<p>where $ C $ is a matrix satisfying $ E[_{other}] = $.</p>
<p>Expanding:</p>
<p><span class="math display">\[ \hat{\beta}_{other} = C(X \beta + \epsilon) \]</span></p>
<p><span class="math display">\[ = C X \beta + C \epsilon \]</span></p>
<p>For unbiasedness:</p>
<p><span class="math display">\[ C X = I \]</span></p>
<p>Thus, $ C $ can be written as:</p>
<p><span class="math display">\[ C = (X^T X)^{-1} X^T + D \]</span></p>
<p>where $ D $ is an arbitrary matrix satisfying $ D X = 0 $.</p>
<p>Computing variance:</p>
<p><span class="math display">\[ V[\hat{\beta}_{other}] = V[(X^T X)^{-1} X^T Y + D Y] \]</span></p>
<p>Using $ V[] = ^2 I $:</p>
<p><span class="math display">\[ V[\hat{\beta}_{other}] = \sigma^2 (X^T X)^{-1} + \sigma^2 D D^T \]</span></p>
<p>Since $ D D^T $ is a positive semi-definite matrix, we conclude:</p>
<p><span class="math display">\[ V[\hat{\beta}_{other}] \geq V[\hat{\beta}_{OLS}] \]</span></p>
<p>which implies:</p>
<p><span class="math display">\[ E[(\hat{\beta}_{other} - \beta)^2] \geq E[(\hat{\beta}_{OLS} - \beta)^2] \]</span></p>
<p>Since OLS is: 1. Unbiased: $ E[_{OLS}] = $. 2. Efficient: $ V[_{OLS}] $ is the lowest possible variance among unbiased estimators.</p>
<p>By the Gauss-Markov theorem:</p>
<p><span class="math display">\[ \Rightarrow \text{OLS is the Best Linear Unbiased Estimator.} \]</span></p>
</section>
</section>
<section id="computation" class="level1">
<h1>2. Computation</h1>
<section id="derivation-of-the-ols-closed-form-solution" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-the-ols-closed-form-solution">2.1 Derivation of the OLS Closed-Form Solution**</h2>
<p>Ordinary Least Squares (OLS) aims to minimize the sum of squared errors between the predicted and actual values in a linear regression model. The problem is formulated as:</p>
<p><span class="math display">\[\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \frac{1}{n} \| y - X\beta \|_2^2.\]</span></p>
<p>where:</p>
<ul>
<li>$ y $ is the $ n $ vector of observed dependent variable values,<br>
</li>
<li>$ X $ is the $ n p $ feature matrix (with $ n $ observations and $ p $ features),<br>
</li>
<li>$ $ is the $ p $ vector of regression coefficients.</li>
</ul>
<p>For convenience, we modify this slightly:</p>
<p><span class="math display">\[\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \frac{1}{2} \| y - X\beta \|_2^2.\]</span></p>
<p>Expanding the squared norm,</p>
<p><span class="math display">\[\| y - X\beta \|_2^2 = (y - X\beta)^T (y - X\beta).\]</span></p>
<p>Thus, the objective function becomes:</p>
<p><span class="math display">\[S(\beta) = \frac{1}{2} (y - X\beta)^T (y - X\beta).\]</span></p>
<p>Taking the derivative,</p>
<p><span class="math display">\[\frac{d}{d\beta} S(\beta) = \frac{1}{2} \cdot 2 X^T (X\beta - y) = X^T (X\beta - y).\]</span></p>
<p>Setting the gradient to zero for minimization:</p>
<p><span class="math display">\[X^T X \beta = X^T y.\]</span></p>
<p>Assuming <span class="math inline">\(X^T X\)</span> is invertible(not singular or det(<span class="math inline">\(X^T X\)</span>)=0):</p>
<p><span class="math display">\[\hat{\beta} = (X^T X)^{-1} X^T y.\]</span></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute (X^T X) to check if it's invertible</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>determinant <span class="op">=</span> np.linalg.det(XTX)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure determinant is nonzero (invertible)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> determinant <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Singular matrix, choosing a different dataset."</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Solve for weight vector w using the closed-form OLS solution</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.linalg.inv(XTX) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Beta:</span><span class="ch">\n</span><span class="st">"</span>, beta)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Beta:
 [-55.  90. -40.]</code></pre>
</section>
<section id="derivation-of-the-ridge-regression-closed-form-solution" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-the-ridge-regression-closed-form-solution">2.2 Derivation of the Ridge Regression Closed form Solution**</h2>
<p>Ridge Regression extends Ordinary Least Squares (OLS) by adding a regularization term to prevent overfitting and handle singular matrices. The optimization problem is:</p>
<p><span class="math display">\[\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \frac{1}{2} \| y - X\beta \|_2^2 + \frac{\lambda}{2} \| \beta \|_2^2.\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the regularization parameter, controlling the penalty on large coefficients.</li>
</ul>
<p>The Ridge Regression objective function consists of two terms: 1. Residual Sum of Squares (RSS): Measures the error in predictions. 2. L2 Regularization Term: Penalizes large values of <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[\
S(\beta) = \frac{1}{2} (y - X\beta)^T (y - X\beta) + \frac{\lambda}{2} \beta^T \beta.
\\]</span></p>
<p>Expanding the first term:</p>
<p><span class="math display">\[\
(y - X\beta)^T (y - X\beta) = y^T y - 2\beta^T X^T y + \beta^T X^T X \beta.
\\]</span></p>
<p>Thus, the objective function becomes:</p>
<p><span class="math display">\[
S(\beta) = \frac{1}{2} \left( y^T y - 2\beta^T X^T y + \beta^T X^T X \beta \right) + \frac{\lambda}{2} \beta^T \beta.
\]</span></p>
<p>Taking the derivative:</p>
<p><span class="math display">\[\
\frac{d}{d\beta} S(\beta) = \frac{1}{2} \left( -2X^T y + 2X^T X \beta \right) + \frac{1}{2} \left( 2\lambda \beta \right).
\\]</span></p>
<p>Simplifying:</p>
<p><span class="math display">\[\
X^T X \beta - X^T y + \lambda \beta = 0.
\\]</span></p>
<p>Rearrange to solve for <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[\
(X^T X + \lambda I) \beta = X^T y.
\\]</span></p>
<p><span class="math display">\[\
\hat{\beta} = (X^T X + \lambda I)^{-1} X^T y.
\\]</span></p>
<p>Key Insights:<br>
Regularization Ensures Invertibility - Unlike OLS, where <span class="math inline">\(X^T X\)</span> might be singular, Ridge Regression adds <span class="math inline">\(\lambda I\)</span>, making <span class="math inline">\((X^T X + \lambda I)\)</span> always invertible for <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<p>Prevents Overfitting - Large values of <span class="math inline">\(\beta\)</span> are penalized, reducing variance and improving generalization.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the regularization parameter (lambda)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute (X^T X) and add regularization term (lambda * I)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(X.shape[<span class="dv">1</span>])  <span class="co"># Identity matrix of size (p x p)</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>ridge_matrix <span class="op">=</span> XTX <span class="op">+</span> lambda_ <span class="op">*</span> I</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve for beta using the closed-form Ridge Regression solution</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>beta_ridge <span class="op">=</span> np.linalg.inv(ridge_matrix) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Ridge Regression Beta:</span><span class="ch">\n</span><span class="st">"</span>, beta_ridge)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Ridge Regression Beta:
 [-1.36034779  2.71427884  0.95101358]</code></pre>
</section>
<section id="derivation-of-gradient-descent-for-ols" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-gradient-descent-for-ols">2.3 Derivation of Gradient Descent for OLS</h2>
<p>Instead of solving $ = (X^T X)^{-1} X^T y $ directly, we use Gradient Descent.</p>
<p>The Mean Squared Error (MSE) is used as the loss function:</p>
<p><span class="math display">\[ S(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - X_i\beta)^2. \]</span></p>
<p>This can be rewritten in matrix form:</p>
<p><span class="math display">\[ S(\beta) = \frac{1}{2n} (y - X\beta)^T (y - X\beta). \]</span></p>
<p>To minimize $ S() $, we compute its gradient:</p>
<p><span class="math display">\[ \frac{d}{d\beta} S(\beta) = -\frac{1}{n} X^T (y - X\beta). \]</span></p>
<p>This gradient tells us the direction in which we should move $ $ to reduce the error.</p>
<p>Using Gradient Descent, we iteratively update $ $ as:</p>
<p><span class="math display">\[ \beta^{(t+1)} = \beta^{(t)} - \alpha \cdot \left(-\frac{1}{n} X^T (y - X\beta^{(t)})\right). \]</span></p>
<p>where: - <span class="math inline">\(\alpha\)</span> is the learning rate (step size), - <span class="math inline">\(t\)</span> represents the iteration number.</p>
<p>Simplifying,</p>
<p><span class="math display">\[ \beta^{(t+1)} = \beta^{(t)} + \frac{\alpha}{n} X^T (y - X\beta^{(t)}). \]</span></p>
<p>This process is repeated until convergence.</p>
<p>Important things to remember:</p>
<ul>
<li>Learning Rate $ $:
<ul>
<li>If too large, the updates may overshoot and diverge.</li>
<li>If too small, the process may take too long.</li>
</ul></li>
<li>Stopping Criteria:
<ul>
<li>If the change in $ $ is very small, we stop early to save computation.</li>
</ul></li>
<li>Feature Scaling:
<ul>
<li>Gradient Descent works better if features are scaled(like using standardization).</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the learning rate (alpha) and number of iterations</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># Learning rate</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Number of iterations</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-6</span>  <span class="co"># Convergence threshold</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize beta (coefficients)</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> X.shape</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.zeros((p, <span class="dv">1</span>))  <span class="co"># Initialize beta with zeros</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Ensure y is a column vector</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient Descent for OLS</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> beta)  <span class="co"># Compute gradient for OLS</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> alpha <span class="op">*</span> gradient  <span class="co"># Update beta</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if the change in beta is very small</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.linalg.norm(beta_new <span class="op">-</span> beta, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>) <span class="op">&lt;</span> tolerance:</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Gradient Descent Converged in </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta_new  <span class="co"># Update beta for next iteration</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">OLS Beta (Gradient Descent):</span><span class="ch">\n</span><span class="st">"</span>, beta)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>OLS Beta (Gradient Descent):
 [[-0.58672692]
 [ 1.42766572]
 [ 1.56502404]]</code></pre>
</section>
<section id="ols-gradient-descent-with-weight-decay" class="level2">
<h2 class="anchored" data-anchor-id="ols-gradient-descent-with-weight-decay">2.4 OLS Gradient Descent with Weight Decay</h2>
<p>Weight Decay is a regularization technique that discourages large weights by adding a penalty term to the loss function. It is equivalent to L2 regularization(used in Ridge Regression) and helps prevent overfitting.</p>
<p>The modified OLS loss function with weight decay is:</p>
<p><span class="math display">\[
S(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - X\beta)^2 + \frac{\lambda}{2} \|\beta\|_2^2
\]</span></p>
<p>where: - The first term is the standard Mean Squared Error (MSE) loss. - The second term is the L2 regularization (weight decay), where <span class="math inline">\(\lambda\)</span> is the weight decay parameter.</p>
<p>To minimize <span class="math inline">\(S(\beta)\)</span>, we compute its gradient.</p>
<p>The gradient of the original OLS loss function is:</p>
<p><span class="math display">\[
\nabla S(\beta)_{\text{OLS}} = -\frac{1}{n} X^T (y - X\beta)
\]</span></p>
<p>The derivative of <span class="math inline">\(\frac{\lambda}{2} \|\beta\|_2^2\)</span> with respect to <span class="math inline">\(\beta\)</span> is:</p>
<p><span class="math display">\[
\nabla S(\beta)_{\text{reg}} = \lambda \beta
\]</span></p>
<p>The total gradient of <span class="math inline">\(S(\beta)\)</span> with weight decay is:</p>
<p><span class="math display">\[
\nabla S(\beta) = -\frac{1}{n} X^T (y - X\beta) + \lambda \beta
\]</span></p>
<p>Using gradient descent, we update <span class="math inline">\(\beta\)</span> iteratively:</p>
<p><span class="math display">\[
\beta^{(t+1)} = \beta^{(t)} + \alpha \left( \frac{1}{n} X^T (y - X\beta^{(t)}) - \lambda \beta^{(t)} \right)
\]</span></p>
<p>where: - <span class="math inline">\(\alpha\)</span> is the learning rate. - <span class="math inline">\(\lambda\)</span> is the weight decay parameter. - The second term shrinks the weights, preventing them from growing too large.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the regularization parameter (lambda) and learning rate (alpha)</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize beta (coefficients)</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> X.shape</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.zeros((p, <span class="dv">1</span>))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co"># OLS Gradient Descent with weight decay</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> beta) <span class="op">-</span> lambda_ <span class="op">*</span> beta</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> alpha <span class="op">*</span> gradient</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if the change in beta is very small</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.linalg.norm(beta_new <span class="op">-</span> beta, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>) <span class="op">&lt;</span> tolerance:</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Gradient Descent Converged in </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta_new  <span class="co"># Update beta for next iteration</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Ridge Regression Beta (Gradient Descent):</span><span class="ch">\n</span><span class="st">"</span>, beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Ridge Regression Beta (Gradient Descent):
 [[-0.39537272]
 [ 1.19539342]
 [ 1.64430062]]</code></pre>
<p><strong>Justification for Low $ $ (Weight Decay) and $ $ (Learning Rate)</strong></p>
<p>1.Why Use a Low $ $?</p>
<p>The regularization term $ ||_2^2 $ discourages large weights, improving generalization.</p>
<p>A high $ $ can: - Over-penalize large coefficients, causing underfitting. - Slow down convergence in Gradient Descent. - Shrink weights excessively, distorting data relationships.</p>
<p>A low $ $: - Preserves meaningful information. - Balances regularization and model complexity. - Ensures smooth weight decay without over-shrinking.</p>
<p>2.Why Use a Low $ $?</p>
<p>The learning rate $ $ controls the step size in Gradient Descent:</p>
<p>A high $ $: - Causes instability, leading to divergence. - Interferes with weight decay, making updates too aggressive.</p>
<p>A low $ $: - Ensures stable convergence. - Prevents oscillations in parameter updates. - Works well with regularization for smooth training.</p>
</section>
<section id="empirical-demonstration-of-the-equivalence-of-olsweight-decay-with-ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="empirical-demonstration-of-the-equivalence-of-olsweight-decay-with-ridge-regression">2.5 Empirical Demonstration of the equivalence of (OLS+Weight Decay) with Ridge Regression</h2>
<p>Ridge Regression optimizes: <span class="math display">\[
    \hat{\beta}_{{ridge}} = (X^TX + \lambda I)^{-1}X^Ty
\]</span></p>
<p>Weight Decay modifies the Gradient Descent update as: <span class="math display">\[
    \beta^{(t+1)} = \beta^{(t)} + \alpha \left( \frac{1}{n} X^T(y - X\beta^{(t)}) - \lambda \beta^{(t)} \right)
\]</span></p>
<p>As iterations <span class="math inline">\(\to \infty\)</span>, the Gradient Descent solution should converge to Ridge Regression.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>n_samples, n_features <span class="op">=</span> <span class="dv">100</span>, <span class="dv">5</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(n_samples, n_features)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>true_beta <span class="op">=</span> np.random.randn(n_features, <span class="dv">1</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">@</span> true_beta <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> np.random.randn(n_samples, <span class="dv">1</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the regularization parameter and learning rate</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># Learning rate</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">5000</span>  <span class="co"># Large number of iterations to ensure convergence</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-8</span>  <span class="co"># Lower tolerance for better accuracy</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge Regression (Closed-Form Solution)</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(X.shape[<span class="dv">1</span>])  <span class="co"># Identity matrix</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>ridge_matrix <span class="op">=</span> XTX <span class="op">+</span> lambda_ <span class="op">*</span> I</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>beta_ridge <span class="op">=</span> np.linalg.inv(ridge_matrix) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize beta (coefficients) for Gradient Descent</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> X.shape</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.zeros((p, <span class="dv">1</span>))  <span class="co"># Initialize beta with zeros</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Store error for visualization</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient Descent for OLS with Weight Decay</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> beta) <span class="op">-</span> lambda_ <span class="op">*</span> beta</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> alpha <span class="op">*</span> gradient</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute error between Ridge and GD solutions</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> np.linalg.norm(beta_new <span class="op">-</span> beta_ridge, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    errors.append(error)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if close to the closed-form solution</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> error <span class="op">&lt;</span> tolerance:</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Gradient Descent Converged in </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta_new</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>beta_weight_decay <span class="op">=</span> beta  <span class="co"># Store final result</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare numerical results</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>df_results <span class="op">=</span> pd.DataFrame({<span class="st">"Ridge Closed-Form"</span>: beta_ridge.flatten(),</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"Gradient Descent (Weight Decay)"</span>: beta_weight_decay.flatten()})</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Comparison of Ridge Regression and OLS with Weight Decay:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Comparison of Ridge Regression and OLS with Weight Decay:

   Ridge Closed-Form  Gradient Descent (Weight Decay)
0           0.922036                         0.778632
1           1.900938                         1.688677
2          -1.388070                        -1.233538
3           0.567381                         0.509230
4          -0.626401                        -0.585660</code></pre>
<div class="columns">
<div class="column" style="width:50%;">
<img src="images/Convergence.png" class="img-fluid" style="width:100.0%">
<center>
Convergence of OLS with Weight Decay to Ridge Regression
</center>
</div><div class="column" style="width:50%;">
<img src="images/Comparison.png" class="img-fluid" style="width:100.0%">
<center>
Comparison of Ridge Regression and OLS with Weight Decay
</center>
</div>
</div>
</section>
</section>
<section id="bias-and-variance-under-linear-dgp" class="level1">
<h1>3. Bias and Variance under Linear DGP</h1>
<p>Linear DGP: y=Xβ+ϵ We assume no multicollinearity among predictors in a linear DGP. However, with real data there will be correlation between predictors, violating the assumption of independent features. I wanted to introduce controlled correlation using an AR(2) model, which models each predictor as a function of its two preceding values. I was really interested to explore how OLS and Ridge Regression behave when predictors are not independent.</p>
<div id="a24398a6" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate AR(2) covariance matrix</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_ar2_cov(p, rho1, rho2):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    Sigma <span class="op">=</span> np.zeros((p, p))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> j:</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">abs</span>(i <span class="op">-</span> j) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> rho1</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">abs</span>(i <span class="op">-</span> j) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> rho2</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Sigma</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="130a6775" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to compute OLS</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_estimators_OLS(X, y):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    beta_ols <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_ols</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to compute Ridge</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_ridge_regression(X, y, lambda_):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    I <span class="op">=</span> np.eye(X.shape[<span class="dv">1</span>])  <span class="co"># Identity matrix</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    ridge_matrix <span class="op">=</span> XTX <span class="op">+</span> lambda_ <span class="op">*</span> I</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    beta_ridge <span class="op">=</span> np.linalg.inv(ridge_matrix) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_ridge</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>I performed the Montecarlo simulations separately for OLS and Ridge Regression. In the OLS simulation, we compute the regression coefficients using the closed-form least squares estimator. For each repetition, the training set is used to fit the model, and predictions are made on both training and held-out test sets. The mean squared error (MSE), bias, and variance of the OLS estimates are computed and averaged over multiple repetitions to reduce randomness. The bias is approximated as the mean deviation of the estimated coefficients from the true coefficients, while the variance is measured as the variability of the estimated coefficients.</p>
<p>In contrast, the Ridge regression simulation includes an additional loop over a grid of regularization parameters (lambda values). For each lambda, the Ridge estimator is used to compute the regression coefficients, and similar metrics—MSE, bias, variance, and out-of-sample error—are calculated. These values are averaged over repetitions for each lambda and then averaged again across all lambdas to obtain a summary measure for each sample size. While this approach provides a general picture of Ridge performance, a more refined strategy might involve selecting the lambda that minimizes out-of-sample error for each repetition.</p>
<p>Both simulations aim to capture how the MSE decomposes into bias and variance components and how these change with increasing sample size. By comparing OLS and Ridge regression, the code highlights how regularization reduces variance at the cost of increased bias, often leading to better generalization on unseen data, especially when the sample size is small or when multicollinearity is present in the predictors.</p>
<div id="dac95e9e" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for OLS</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ols_mse_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>ols_bias_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>ols_variance_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>ols_out_sample_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation for OLS</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, size <span class="kw">in</span> <span class="bu">enumerate</span>(sample_sizes):</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    ols_mse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    ols_bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    ols_variance <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    out_sample_mse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.multivariate_normal(np.zeros(p), cov_matrix, size<span class="op">=</span>size)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate new data for each repetition</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        y_subsample <span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> np.random.randn(size)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train/Test Split</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_subsample, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute OLS estimators</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        beta_ols <span class="op">=</span> compute_estimators_OLS(X_train, y_train)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute Predictions</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        y_pred_ols_train <span class="op">=</span> X_train <span class="op">@</span> beta_ols</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        y_pred_ols_test <span class="op">=</span> X_test <span class="op">@</span> beta_ols</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute MSE, Bias, and Variance</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        ols_mse <span class="op">+=</span> np.mean((y_train <span class="op">-</span> y_pred_ols_train) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        ols_bias <span class="op">+=</span> np.mean(beta_ols <span class="op">-</span> beta_true)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        ols_variance <span class="op">+=</span> np.var(beta_ols)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        out_sample_mse <span class="op">+=</span> np.mean((y_test <span class="op">-</span> y_pred_ols_test) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    ols_mse_values[idx] <span class="op">=</span> ols_mse <span class="op">/</span> n_reps</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    ols_bias_values[idx] <span class="op">=</span> ols_bias <span class="op">/</span> n_reps</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    ols_variance_values[idx] <span class="op">=</span> ols_variance <span class="op">/</span> n_reps</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    ols_out_sample_mse[idx] <span class="op">=</span> out_sample_mse <span class="op">/</span> n_reps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="21687618" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for Ridge Regression</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>ridge_mse_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>ridge_bias_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>ridge_variance_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>ridge_mse_per_sample <span class="op">=</span> []</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>ridge_out_per_sample <span class="op">=</span> []</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>ridge_out_sample_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation for Ridge</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, size <span class="kw">in</span> <span class="bu">enumerate</span>(sample_sizes):</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    ridge_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    ridge_bias <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    ridge_variance <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    out_sample_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, lmb <span class="kw">in</span> <span class="bu">enumerate</span>(lambda_grid):</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        mse_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        bias_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        variance_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        out_mse_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> np.random.multivariate_normal(np.zeros(p), cov_matrix, size<span class="op">=</span>size)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Generate new data for each repetition</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>            y_subsample <span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> np.random.randn(size)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Train-test split</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_subsample, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Ridge estimators</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            beta_ridge <span class="op">=</span> compute_estimators_Ridge(X_train, y_train, lmb)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Predictions</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            y_pred_ridge_train <span class="op">=</span> X_train <span class="op">@</span> beta_ridge</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            y_pred_ridge_test <span class="op">=</span> X_test <span class="op">@</span> beta_ridge</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute MSE, Bias, and Variance</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>            mse_sum <span class="op">+=</span> np.mean((y_train <span class="op">-</span> y_pred_ridge_train) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            bias_sum <span class="op">+=</span> np.mean(beta_ridge <span class="op">-</span> beta_true)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>            variance_sum <span class="op">+=</span> np.var(beta_ridge)</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>            out_mse_sum <span class="op">+=</span> np.mean((y_test <span class="op">-</span> y_pred_ridge_test) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store averaged values for Ridge</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>        ridge_mse[i] <span class="op">=</span> mse_sum <span class="op">/</span> n_reps</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        ridge_bias[i] <span class="op">=</span> bias_sum <span class="op">/</span> n_reps</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>        ridge_variance[i] <span class="op">=</span> variance_sum <span class="op">/</span> n_reps</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>        out_sample_mse[i] <span class="op">=</span> out_mse_sum <span class="op">/</span> n_reps</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>    ridge_mse_values[idx] <span class="op">=</span> np.mean(ridge_mse)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>    ridge_bias_values[idx] <span class="op">=</span> np.mean(ridge_bias)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>    ridge_variance_values[idx] <span class="op">=</span> np.mean(ridge_variance)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>    ridge_out_sample_mse[idx] <span class="op">=</span> np.mean(out_sample_mse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The bias comparison plot reveals that OLS remains approximately unbiased across all sample sizes, as expected from its closed-form derivation that minimizes residuals without regularization. Ridge regression, however, introduces a small negative bias due to its penalty term that shrinks coefficients toward zero. Interestingly, this bias diminishes as the sample size increases, indicating that with more data, the regularization effect becomes less influential and the Ridge estimator begins to approach the OLS solution. This aligns with the theoretical understanding that regularization primarily helps in low-sample or high-collinearity scenarios.</p>
<p>The variance comparison plot shows that Ridge regression consistently achieves slightly lower variance than OLS at every sample size. This is a direct consequence of the regularization term in Ridge, which stabilizes coefficient estimates by penalizing large weights, especially in scenarios with multicollinearity or limited data. Both OLS and Ridge exhibit a steep drop in variance as sample size increases, demonstrating that more data leads to more stable parameter estimation in both models.</p>
<p>In the MSE comparison plot, we observe that Ridge regression has a higher in-sample MSE than OLS across all sample sizes, which is expected due to its bias penalty. However, Ridge achieves a consistently lower out-of-sample MSE than OLS for small to moderate sample sizes. This highlights the bias-variance tradeoff in action: while Ridge sacrifices some in-sample accuracy by introducing bias, it benefits from reduced variance, leading to better generalization on unseen data. As the sample size increases to 500, both models converge in their out-of-sample performance, with OLS catching up due to its decreasing variance and absence of bias.</p>
<div class="columns">
<div class="column" style="width:33%;">
<img src="images/Bias_Comparison.png" class="img-fluid" style="width:100.0%">
<center>
Bias Comparison: OLS vs Ridge
</center>
</div><div class="column" style="width:33%;">
<img src="images/Variance_Comparison.png" class="img-fluid" style="width:100.0%">
<center>
Variance Comparison: OLS vs Ridge
</center>
</div><div class="column" style="width:33%;">
<img src="images/MSE_Comparison.png" class="img-fluid" style="width:100.0%">
<center>
MSE Comparison: OLS vs Ridge
</center>
</div>
</div>
<p>Ridge regression was evaluated across a grid of regularization parameters using k-fold cross-validation with repeated sampling. For each lambda value, the data was split into training and validation sets multiple times to compute stable estimates of out-of-sample mean squared error. The mean MSE across all folds and repetitions was recorded for each lambda, allowing for the identification of the value that yielded the lowest average validation error.</p>
<p>This process resulted in an optimal lambda of approximately 0.1963, which minimized the out-of-sample MSE to 0.7268. In comparison, the corresponding out-of-sample MSE for ordinary least squares, which does not include any regularization, was slightly higher at 0.7505. This demonstrates that a modest amount of regularization can improve generalization performance by balancing bias and variance, especially in settings where noise or multicollinearity is present in the predictors.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for Ridge Regression results</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>ridge_mse_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>k_folds, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation with Cross-Validation for Ridge</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, lmb <span class="kw">in</span> <span class="bu">enumerate</span>(lambda_grid):</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    mse_sum <span class="op">=</span> np.zeros((k_folds, n_reps))</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> np.random.randn(n)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold_idx, (train_index, val_index) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(np.arange(n))):</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> rep <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            X_train, X_val <span class="op">=</span> X[train_index], X[val_index]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>            y_train, y_val <span class="op">=</span> y[train_index], y[val_index]</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Ridge estimators manually</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>            beta_ridge <span class="op">=</span> compute_ridge_regression(X_train, y_train, lmb)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Predictions &amp; MSE</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            mse_sum[fold_idx, rep] <span class="op">=</span> np.mean((y_val <span class="op">-</span> X_val <span class="op">@</span> beta_ridge) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    ridge_mse_values[i] <span class="op">=</span> mse_sum.mean()  <span class="co"># Average MSE across folds &amp; Monte Carlo repetitions</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>optimal_lambda <span class="op">=</span> lambda_grid[np.argmin(ridge_mse_values)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>             Method  Optimal Lambda (Out-of-Sample)  Out-of-Sample MSE
0               OLS                             NaN           0.750544
1  Ridge Regression                        0.196304           0.726801</code></pre>
The MSE existence theorem essentially states that for some optimal λ, Ridge Regression achieves a lower expected MSE than OLS. This graph shows how the out-of-sample mean squared error (MSE) for Ridge regression varies with different values of the regularization parameter lambda, plotted on a logarithmic scale. The blue curve represents Ridge’s MSE across a wide range of lambda values. As lambda increases from very small to very large values, the model transitions from low bias–high variance to high bias–low variance behavior. When lambda is too small, the model risks overfitting, while large lambdas overly shrink the coefficients, hurting predictive accuracy. The curve dips at a lambda value around 0.1963, where the MSE is minimized. This value represents the optimal balance between bias and variance for this dataset. Beyond this point, the MSE rises rapidly as excessive regularization starts to dominate. The red dashed line indicates the out-of-sample MSE for OLS, which remains constant since OLS does not involve any regularization. The fact that the Ridge curve falls below this line around the optimal lambda shows that Ridge, when properly tuned, can outperform OLS in terms of generalization. <img src="images/Out_of_Sample_MSE.png" class="img-fluid" style="width:80.0%">
<center>
Out-of-Sample MSE vs.&nbsp;Lambda (log scale) for Ridge Regression
</center>
</section>
<section id="bias-and-variance-under-non-linear-dgp" class="level1">
<h1>4. Bias and Variance under Non-Linear DGP</h1>
<p>Non-Linear DGP:<span class="math display">\[y = \frac{1}{1 + \exp(-X\beta)} + \varepsilon\]</span> This is a logistic transformation applied only to the first predictor(X0), meaning only X0 affects y. I’ve decided to only use the first predictor because if other predictors are included linearly, the function would become partially nonlinear and partially linear. This will make it harder to isolate the effects of nonlinearity when comparing OLS and Ridge Regression, essentially making it harder to understand when and why linear models fail. Additionally, the remaining predictors will act as noise, making it a good case for Ridge Regression to shrink irrelevant features.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Non-Linear Data Generating Process (DGP)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_nonlinear_data(n, p):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.randn(n, p)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    y_raw <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">1</span> <span class="op">+</span> X[:, <span class="dv">0</span>]))  <span class="co"># Compute logistic function using only the first predictor</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y_raw <span class="op">+</span> np.random.randn(n) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
Aside from introducing the non-linear transformation through the sigmoid function in the data-generating process, the overall experimental setup remained consistent with the procedures described in Section 3. The same Monte Carlo simulation framework was applied to evaluate the performance of both OLS and Ridge regression under this non-linear setting. For each simulation run, synthetic datasets were generated, models were trained on a subset of the data, and evaluated on held-out test sets to compute key metrics such as bias, variance, and out-of-sample MSE. This allowed for a direct comparison between the linear and non-linear DGPs, highlighting how model assumptions interact with the underlying structure of the data. The plot compares the mean squared error of OLS and Ridge regression across increasing sample sizes under the same non-linear setting. At all sample sizes, Ridge maintains a slight advantage over OLS, consistently producing lower MSE. This suggests that even in the presence of model misspecification due to non-linearity, regularization provides a stabilizing effect by reducing variance without dramatically increasing bias. Both models show increasing MSE with larger sample sizes, which is expected in this case, since the sigmoid-transformed targets are bounded and harder to approximate well with a linear model as more data exposes more of the non-linearity. <img src="images/Non_Linear_MSE_Comparison.png" class="img-fluid" style="width:80.0%">
<center>
Comparison of OLS and Ridge MSE vs Sample Size (Non-Linear DGP)
</center>
The plot shows how Ridge regression performs under a non-linear data-generating process where the response is a sigmoid transformation of a linear combination of predictors. The curve captures out-of-sample MSE across a wide range of lambda values on a logarithmic scale. A noticeable dip occurs around lambda = 0.0069, indicating the optimal regularization strength that minimizes prediction error. Outside this region, the MSE fluctuates more sharply, especially at higher lambda values, where the model likely overshrinks the coefficients. The overall noise in the curve reflects the added complexity of fitting a linear model to a non-linear signal, where small changes in lambda can lead to more volatile performance. <img src="images/Ridge_Out_of_Sample_MSE.png" class="img-fluid" style="width:80.0%">
<center>
Out-of-Sample MSE vs Lambda (Log Scale) under Non-Linear DGP
</center>
<p><strong>B-spline Regression</strong>: I used B-spline regression to get the “best approximate” linear regression coefficients. This cubic B-spline regression with 5 degrees of freedom allows the model to smoothly fit approximate the non-linear DGP, avoiding issues of high-degree polynomials. B-spline regression is considered an OLS model because it follows the same fundamental principles of linear regression, however with a transformed feature space.</p>
<div id="7a7b084c" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply splines to approximate the function and estimate coefficients</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_splines(X, y):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    X_spline <span class="op">=</span> dmatrix(<span class="ss">f"bs(X0, df=5, degree=3, include_intercept=True)"</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                        {<span class="st">"X0"</span>: X[:, <span class="dv">0</span>]}, return_type<span class="op">=</span><span class="st">'matrix'</span>) <span class="co"># Only use the first predictor</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    beta_spline <span class="op">=</span> np.linalg.pinv(X_spline.T <span class="op">@</span> X_spline) <span class="op">@</span> X_spline.T <span class="op">@</span> y</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_spline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The goal here is to test how well spline regression can model non-linear relationships in data. For each sample size, a synthetic dataset is generated where the relationship between the features and the response is curved or non-linear. Instead of fitting a model to all predictors, the simulation focuses on just the first predictor and tries to capture its non-linear effect using spline basis functions.</p>
<p>In each repetition, the data is split into training and test sets. A spline regression model is fit on the training set by expressing the first predictor using a set of cubic spline basis functions. These basis functions break the input into smooth polynomial pieces connected at specific points called knots. Once the model is fit, it’s used to predict outcomes on the test set, and the accuracy of these predictions is measured using mean squared error (MSE).</p>
<p>This process is repeated many times to reduce the effect of randomness from data splits. The final MSE for each sample size is the average over all repetitions. Additionally, the spline coefficients — which determine the shape of the fitted curve — are also averaged across repetitions. This gives a sense of the typical function shape learned by the spline model and helps assess how the fitted relationship evolves as more data becomes available.</p>
<div id="45771233" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, size <span class="kw">in</span> <span class="bu">enumerate</span>(sample_sizes):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    splines_mse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    spline_coef_sum <span class="op">=</span> np.zeros(<span class="dv">6</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> generate_nonlinear_data(size, p)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit splines using only the first predictor</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        beta_spline <span class="op">=</span> fit_splines(X_train, y_train)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        X_test_spline <span class="op">=</span> dmatrix(<span class="ss">f"bs(X0, df=5, degree=3, include_intercept=True)"</span>, {<span class="st">"X0"</span>: X_test[:, <span class="dv">0</span>]}, return_type<span class="op">=</span><span class="st">'matrix'</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict using splines</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        y_pred_splines <span class="op">=</span> X_test_spline <span class="op">@</span> beta_spline</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute MSE</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        splines_mse <span class="op">+=</span> mean_squared_error(y_test, y_pred_splines)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        spline_coef_sum <span class="op">+=</span> beta_spline.ravel()</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    splines_mse_values[idx] <span class="op">=</span> splines_mse <span class="op">/</span> n_reps</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    best_spline_coefficients[idx] <span class="op">=</span> spline_coef_sum <span class="op">/</span> n_reps</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The first plot shows how spline regression is used to approximate a non-linear relationship between the response variable and the first predictor. The black dots represent the true data, while the red curve represents the fitted spline function. The shape of the red curve reflects the flexibility of spline basis functions, which are able to adapt to local changes in the curvature of the data. Compared to linear regression, this approach captures non-linear structure more accurately, especially in regions where the slope changes direction or flattens out.</p>
<p>The second plot tracks the values of the spline basis coefficients as the sample size increases. Each colored line corresponds to one of the coefficients in the B-spline expansion. As the sample size grows, the coefficients begin to stabilize, indicating convergence toward a consistent underlying functional form. This pattern suggests that with more data, the estimated spline model becomes less sensitive to noise and better represents the true signal. The behavior also shows which parts of the spline basis are most influential in modeling the shape of the target curve.</p>
<p>The third plot presents the mean squared error of the spline predictions across different sample sizes. There is a clear downward trend: as the sample size increases, the MSE decreases steadily. This confirms that the spline model generalizes better with more data and is able to capture the non-linear structure more accurately. Unlike linear models, which can suffer from persistent bias under non-linear DGPs, spline regression benefits directly from sample size by refining its fit to the curved underlying function.</p>
<div class="columns">
<div class="column" style="width:33%;">
<img src="images/Spline_approximation_to_Non_Linear_Function.png" class="img-fluid" style="width:100.0%">
<center>
Spline Approximation to Non-Linear Function
</center>
</div><div class="column" style="width:33%;">
<img src="images/Convergence_of_spline_coeff.png" class="img-fluid" style="width:100.0%">
<center>
Convergence of Best Approximate Spline Coefficients
</center>
</div><div class="column" style="width:33%;">
<img src="images/MSE_of_Splines_approx.png" class="img-fluid" style="width:100.0%">
<center>
MSE of Spline Approximation vs Sample Size
</center>
</div>
</div>
<p>References: SNU AI. (n.d.). The bias-variance trade-off: A mathematical view. Medium. Retrieved from https://medium.com/snu-ai/the-bias-variance-trade-off-a-mathematical-view-14ff9dfe5a3c Shubham, S. (n.d.). All about Gauss-Markov theorem for ordinary least squares regression (OLS) &amp; BLUE properties of OLS estimators. Medium. Retrieved from https://medium.com/<span class="citation" data-cites="shubhamsd100/all-about-gauss">@shubhamsd100/all-about-gauss</span>- markov-theorem-for-ordinary-least-squares-regression-ols-blue-properties-of-e1e1864fe087 Weylandt, M. (n.d.). STA9890 notes. Retrieved from https://michael-weylandt.com/STA9890/notes.html</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "STA 9890 Project: Bias and Variance in Linear Regression"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">  output: true</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">  eval: false</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="fu"># 1. The Bias &amp; Variance Tradeoff</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>The bias-variance trade-off is an important concept used to optimize performance out of machine learning models.</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>The total error of a model can be decomposed into Reducible error and Irreducible error respectively. Bias and variance are reducible errors. The irreducible error, often known as noise, is the error that can't be reduced. The best models will always have an error which can't be removed. Therefore, there is a trade-off between bias and variance to decrease the reducible error which essentially minimizes the total error. The total error can be expressed as the following:</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>\text{Err}(x) = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: center;"&gt;&lt;strong&gt;OR&lt;/strong&gt;&lt;/p&gt;</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span> = \text{bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>Before dissecting the equation, lets delve deeper into bias and variance. Bias is the inability of the model to capture the true relationship. Models with high bias are too simple and fail to capture the complexity of the data, leading to higher training and test error. Models with low bias corresponds to a good fit to the training dataset. Variance refers to the amount an estimate would change on using a different training set. Models with high variance implies that the model doesn't perform well on previously unseen data(testing data) even if it fits the training data well. In contrast, low variance implies that the model performs well on the testing set. The models that have low bias and high variance are known as overfitted models. The models that have high bias and low variance are known as underfitted models. The above equation suggests we need to find a model that simultaneosly achieves low bias and low variance. Variance is a non-negative term and bias squared is non-negative term, which implies that the total error can never go below irreducible error.</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>This graph suggests as model complexity increases, bias decreases faster than variance increases, reducing total error. Beyond a point, bias stabilizes while variance grows significantly, increasing total error and that point is the optimal point for minimum total error.</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## 1.1 Derivation of the Equation</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>We have independent variables x that affects the value of a dependent variable y. Function f captures the true relationship between x and y. This is mathemetically expressed as:</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>y = f(x) + \epsilon</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>𝜖 represents the random noise in the relationship between X and Y. 𝜖 has the following properties:</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span> = 0, \quad \text{var}(\epsilon) = \mathbb{E}<span class="co">[</span><span class="ot">\epsilon^2</span><span class="co">]</span> = \sigma_{\epsilon}^2</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>The goal here is to bring the prediction as close as possible to the actual value(y~f(x)) to minimise the error.</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>Coming to the bias-variance decomposition equation;</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span> = \text{bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span>$: Mean Squared Error(MSE). The average squared difference of a prediction f̂(x) from its true value y.</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>$\text{bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> - f(x)$: The difference of the average value of prediction from the true relationship function f(x)</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>$\text{var}(\hat{f}(x)) = \mathbb{E}<span class="co">[</span><span class="ot">(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2</span><span class="co">]</span>$: The expectation of the squared deviation of f̂(x) from its expected value 𝔼<span class="co">[</span><span class="ot">f̂(x)</span><span class="co">]</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>Starting from the LHS,</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span> &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(f(x) + \epsilon - \hat{f}(x))^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(f(x) - \hat{f}(x))^2</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">\epsilon^2</span><span class="co">]</span> + 2\mathbb{E}<span class="co">[</span><span class="ot">(f(x) - \hat{f}(x))\epsilon</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(f(x) - \hat{f}(x))^2</span><span class="co">]</span> + \underbrace{\mathbb{E}<span class="co">[</span><span class="ot">\epsilon^2</span><span class="co">]</span>}_{\sigma^2_{\epsilon}} + 2\mathbb{E}[(f(x) - \hat{f}(x))] \underbrace{\mathbb{E}[\epsilon]}_{=0} <span class="sc">\\</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(f(x) - \hat{f}(x))^2</span><span class="co">]</span> + \sigma^2_{\epsilon}</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>Continuing on the RHS, 𝔼<span class="co">[</span><span class="ot">(f(x) −f̂(x))²</span><span class="co">]</span>;</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(f(x) - \hat{f}(x))^2</span><span class="co">]</span> &amp;= \mathbb{E} \left<span class="co">[</span><span class="ot"> \left( (f(x) - \mathbb{E}[\hat{f}(x)]) - (\hat{f}(x) - \mathbb{E}[\hat{f}(x)]) \right)^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E} \left<span class="co">[</span><span class="ot"> \left( \mathbb{E}[\hat{f}(x)] - f(x) \right)^2 \right</span><span class="co">]</span> + \mathbb{E} \left<span class="co">[</span><span class="ot"> \left( \hat{f}(x) - \mathbb{E}[\hat{f}(x)] \right)^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>&amp;\quad - 2 \mathbb{E} \left<span class="co">[</span><span class="ot"> (f(x) - \mathbb{E}[\hat{f}(x)]) (\hat{f}(x) - \mathbb{E}[\hat{f}(x)]) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>&amp;= \underbrace{(\mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> - f(x))^2}_{\text{bias}^2[\hat{f}(x)]} + \underbrace{\mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2]}_{\text{var}(\hat{f}(x))} <span class="sc">\\</span></span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>&amp;\quad - 2 (f(x) - \mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>) \mathbb{E}<span class="co">[</span><span class="ot">(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>&amp;= \text{bias}^2<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> + \text{var}(\hat{f}(x)) <span class="sc">\\</span></span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>&amp;\quad - 2 (f(x) - \mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>) (\mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>) <span class="sc">\\</span></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>&amp;= \text{bias}^2<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> + \text{var}(\hat{f}(x))</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>plugging this back into the equation for 𝔼<span class="co">[</span><span class="ot">(f(x) −f̂(x))²</span><span class="co">]</span>, we arrive at our bias variance decomposition equation</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span> = \text{bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>**Let's move on and discuss Bias-Variance decomposition in two contexts: Parameter Estimation &amp; Model Misspecification.**</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>Parameter Estimation:</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>Here we're looking at a Linear Data Generating Process(DGP)</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>y = f(x) + \epsilon</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>where f(x) is a linear function</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>If we now fit a linear model(using OLS) to the above linear DGP, our estimator is unbiased</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>\text{Bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> - f(x)</span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> = f(x) \quad \Rightarrow \quad \text{Bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span> = 0</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>Thus, the total error would now just be the variance and irreducible error respectively. Like I mentioned earlier, the irreducible error will always exist and can't be eliminated.</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span> =\text{var}(\hat{f}(x)) + \sigma_\epsilon^2</span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>Misspecified Model:</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a>Here we're looking at a nonlinear DGP</span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a>y = f(x) + \epsilon</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a>where f(x) is a nonlinear function</span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a>If we now fit a linear model to the above nonlinear DGP, our estimator is biased.</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a>$$\text{Bias}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>^2 = (f(x) - E<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>)^2 \neq 0$$</span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a>Thus, the total error would now include bias, variance and irreducible error respectively. Again , the irreducible error can't be eliminated.</span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">(y - \hat{f}(x))^2</span><span class="co">]</span> = \text{bias}<span class="co">[</span><span class="ot">\hat{f}(x)</span><span class="co">]</span>^2 + \text{var}(\hat{f}(x)) + \sigma_\epsilon^2</span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a>**Blue Property**</span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>There are a few key assumptions in the OLS regression and if these assumptions are met, according to the Markov Theorem, the OLS estimates are BLUE-Best Linear Unbiased Estimator. The assumptions are:</span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a>1-Linearity:The relationship between the explanatory variable and the target variable is linear</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a>$$ y = \beta_0 + \beta_1 x + u $$</span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>y:dependent variable</span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a>x:independent variable</span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a>𝛽:Parameter</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a>𝑢:error</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a>2-Independence of errors:There is no relationship between the residuals and the y variable</span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a>3-Normality of errors: The error terms will follow a normal distribution,called multivariate normality.</span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a>4-Homoscedasticity: The error terms have Equal variances.</span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a>5-The successive error terms are uncorrelated with each other, meaning the covariance is zero.If this assumption is violated, then it causes Auto-correlation.</span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a>6-The error term is uncorrelated with the independent variable</span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a>7-No Multicollinearity:The explanatory variables are uncorrelated.</span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a>**Unbiased Property**</span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a>The OLS estimator is given by:</span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a>$$ \hat{\beta}_{OLS} = (X^T X)^{-1} X^T Y $$</span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a>Taking expectations:</span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a>$$ E<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = E<span class="co">[</span><span class="ot">(X^T X)^{-1} X^T Y</span><span class="co">]</span> $$</span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a>Substituting $ Y = X \beta + \epsilon $:</span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a>$$ E<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = (X^T X)^{-1} X^T E<span class="co">[</span><span class="ot">X \beta + \epsilon</span><span class="co">]</span> $$</span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a>By OLS $ E<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span> = 0 $:</span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a>$$ E<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = (X^T X)^{-1} X^T (X \beta) $$</span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a>$$ = (X^T X)^{-1} X^T X \beta $$</span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a>$$ = \beta $$</span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a>Thus, $ \hat{\beta}_{OLS} $ is an unbiased estimator of $ \beta $.</span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a>**Best Property**</span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a>The variance of $ \hat{\beta}_{OLS} $ is:</span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = V<span class="co">[</span><span class="ot">(X^T X)^{-1} X^T Y</span><span class="co">]</span> $$</span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a>Using $ Y = X\beta + \epsilon $:</span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = V<span class="co">[</span><span class="ot">(X^T X)^{-1} X^T (X\beta + \epsilon)</span><span class="co">]</span> $$</span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a>Since $ \beta $ is a constant:</span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = (X^T X)^{-1} X^T V<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span> X (X^T X)^{-1} $$</span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a>Assuming $ \epsilon \sim N(0, \sigma^2 I) $, we substitute $ V<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span> = \sigma^2 I $:</span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = (X^T X)^{-1} X^T (\sigma^2 I) X (X^T X)^{-1} $$</span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a>$$ = \sigma^2 (X^T X)^{-1} $$</span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a>Thus, the variance of the OLS estimator is:</span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = \sigma^2 (X^T X)^{-1} $$</span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a>**Minimum Variance Property**</span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>For any other unbiased linear estimator:</span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>$$ \hat{\beta}_{other} = C Y $$</span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a>where $ C $ is a matrix satisfying $ E<span class="co">[</span><span class="ot">\hat{\beta}_{other}</span><span class="co">]</span> = \beta $.</span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a>Expanding:</span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a>$$ \hat{\beta}_{other} = C(X \beta + \epsilon) $$</span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a>$$ = C X \beta + C \epsilon $$</span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>For unbiasedness:</span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a>$$ C X = I $$</span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a>Thus, $ C $ can be written as:</span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a>$$ C = (X^T X)^{-1} X^T + D $$</span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a>where $ D $ is an arbitrary matrix satisfying $ D X = 0 $.</span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a>Computing variance:</span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{other}</span><span class="co">]</span> = V<span class="co">[</span><span class="ot">(X^T X)^{-1} X^T Y + D Y</span><span class="co">]</span> $$</span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a>Using $ V<span class="co">[</span><span class="ot">\epsilon</span><span class="co">]</span> = \sigma^2 I $:</span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{other}</span><span class="co">]</span> = \sigma^2 (X^T X)^{-1} + \sigma^2 D D^T $$</span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a>Since $ D D^T $ is a positive semi-definite matrix, we conclude:</span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a>$$ V<span class="co">[</span><span class="ot">\hat{\beta}_{other}</span><span class="co">]</span> \geq V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> $$</span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a>which implies:</span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a>$$ E<span class="co">[</span><span class="ot">(\hat{\beta}_{other} - \beta)^2</span><span class="co">]</span> \geq E<span class="co">[</span><span class="ot">(\hat{\beta}_{OLS} - \beta)^2</span><span class="co">]</span> $$</span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a>Since OLS is:</span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Unbiased: $ E<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> = \beta $.</span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Efficient: $ V<span class="co">[</span><span class="ot">\hat{\beta}_{OLS}</span><span class="co">]</span> $ is the lowest possible variance among unbiased estimators.</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a>By the Gauss-Markov theorem:</span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a>$$ \Rightarrow \text{OLS is the Best Linear Unbiased Estimator.} $$</span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a><span class="fu"># 2. Computation</span></span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.1 Derivation of the OLS Closed-Form Solution**</span></span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a>Ordinary Least Squares (OLS) aims to minimize the sum of squared errors between the predicted and actual values in a linear regression model. The problem is formulated as:</span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \frac{1}{n} \| y - X\beta \|_2^2.$$</span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$ y $ is the $ n \times 1 $ vector of observed dependent variable values,  </span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$ X $ is the $ n \times p $ feature matrix (with $ n $ observations and $ p $ features),  </span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$ \beta $ is the $ p \times 1 $ vector of regression coefficients.  </span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a>For convenience, we modify this slightly:</span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \frac{1}{2} \| y - X\beta \|_2^2.$$</span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a>Expanding the squared norm,</span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a>$$\| y - X\beta \|_2^2 = (y - X\beta)^T (y - X\beta).$$</span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a>Thus, the objective function becomes:</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a>$$S(\beta) = \frac{1}{2} (y - X\beta)^T (y - X\beta).$$</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a>Taking the derivative,</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>$$\frac{d}{d\beta} S(\beta) = \frac{1}{2} \cdot 2 X^T (X\beta - y) = X^T (X\beta - y).$$</span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a>Setting the gradient to zero for minimization:</span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a>$$X^T X \beta = X^T y.$$</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a>Assuming $X^T X$ is invertible(not singular or det($X^T X$)=0):</span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta} = (X^T X)^{-1} X^T y.$$</span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-303"><a href="#cb20-303" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-304"><a href="#cb20-304" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-305"><a href="#cb20-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-306"><a href="#cb20-306" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb20-307"><a href="#cb20-307" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb20-308"><a href="#cb20-308" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb20-309"><a href="#cb20-309" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb20-310"><a href="#cb20-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-311"><a href="#cb20-311" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb20-312"><a href="#cb20-312" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb20-313"><a href="#cb20-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-314"><a href="#cb20-314" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute (X^T X) to check if it's invertible</span></span>
<span id="cb20-315"><a href="#cb20-315" aria-hidden="true" tabindex="-1"></a>XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb20-316"><a href="#cb20-316" aria-hidden="true" tabindex="-1"></a>determinant <span class="op">=</span> np.linalg.det(XTX)</span>
<span id="cb20-317"><a href="#cb20-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-318"><a href="#cb20-318" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure determinant is nonzero (invertible)</span></span>
<span id="cb20-319"><a href="#cb20-319" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> determinant <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-320"><a href="#cb20-320" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Singular matrix, choosing a different dataset."</span>)</span>
<span id="cb20-321"><a href="#cb20-321" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb20-322"><a href="#cb20-322" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Solve for weight vector w using the closed-form OLS solution</span></span>
<span id="cb20-323"><a href="#cb20-323" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.linalg.inv(XTX) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb20-324"><a href="#cb20-324" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Beta:</span><span class="ch">\n</span><span class="st">"</span>, beta)</span>
<span id="cb20-325"><a href="#cb20-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-326"><a href="#cb20-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-327"><a href="#cb20-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-328"><a href="#cb20-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-329"><a href="#cb20-329" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-330"><a href="#cb20-330" aria-hidden="true" tabindex="-1"></a><span class="in">    Beta:</span></span>
<span id="cb20-331"><a href="#cb20-331" aria-hidden="true" tabindex="-1"></a><span class="in">     [-55.  90. -40.]</span></span>
<span id="cb20-332"><a href="#cb20-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-333"><a href="#cb20-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-334"><a href="#cb20-334" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.2 Derivation of the Ridge Regression Closed form Solution**</span></span>
<span id="cb20-335"><a href="#cb20-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-336"><a href="#cb20-336" aria-hidden="true" tabindex="-1"></a>Ridge Regression extends Ordinary Least Squares (OLS) by adding a regularization term to prevent overfitting and handle singular matrices. The optimization problem is:</span>
<span id="cb20-337"><a href="#cb20-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-338"><a href="#cb20-338" aria-hidden="true" tabindex="-1"></a>$$\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \frac{1}{2} \| y - X\beta \|_2^2 + \frac{\lambda}{2} \| \beta \|_2^2.$$</span>
<span id="cb20-339"><a href="#cb20-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-340"><a href="#cb20-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-341"><a href="#cb20-341" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\lambda$ is the regularization parameter, controlling the penalty on large coefficients.</span>
<span id="cb20-342"><a href="#cb20-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-343"><a href="#cb20-343" aria-hidden="true" tabindex="-1"></a>The Ridge Regression objective function consists of two terms:</span>
<span id="cb20-344"><a href="#cb20-344" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Residual Sum of Squares (RSS): Measures the error in predictions.</span>
<span id="cb20-345"><a href="#cb20-345" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>L2 Regularization Term: Penalizes large values of $\beta$.</span>
<span id="cb20-346"><a href="#cb20-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-347"><a href="#cb20-347" aria-hidden="true" tabindex="-1"></a>$$\</span>
<span id="cb20-348"><a href="#cb20-348" aria-hidden="true" tabindex="-1"></a>S(\beta) = \frac{1}{2} (y - X\beta)^T (y - X\beta) + \frac{\lambda}{2} \beta^T \beta.</span>
<span id="cb20-349"><a href="#cb20-349" aria-hidden="true" tabindex="-1"></a>\$$</span>
<span id="cb20-350"><a href="#cb20-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-351"><a href="#cb20-351" aria-hidden="true" tabindex="-1"></a>Expanding the first term:</span>
<span id="cb20-352"><a href="#cb20-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-353"><a href="#cb20-353" aria-hidden="true" tabindex="-1"></a>$$\</span>
<span id="cb20-354"><a href="#cb20-354" aria-hidden="true" tabindex="-1"></a>(y - X\beta)^T (y - X\beta) = y^T y - 2\beta^T X^T y + \beta^T X^T X \beta.</span>
<span id="cb20-355"><a href="#cb20-355" aria-hidden="true" tabindex="-1"></a>\$$</span>
<span id="cb20-356"><a href="#cb20-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-357"><a href="#cb20-357" aria-hidden="true" tabindex="-1"></a>Thus, the objective function becomes:</span>
<span id="cb20-358"><a href="#cb20-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-359"><a href="#cb20-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-360"><a href="#cb20-360" aria-hidden="true" tabindex="-1"></a>S(\beta) = \frac{1}{2} \left( y^T y - 2\beta^T X^T y + \beta^T X^T X \beta \right) + \frac{\lambda}{2} \beta^T \beta.</span>
<span id="cb20-361"><a href="#cb20-361" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-362"><a href="#cb20-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-363"><a href="#cb20-363" aria-hidden="true" tabindex="-1"></a>Taking the derivative:</span>
<span id="cb20-364"><a href="#cb20-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-365"><a href="#cb20-365" aria-hidden="true" tabindex="-1"></a>$$\</span>
<span id="cb20-366"><a href="#cb20-366" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\beta} S(\beta) = \frac{1}{2} \left( -2X^T y + 2X^T X \beta \right) + \frac{1}{2} \left( 2\lambda \beta \right).</span>
<span id="cb20-367"><a href="#cb20-367" aria-hidden="true" tabindex="-1"></a>\$$</span>
<span id="cb20-368"><a href="#cb20-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-369"><a href="#cb20-369" aria-hidden="true" tabindex="-1"></a>Simplifying:</span>
<span id="cb20-370"><a href="#cb20-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-371"><a href="#cb20-371" aria-hidden="true" tabindex="-1"></a>$$\</span>
<span id="cb20-372"><a href="#cb20-372" aria-hidden="true" tabindex="-1"></a>X^T X \beta - X^T y + \lambda \beta = 0.</span>
<span id="cb20-373"><a href="#cb20-373" aria-hidden="true" tabindex="-1"></a>\$$</span>
<span id="cb20-374"><a href="#cb20-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-375"><a href="#cb20-375" aria-hidden="true" tabindex="-1"></a>Rearrange to solve for $\beta$:</span>
<span id="cb20-376"><a href="#cb20-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-377"><a href="#cb20-377" aria-hidden="true" tabindex="-1"></a>$$\</span>
<span id="cb20-378"><a href="#cb20-378" aria-hidden="true" tabindex="-1"></a>(X^T X + \lambda I) \beta = X^T y.</span>
<span id="cb20-379"><a href="#cb20-379" aria-hidden="true" tabindex="-1"></a>\$$</span>
<span id="cb20-380"><a href="#cb20-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-381"><a href="#cb20-381" aria-hidden="true" tabindex="-1"></a>$$\</span>
<span id="cb20-382"><a href="#cb20-382" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = (X^T X + \lambda I)^{-1} X^T y.</span>
<span id="cb20-383"><a href="#cb20-383" aria-hidden="true" tabindex="-1"></a>\$$</span>
<span id="cb20-384"><a href="#cb20-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-385"><a href="#cb20-385" aria-hidden="true" tabindex="-1"></a>Key Insights:  </span>
<span id="cb20-386"><a href="#cb20-386" aria-hidden="true" tabindex="-1"></a>Regularization Ensures Invertibility</span>
<span id="cb20-387"><a href="#cb20-387" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Unlike OLS, where $X^T X$ might be singular, Ridge Regression adds $\lambda I$, making $(X^T X + \lambda I)$ always invertible for $\lambda &gt; 0$.</span>
<span id="cb20-388"><a href="#cb20-388" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb20-389"><a href="#cb20-389" aria-hidden="true" tabindex="-1"></a>Prevents Overfitting</span>
<span id="cb20-390"><a href="#cb20-390" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Large values of $\beta$ are penalized, reducing variance and improving generalization.</span>
<span id="cb20-391"><a href="#cb20-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-392"><a href="#cb20-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-393"><a href="#cb20-393" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-394"><a href="#cb20-394" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-395"><a href="#cb20-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-396"><a href="#cb20-396" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb20-397"><a href="#cb20-397" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb20-398"><a href="#cb20-398" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb20-399"><a href="#cb20-399" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb20-400"><a href="#cb20-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-401"><a href="#cb20-401" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb20-402"><a href="#cb20-402" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb20-403"><a href="#cb20-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-404"><a href="#cb20-404" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the regularization parameter (lambda)</span></span>
<span id="cb20-405"><a href="#cb20-405" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-406"><a href="#cb20-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-407"><a href="#cb20-407" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute (X^T X) and add regularization term (lambda * I)</span></span>
<span id="cb20-408"><a href="#cb20-408" aria-hidden="true" tabindex="-1"></a>XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb20-409"><a href="#cb20-409" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(X.shape[<span class="dv">1</span>])  <span class="co"># Identity matrix of size (p x p)</span></span>
<span id="cb20-410"><a href="#cb20-410" aria-hidden="true" tabindex="-1"></a>ridge_matrix <span class="op">=</span> XTX <span class="op">+</span> lambda_ <span class="op">*</span> I</span>
<span id="cb20-411"><a href="#cb20-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-412"><a href="#cb20-412" aria-hidden="true" tabindex="-1"></a><span class="co"># Solve for beta using the closed-form Ridge Regression solution</span></span>
<span id="cb20-413"><a href="#cb20-413" aria-hidden="true" tabindex="-1"></a>beta_ridge <span class="op">=</span> np.linalg.inv(ridge_matrix) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb20-414"><a href="#cb20-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-415"><a href="#cb20-415" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Ridge Regression Beta:</span><span class="ch">\n</span><span class="st">"</span>, beta_ridge)</span>
<span id="cb20-416"><a href="#cb20-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-417"><a href="#cb20-417" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-418"><a href="#cb20-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-419"><a href="#cb20-419" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-420"><a href="#cb20-420" aria-hidden="true" tabindex="-1"></a><span class="in">    Ridge Regression Beta:</span></span>
<span id="cb20-421"><a href="#cb20-421" aria-hidden="true" tabindex="-1"></a><span class="in">     [-1.36034779  2.71427884  0.95101358]</span></span>
<span id="cb20-422"><a href="#cb20-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-423"><a href="#cb20-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-424"><a href="#cb20-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-425"><a href="#cb20-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.3 Derivation of Gradient Descent for OLS</span></span>
<span id="cb20-426"><a href="#cb20-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-427"><a href="#cb20-427" aria-hidden="true" tabindex="-1"></a>Instead of solving $ \beta = (X^T X)^{-1} X^T y $ directly, we use Gradient Descent.</span>
<span id="cb20-428"><a href="#cb20-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-429"><a href="#cb20-429" aria-hidden="true" tabindex="-1"></a>The Mean Squared Error (MSE) is used as the loss function:</span>
<span id="cb20-430"><a href="#cb20-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-431"><a href="#cb20-431" aria-hidden="true" tabindex="-1"></a>$$ S(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - X_i\beta)^2. $$</span>
<span id="cb20-432"><a href="#cb20-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-433"><a href="#cb20-433" aria-hidden="true" tabindex="-1"></a>This can be rewritten in matrix form:</span>
<span id="cb20-434"><a href="#cb20-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-435"><a href="#cb20-435" aria-hidden="true" tabindex="-1"></a>$$ S(\beta) = \frac{1}{2n} (y - X\beta)^T (y - X\beta). $$</span>
<span id="cb20-436"><a href="#cb20-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-437"><a href="#cb20-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-438"><a href="#cb20-438" aria-hidden="true" tabindex="-1"></a>To minimize $ S(\beta) $, we compute its gradient:</span>
<span id="cb20-439"><a href="#cb20-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-440"><a href="#cb20-440" aria-hidden="true" tabindex="-1"></a>$$ \frac{d}{d\beta} S(\beta) = -\frac{1}{n} X^T (y - X\beta). $$</span>
<span id="cb20-441"><a href="#cb20-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-442"><a href="#cb20-442" aria-hidden="true" tabindex="-1"></a>This gradient tells us the direction in which we should move $ \beta $ to reduce the error.</span>
<span id="cb20-443"><a href="#cb20-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-444"><a href="#cb20-444" aria-hidden="true" tabindex="-1"></a>Using Gradient Descent, we iteratively update $ \beta $ as:</span>
<span id="cb20-445"><a href="#cb20-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-446"><a href="#cb20-446" aria-hidden="true" tabindex="-1"></a>$$ \beta^{(t+1)} = \beta^{(t)} - \alpha \cdot \left(-\frac{1}{n} X^T (y - X\beta^{(t)})\right). $$</span>
<span id="cb20-447"><a href="#cb20-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-448"><a href="#cb20-448" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb20-449"><a href="#cb20-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha$ is the learning rate (step size),</span>
<span id="cb20-450"><a href="#cb20-450" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$t$ represents the iteration number.</span>
<span id="cb20-451"><a href="#cb20-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-452"><a href="#cb20-452" aria-hidden="true" tabindex="-1"></a>Simplifying,</span>
<span id="cb20-453"><a href="#cb20-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-454"><a href="#cb20-454" aria-hidden="true" tabindex="-1"></a>$$ \beta^{(t+1)} = \beta^{(t)} + \frac{\alpha}{n} X^T (y - X\beta^{(t)}). $$</span>
<span id="cb20-455"><a href="#cb20-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-456"><a href="#cb20-456" aria-hidden="true" tabindex="-1"></a>This process is repeated until convergence.</span>
<span id="cb20-457"><a href="#cb20-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-458"><a href="#cb20-458" aria-hidden="true" tabindex="-1"></a>Important things to remember:</span>
<span id="cb20-459"><a href="#cb20-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-460"><a href="#cb20-460" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learning Rate $ \alpha $:  </span>
<span id="cb20-461"><a href="#cb20-461" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>If too large, the updates may overshoot and diverge.</span>
<span id="cb20-462"><a href="#cb20-462" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>If too small, the process may take too long.</span>
<span id="cb20-463"><a href="#cb20-463" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-464"><a href="#cb20-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stopping Criteria:  </span>
<span id="cb20-465"><a href="#cb20-465" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>If the change in $ \beta $ is very small, we stop early to save computation.</span>
<span id="cb20-466"><a href="#cb20-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-467"><a href="#cb20-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Feature Scaling:  </span>
<span id="cb20-468"><a href="#cb20-468" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Gradient Descent works better if features are scaled(like using standardization).</span>
<span id="cb20-469"><a href="#cb20-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-470"><a href="#cb20-470" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-471"><a href="#cb20-471" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-472"><a href="#cb20-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-473"><a href="#cb20-473" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb20-474"><a href="#cb20-474" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb20-475"><a href="#cb20-475" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb20-476"><a href="#cb20-476" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb20-477"><a href="#cb20-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-478"><a href="#cb20-478" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb20-479"><a href="#cb20-479" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb20-480"><a href="#cb20-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-481"><a href="#cb20-481" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the learning rate (alpha) and number of iterations</span></span>
<span id="cb20-482"><a href="#cb20-482" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># Learning rate</span></span>
<span id="cb20-483"><a href="#cb20-483" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># Number of iterations</span></span>
<span id="cb20-484"><a href="#cb20-484" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-6</span>  <span class="co"># Convergence threshold</span></span>
<span id="cb20-485"><a href="#cb20-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-486"><a href="#cb20-486" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize beta (coefficients)</span></span>
<span id="cb20-487"><a href="#cb20-487" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> X.shape</span>
<span id="cb20-488"><a href="#cb20-488" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.zeros((p, <span class="dv">1</span>))  <span class="co"># Initialize beta with zeros</span></span>
<span id="cb20-489"><a href="#cb20-489" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Ensure y is a column vector</span></span>
<span id="cb20-490"><a href="#cb20-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-491"><a href="#cb20-491" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient Descent for OLS</span></span>
<span id="cb20-492"><a href="#cb20-492" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb20-493"><a href="#cb20-493" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> beta)  <span class="co"># Compute gradient for OLS</span></span>
<span id="cb20-494"><a href="#cb20-494" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> alpha <span class="op">*</span> gradient  <span class="co"># Update beta</span></span>
<span id="cb20-495"><a href="#cb20-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-496"><a href="#cb20-496" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if the change in beta is very small</span></span>
<span id="cb20-497"><a href="#cb20-497" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.linalg.norm(beta_new <span class="op">-</span> beta, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>) <span class="op">&lt;</span> tolerance:</span>
<span id="cb20-498"><a href="#cb20-498" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Gradient Descent Converged in </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb20-499"><a href="#cb20-499" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb20-500"><a href="#cb20-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-501"><a href="#cb20-501" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta_new  <span class="co"># Update beta for next iteration</span></span>
<span id="cb20-502"><a href="#cb20-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-503"><a href="#cb20-503" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">OLS Beta (Gradient Descent):</span><span class="ch">\n</span><span class="st">"</span>, beta)</span>
<span id="cb20-504"><a href="#cb20-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-505"><a href="#cb20-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-506"><a href="#cb20-506" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-507"><a href="#cb20-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-508"><a href="#cb20-508" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-509"><a href="#cb20-509" aria-hidden="true" tabindex="-1"></a><span class="in">    OLS Beta (Gradient Descent):</span></span>
<span id="cb20-510"><a href="#cb20-510" aria-hidden="true" tabindex="-1"></a><span class="in">     [[-0.58672692]</span></span>
<span id="cb20-511"><a href="#cb20-511" aria-hidden="true" tabindex="-1"></a><span class="in">     [ 1.42766572]</span></span>
<span id="cb20-512"><a href="#cb20-512" aria-hidden="true" tabindex="-1"></a><span class="in">     [ 1.56502404]]</span></span>
<span id="cb20-513"><a href="#cb20-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-514"><a href="#cb20-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-515"><a href="#cb20-515" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.4 OLS Gradient Descent with Weight Decay</span></span>
<span id="cb20-516"><a href="#cb20-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-517"><a href="#cb20-517" aria-hidden="true" tabindex="-1"></a>Weight Decay is a regularization technique that discourages large weights by adding a penalty term to the loss function. It is equivalent to L2 regularization(used in Ridge Regression) and helps prevent overfitting.</span>
<span id="cb20-518"><a href="#cb20-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-519"><a href="#cb20-519" aria-hidden="true" tabindex="-1"></a>The modified OLS loss function with weight decay is:</span>
<span id="cb20-520"><a href="#cb20-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-521"><a href="#cb20-521" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-522"><a href="#cb20-522" aria-hidden="true" tabindex="-1"></a>S(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - X\beta)^2 + \frac{\lambda}{2} \|\beta\|_2^2</span>
<span id="cb20-523"><a href="#cb20-523" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-524"><a href="#cb20-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-525"><a href="#cb20-525" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb20-526"><a href="#cb20-526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The first term is the standard Mean Squared Error (MSE) loss.</span>
<span id="cb20-527"><a href="#cb20-527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The second term is the L2 regularization (weight decay), where $\lambda$ is the weight decay parameter.</span>
<span id="cb20-528"><a href="#cb20-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-529"><a href="#cb20-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-530"><a href="#cb20-530" aria-hidden="true" tabindex="-1"></a>To minimize $S(\beta)$, we compute its gradient.</span>
<span id="cb20-531"><a href="#cb20-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-532"><a href="#cb20-532" aria-hidden="true" tabindex="-1"></a>The gradient of the original OLS loss function is:</span>
<span id="cb20-533"><a href="#cb20-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-534"><a href="#cb20-534" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-535"><a href="#cb20-535" aria-hidden="true" tabindex="-1"></a>\nabla S(\beta)_{\text{OLS}} = -\frac{1}{n} X^T (y - X\beta)</span>
<span id="cb20-536"><a href="#cb20-536" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-537"><a href="#cb20-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-538"><a href="#cb20-538" aria-hidden="true" tabindex="-1"></a>The derivative of $\frac{\lambda}{2} \|\beta\|_2^2$ with respect to $\beta$ is:</span>
<span id="cb20-539"><a href="#cb20-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-540"><a href="#cb20-540" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-541"><a href="#cb20-541" aria-hidden="true" tabindex="-1"></a>\nabla S(\beta)_{\text{reg}} = \lambda \beta</span>
<span id="cb20-542"><a href="#cb20-542" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-543"><a href="#cb20-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-544"><a href="#cb20-544" aria-hidden="true" tabindex="-1"></a>The total gradient of $S(\beta)$ with weight decay is:</span>
<span id="cb20-545"><a href="#cb20-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-546"><a href="#cb20-546" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-547"><a href="#cb20-547" aria-hidden="true" tabindex="-1"></a>\nabla S(\beta) = -\frac{1}{n} X^T (y - X\beta) + \lambda \beta</span>
<span id="cb20-548"><a href="#cb20-548" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-549"><a href="#cb20-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-550"><a href="#cb20-550" aria-hidden="true" tabindex="-1"></a>Using gradient descent, we update $\beta$ iteratively:</span>
<span id="cb20-551"><a href="#cb20-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-552"><a href="#cb20-552" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-553"><a href="#cb20-553" aria-hidden="true" tabindex="-1"></a>\beta^{(t+1)} = \beta^{(t)} + \alpha \left( \frac{1}{n} X^T (y - X\beta^{(t)}) - \lambda \beta^{(t)} \right)</span>
<span id="cb20-554"><a href="#cb20-554" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-555"><a href="#cb20-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-556"><a href="#cb20-556" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb20-557"><a href="#cb20-557" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha$ is the learning rate.</span>
<span id="cb20-558"><a href="#cb20-558" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\lambda$ is the weight decay parameter.</span>
<span id="cb20-559"><a href="#cb20-559" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The second term shrinks the weights, preventing them from growing too large.</span>
<span id="cb20-560"><a href="#cb20-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-561"><a href="#cb20-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-562"><a href="#cb20-562" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-563"><a href="#cb20-563" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-564"><a href="#cb20-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-565"><a href="#cb20-565" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a data matrix</span></span>
<span id="cb20-566"><a href="#cb20-566" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>],</span>
<span id="cb20-567"><a href="#cb20-567" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb20-568"><a href="#cb20-568" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">10</span>]])</span>
<span id="cb20-569"><a href="#cb20-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-570"><a href="#cb20-570" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the target vector</span></span>
<span id="cb20-571"><a href="#cb20-571" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">25</span>])</span>
<span id="cb20-572"><a href="#cb20-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-573"><a href="#cb20-573" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the regularization parameter (lambda) and learning rate (alpha)</span></span>
<span id="cb20-574"><a href="#cb20-574" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-575"><a href="#cb20-575" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb20-576"><a href="#cb20-576" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb20-577"><a href="#cb20-577" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb20-578"><a href="#cb20-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-579"><a href="#cb20-579" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize beta (coefficients)</span></span>
<span id="cb20-580"><a href="#cb20-580" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> X.shape</span>
<span id="cb20-581"><a href="#cb20-581" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.zeros((p, <span class="dv">1</span>))</span>
<span id="cb20-582"><a href="#cb20-582" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-583"><a href="#cb20-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-584"><a href="#cb20-584" aria-hidden="true" tabindex="-1"></a><span class="co"># OLS Gradient Descent with weight decay</span></span>
<span id="cb20-585"><a href="#cb20-585" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb20-586"><a href="#cb20-586" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> beta) <span class="op">-</span> lambda_ <span class="op">*</span> beta</span>
<span id="cb20-587"><a href="#cb20-587" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> alpha <span class="op">*</span> gradient</span>
<span id="cb20-588"><a href="#cb20-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-589"><a href="#cb20-589" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if the change in beta is very small</span></span>
<span id="cb20-590"><a href="#cb20-590" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.linalg.norm(beta_new <span class="op">-</span> beta, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>) <span class="op">&lt;</span> tolerance:</span>
<span id="cb20-591"><a href="#cb20-591" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Gradient Descent Converged in </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb20-592"><a href="#cb20-592" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb20-593"><a href="#cb20-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-594"><a href="#cb20-594" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta_new  <span class="co"># Update beta for next iteration</span></span>
<span id="cb20-595"><a href="#cb20-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-596"><a href="#cb20-596" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Ridge Regression Beta (Gradient Descent):</span><span class="ch">\n</span><span class="st">"</span>, beta)</span>
<span id="cb20-597"><a href="#cb20-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-598"><a href="#cb20-598" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-599"><a href="#cb20-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-600"><a href="#cb20-600" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-601"><a href="#cb20-601" aria-hidden="true" tabindex="-1"></a><span class="in">    Ridge Regression Beta (Gradient Descent):</span></span>
<span id="cb20-602"><a href="#cb20-602" aria-hidden="true" tabindex="-1"></a><span class="in">     [[-0.39537272]</span></span>
<span id="cb20-603"><a href="#cb20-603" aria-hidden="true" tabindex="-1"></a><span class="in">     [ 1.19539342]</span></span>
<span id="cb20-604"><a href="#cb20-604" aria-hidden="true" tabindex="-1"></a><span class="in">     [ 1.64430062]]</span></span>
<span id="cb20-605"><a href="#cb20-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-606"><a href="#cb20-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-607"><a href="#cb20-607" aria-hidden="true" tabindex="-1"></a>**Justification for Low $ \lambda $ (Weight Decay) and $ \alpha $ (Learning Rate)**</span>
<span id="cb20-608"><a href="#cb20-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-609"><a href="#cb20-609" aria-hidden="true" tabindex="-1"></a>1.Why Use a Low $ \lambda $?</span>
<span id="cb20-610"><a href="#cb20-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-611"><a href="#cb20-611" aria-hidden="true" tabindex="-1"></a>The regularization term $ \frac{\lambda}{2} \|\beta\|_2^2 $ discourages large weights, improving generalization.</span>
<span id="cb20-612"><a href="#cb20-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-613"><a href="#cb20-613" aria-hidden="true" tabindex="-1"></a>A high $ \lambda $ can:</span>
<span id="cb20-614"><a href="#cb20-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Over-penalize large coefficients, causing underfitting.</span>
<span id="cb20-615"><a href="#cb20-615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Slow down convergence in Gradient Descent.</span>
<span id="cb20-616"><a href="#cb20-616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shrink weights excessively, distorting data relationships.</span>
<span id="cb20-617"><a href="#cb20-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-618"><a href="#cb20-618" aria-hidden="true" tabindex="-1"></a>A low $ \lambda $:</span>
<span id="cb20-619"><a href="#cb20-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Preserves meaningful information.</span>
<span id="cb20-620"><a href="#cb20-620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Balances regularization and model complexity.</span>
<span id="cb20-621"><a href="#cb20-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensures smooth weight decay without over-shrinking.</span>
<span id="cb20-622"><a href="#cb20-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-623"><a href="#cb20-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-624"><a href="#cb20-624" aria-hidden="true" tabindex="-1"></a>2.Why Use a Low $ \alpha $?</span>
<span id="cb20-625"><a href="#cb20-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-626"><a href="#cb20-626" aria-hidden="true" tabindex="-1"></a>The learning rate $ \alpha $ controls the step size in Gradient Descent:</span>
<span id="cb20-627"><a href="#cb20-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-628"><a href="#cb20-628" aria-hidden="true" tabindex="-1"></a>A high $ \alpha $:</span>
<span id="cb20-629"><a href="#cb20-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Causes instability, leading to divergence.</span>
<span id="cb20-630"><a href="#cb20-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interferes with weight decay, making updates too aggressive.</span>
<span id="cb20-631"><a href="#cb20-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-632"><a href="#cb20-632" aria-hidden="true" tabindex="-1"></a>A low $ \alpha $:</span>
<span id="cb20-633"><a href="#cb20-633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensures stable convergence.</span>
<span id="cb20-634"><a href="#cb20-634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prevents oscillations in parameter updates.</span>
<span id="cb20-635"><a href="#cb20-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works well with regularization for smooth training.</span>
<span id="cb20-636"><a href="#cb20-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-637"><a href="#cb20-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-638"><a href="#cb20-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-639"><a href="#cb20-639" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.5 Empirical Demonstration of the equivalence of (OLS+Weight Decay) with Ridge Regression</span></span>
<span id="cb20-640"><a href="#cb20-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-641"><a href="#cb20-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-642"><a href="#cb20-642" aria-hidden="true" tabindex="-1"></a>Ridge Regression optimizes:</span>
<span id="cb20-643"><a href="#cb20-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-644"><a href="#cb20-644" aria-hidden="true" tabindex="-1"></a>    \hat{\beta}_{{ridge}} = (X^TX + \lambda I)^{-1}X^Ty</span>
<span id="cb20-645"><a href="#cb20-645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-646"><a href="#cb20-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-647"><a href="#cb20-647" aria-hidden="true" tabindex="-1"></a>Weight Decay modifies the Gradient Descent update as:</span>
<span id="cb20-648"><a href="#cb20-648" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-649"><a href="#cb20-649" aria-hidden="true" tabindex="-1"></a>    \beta^{(t+1)} = \beta^{(t)} + \alpha \left( \frac{1}{n} X^T(y - X\beta^{(t)}) - \lambda \beta^{(t)} \right)</span>
<span id="cb20-650"><a href="#cb20-650" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-651"><a href="#cb20-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-652"><a href="#cb20-652" aria-hidden="true" tabindex="-1"></a>As iterations $\to \infty$, the Gradient Descent solution should converge to Ridge Regression.</span>
<span id="cb20-653"><a href="#cb20-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-654"><a href="#cb20-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-655"><a href="#cb20-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-656"><a href="#cb20-656" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-657"><a href="#cb20-657" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-658"><a href="#cb20-658" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-659"><a href="#cb20-659" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-660"><a href="#cb20-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-661"><a href="#cb20-661" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb20-662"><a href="#cb20-662" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb20-663"><a href="#cb20-663" aria-hidden="true" tabindex="-1"></a>n_samples, n_features <span class="op">=</span> <span class="dv">100</span>, <span class="dv">5</span></span>
<span id="cb20-664"><a href="#cb20-664" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(n_samples, n_features)</span>
<span id="cb20-665"><a href="#cb20-665" aria-hidden="true" tabindex="-1"></a>true_beta <span class="op">=</span> np.random.randn(n_features, <span class="dv">1</span>)</span>
<span id="cb20-666"><a href="#cb20-666" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">@</span> true_beta <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> np.random.randn(n_samples, <span class="dv">1</span>)</span>
<span id="cb20-667"><a href="#cb20-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-668"><a href="#cb20-668" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the regularization parameter and learning rate</span></span>
<span id="cb20-669"><a href="#cb20-669" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-670"><a href="#cb20-670" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># Learning rate</span></span>
<span id="cb20-671"><a href="#cb20-671" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">5000</span>  <span class="co"># Large number of iterations to ensure convergence</span></span>
<span id="cb20-672"><a href="#cb20-672" aria-hidden="true" tabindex="-1"></a>tolerance <span class="op">=</span> <span class="fl">1e-8</span>  <span class="co"># Lower tolerance for better accuracy</span></span>
<span id="cb20-673"><a href="#cb20-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-674"><a href="#cb20-674" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge Regression (Closed-Form Solution)</span></span>
<span id="cb20-675"><a href="#cb20-675" aria-hidden="true" tabindex="-1"></a>XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb20-676"><a href="#cb20-676" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(X.shape[<span class="dv">1</span>])  <span class="co"># Identity matrix</span></span>
<span id="cb20-677"><a href="#cb20-677" aria-hidden="true" tabindex="-1"></a>ridge_matrix <span class="op">=</span> XTX <span class="op">+</span> lambda_ <span class="op">*</span> I</span>
<span id="cb20-678"><a href="#cb20-678" aria-hidden="true" tabindex="-1"></a>beta_ridge <span class="op">=</span> np.linalg.inv(ridge_matrix) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb20-679"><a href="#cb20-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-680"><a href="#cb20-680" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize beta (coefficients) for Gradient Descent</span></span>
<span id="cb20-681"><a href="#cb20-681" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> X.shape</span>
<span id="cb20-682"><a href="#cb20-682" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.zeros((p, <span class="dv">1</span>))  <span class="co"># Initialize beta with zeros</span></span>
<span id="cb20-683"><a href="#cb20-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-684"><a href="#cb20-684" aria-hidden="true" tabindex="-1"></a><span class="co"># Store error for visualization</span></span>
<span id="cb20-685"><a href="#cb20-685" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb20-686"><a href="#cb20-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-687"><a href="#cb20-687" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient Descent for OLS with Weight Decay</span></span>
<span id="cb20-688"><a href="#cb20-688" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb20-689"><a href="#cb20-689" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> beta) <span class="op">-</span> lambda_ <span class="op">*</span> beta</span>
<span id="cb20-690"><a href="#cb20-690" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> alpha <span class="op">*</span> gradient</span>
<span id="cb20-691"><a href="#cb20-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-692"><a href="#cb20-692" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute error between Ridge and GD solutions</span></span>
<span id="cb20-693"><a href="#cb20-693" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> np.linalg.norm(beta_new <span class="op">-</span> beta_ridge, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-694"><a href="#cb20-694" aria-hidden="true" tabindex="-1"></a>    errors.append(error)</span>
<span id="cb20-695"><a href="#cb20-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-696"><a href="#cb20-696" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Stop if close to the closed-form solution</span></span>
<span id="cb20-697"><a href="#cb20-697" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> error <span class="op">&lt;</span> tolerance:</span>
<span id="cb20-698"><a href="#cb20-698" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Gradient Descent Converged in </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> iterations"</span>)</span>
<span id="cb20-699"><a href="#cb20-699" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb20-700"><a href="#cb20-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-701"><a href="#cb20-701" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> beta_new</span>
<span id="cb20-702"><a href="#cb20-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-703"><a href="#cb20-703" aria-hidden="true" tabindex="-1"></a>beta_weight_decay <span class="op">=</span> beta  <span class="co"># Store final result</span></span>
<span id="cb20-704"><a href="#cb20-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-705"><a href="#cb20-705" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare numerical results</span></span>
<span id="cb20-706"><a href="#cb20-706" aria-hidden="true" tabindex="-1"></a>df_results <span class="op">=</span> pd.DataFrame({<span class="st">"Ridge Closed-Form"</span>: beta_ridge.flatten(),</span>
<span id="cb20-707"><a href="#cb20-707" aria-hidden="true" tabindex="-1"></a>                           <span class="st">"Gradient Descent (Weight Decay)"</span>: beta_weight_decay.flatten()})</span>
<span id="cb20-708"><a href="#cb20-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-709"><a href="#cb20-709" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Comparison of Ridge Regression and OLS with Weight Decay:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb20-710"><a href="#cb20-710" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_results)</span>
<span id="cb20-711"><a href="#cb20-711" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-712"><a href="#cb20-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-713"><a href="#cb20-713" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-714"><a href="#cb20-714" aria-hidden="true" tabindex="-1"></a><span class="in">    Comparison of Ridge Regression and OLS with Weight Decay:</span></span>
<span id="cb20-715"><a href="#cb20-715" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-716"><a href="#cb20-716" aria-hidden="true" tabindex="-1"></a><span class="in">       Ridge Closed-Form  Gradient Descent (Weight Decay)</span></span>
<span id="cb20-717"><a href="#cb20-717" aria-hidden="true" tabindex="-1"></a><span class="in">    0           0.922036                         0.778632</span></span>
<span id="cb20-718"><a href="#cb20-718" aria-hidden="true" tabindex="-1"></a><span class="in">    1           1.900938                         1.688677</span></span>
<span id="cb20-719"><a href="#cb20-719" aria-hidden="true" tabindex="-1"></a><span class="in">    2          -1.388070                        -1.233538</span></span>
<span id="cb20-720"><a href="#cb20-720" aria-hidden="true" tabindex="-1"></a><span class="in">    3           0.567381                         0.509230</span></span>
<span id="cb20-721"><a href="#cb20-721" aria-hidden="true" tabindex="-1"></a><span class="in">    4          -0.626401                        -0.585660</span></span>
<span id="cb20-722"><a href="#cb20-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-723"><a href="#cb20-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-724"><a href="#cb20-724" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb20-725"><a href="#cb20-725" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb20-726"><a href="#cb20-726" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Convergence.png)</span>{width=100%}</span>
<span id="cb20-727"><a href="#cb20-727" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Convergence of OLS with Weight Decay to Ridge Regression&lt;/center&gt;</span>
<span id="cb20-728"><a href="#cb20-728" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-729"><a href="#cb20-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-730"><a href="#cb20-730" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb20-731"><a href="#cb20-731" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Comparison.png)</span>{width=100%}</span>
<span id="cb20-732"><a href="#cb20-732" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Comparison of Ridge Regression and OLS with Weight Decay&lt;/center&gt;</span>
<span id="cb20-733"><a href="#cb20-733" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-734"><a href="#cb20-734" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-735"><a href="#cb20-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-736"><a href="#cb20-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-737"><a href="#cb20-737" aria-hidden="true" tabindex="-1"></a><span class="fu"># 3. Bias and Variance under Linear DGP</span></span>
<span id="cb20-738"><a href="#cb20-738" aria-hidden="true" tabindex="-1"></a>Linear DGP: y=Xβ+ϵ</span>
<span id="cb20-739"><a href="#cb20-739" aria-hidden="true" tabindex="-1"></a>We assume no multicollinearity among predictors in a linear DGP. However, with real data there will be correlation</span>
<span id="cb20-740"><a href="#cb20-740" aria-hidden="true" tabindex="-1"></a>between predictors, violating the assumption of independent features. I wanted to introduce controlled correlation</span>
<span id="cb20-741"><a href="#cb20-741" aria-hidden="true" tabindex="-1"></a>using an AR(2) model, which models each predictor as a function of its two preceding values. I was really interested</span>
<span id="cb20-742"><a href="#cb20-742" aria-hidden="true" tabindex="-1"></a>to explore how OLS and Ridge Regression behave when predictors are not independent. </span>
<span id="cb20-743"><a href="#cb20-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-746"><a href="#cb20-746" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-747"><a href="#cb20-747" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate AR(2) covariance matrix</span></span>
<span id="cb20-748"><a href="#cb20-748" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_ar2_cov(p, rho1, rho2):</span>
<span id="cb20-749"><a href="#cb20-749" aria-hidden="true" tabindex="-1"></a>    Sigma <span class="op">=</span> np.zeros((p, p))</span>
<span id="cb20-750"><a href="#cb20-750" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb20-751"><a href="#cb20-751" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb20-752"><a href="#cb20-752" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">==</span> j:</span>
<span id="cb20-753"><a href="#cb20-753" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-754"><a href="#cb20-754" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">abs</span>(i <span class="op">-</span> j) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb20-755"><a href="#cb20-755" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> rho1</span>
<span id="cb20-756"><a href="#cb20-756" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">abs</span>(i <span class="op">-</span> j) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb20-757"><a href="#cb20-757" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> rho2</span>
<span id="cb20-758"><a href="#cb20-758" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-759"><a href="#cb20-759" aria-hidden="true" tabindex="-1"></a>                Sigma[i, j] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-760"><a href="#cb20-760" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Sigma</span>
<span id="cb20-761"><a href="#cb20-761" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-762"><a href="#cb20-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-765"><a href="#cb20-765" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-766"><a href="#cb20-766" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to compute OLS</span></span>
<span id="cb20-767"><a href="#cb20-767" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_estimators_OLS(X, y):</span>
<span id="cb20-768"><a href="#cb20-768" aria-hidden="true" tabindex="-1"></a>    beta_ols <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb20-769"><a href="#cb20-769" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_ols</span>
<span id="cb20-770"><a href="#cb20-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-771"><a href="#cb20-771" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to compute Ridge</span></span>
<span id="cb20-772"><a href="#cb20-772" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_ridge_regression(X, y, lambda_):</span>
<span id="cb20-773"><a href="#cb20-773" aria-hidden="true" tabindex="-1"></a>    XTX <span class="op">=</span> X.T <span class="op">@</span> X</span>
<span id="cb20-774"><a href="#cb20-774" aria-hidden="true" tabindex="-1"></a>    I <span class="op">=</span> np.eye(X.shape[<span class="dv">1</span>])  <span class="co"># Identity matrix</span></span>
<span id="cb20-775"><a href="#cb20-775" aria-hidden="true" tabindex="-1"></a>    ridge_matrix <span class="op">=</span> XTX <span class="op">+</span> lambda_ <span class="op">*</span> I</span>
<span id="cb20-776"><a href="#cb20-776" aria-hidden="true" tabindex="-1"></a>    beta_ridge <span class="op">=</span> np.linalg.inv(ridge_matrix) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb20-777"><a href="#cb20-777" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_ridge</span>
<span id="cb20-778"><a href="#cb20-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-779"><a href="#cb20-779" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-780"><a href="#cb20-780" aria-hidden="true" tabindex="-1"></a>I performed the Montecarlo simulations separately for OLS and Ridge Regression. In the OLS simulation, we compute the regression coefficients using the closed-form least squares estimator. For each repetition, the training set is used to fit the model, and predictions are made on both training and held-out test sets. The mean squared error (MSE), bias, and variance of the OLS estimates are computed and averaged over multiple repetitions to reduce randomness. The bias is approximated as the mean deviation of the estimated coefficients from the true coefficients, while the variance is measured as the variability of the estimated coefficients.</span>
<span id="cb20-781"><a href="#cb20-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-782"><a href="#cb20-782" aria-hidden="true" tabindex="-1"></a>In contrast, the Ridge regression simulation includes an additional loop over a grid of regularization parameters (lambda values). For each lambda, the Ridge estimator is used to compute the regression coefficients, and similar metrics—MSE, bias, variance, and out-of-sample error—are calculated. These values are averaged over repetitions for each lambda and then averaged again across all lambdas to obtain a summary measure for each sample size. While this approach provides a general picture of Ridge performance, a more refined strategy might involve selecting the lambda that minimizes out-of-sample error for each repetition.</span>
<span id="cb20-783"><a href="#cb20-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-784"><a href="#cb20-784" aria-hidden="true" tabindex="-1"></a>Both simulations aim to capture how the MSE decomposes into bias and variance components and how these change with increasing sample size. By comparing OLS and Ridge regression, the code highlights how regularization reduces variance at the cost of increased bias, often leading to better generalization on unseen data, especially when the sample size is small or when multicollinearity is present in the predictors.</span>
<span id="cb20-787"><a href="#cb20-787" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-788"><a href="#cb20-788" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for OLS</span></span>
<span id="cb20-789"><a href="#cb20-789" aria-hidden="true" tabindex="-1"></a>ols_mse_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-790"><a href="#cb20-790" aria-hidden="true" tabindex="-1"></a>ols_bias_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-791"><a href="#cb20-791" aria-hidden="true" tabindex="-1"></a>ols_variance_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-792"><a href="#cb20-792" aria-hidden="true" tabindex="-1"></a>ols_out_sample_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-793"><a href="#cb20-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-794"><a href="#cb20-794" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation for OLS</span></span>
<span id="cb20-795"><a href="#cb20-795" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, size <span class="kw">in</span> <span class="bu">enumerate</span>(sample_sizes):</span>
<span id="cb20-796"><a href="#cb20-796" aria-hidden="true" tabindex="-1"></a>    ols_mse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-797"><a href="#cb20-797" aria-hidden="true" tabindex="-1"></a>    ols_bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-798"><a href="#cb20-798" aria-hidden="true" tabindex="-1"></a>    ols_variance <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-799"><a href="#cb20-799" aria-hidden="true" tabindex="-1"></a>    out_sample_mse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-800"><a href="#cb20-800" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.multivariate_normal(np.zeros(p), cov_matrix, size<span class="op">=</span>size)</span>
<span id="cb20-801"><a href="#cb20-801" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb20-802"><a href="#cb20-802" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate new data for each repetition</span></span>
<span id="cb20-803"><a href="#cb20-803" aria-hidden="true" tabindex="-1"></a>        y_subsample <span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> np.random.randn(size)</span>
<span id="cb20-804"><a href="#cb20-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-805"><a href="#cb20-805" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train/Test Split</span></span>
<span id="cb20-806"><a href="#cb20-806" aria-hidden="true" tabindex="-1"></a>        X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_subsample, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-807"><a href="#cb20-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-808"><a href="#cb20-808" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute OLS estimators</span></span>
<span id="cb20-809"><a href="#cb20-809" aria-hidden="true" tabindex="-1"></a>        beta_ols <span class="op">=</span> compute_estimators_OLS(X_train, y_train)</span>
<span id="cb20-810"><a href="#cb20-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-811"><a href="#cb20-811" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute Predictions</span></span>
<span id="cb20-812"><a href="#cb20-812" aria-hidden="true" tabindex="-1"></a>        y_pred_ols_train <span class="op">=</span> X_train <span class="op">@</span> beta_ols</span>
<span id="cb20-813"><a href="#cb20-813" aria-hidden="true" tabindex="-1"></a>        y_pred_ols_test <span class="op">=</span> X_test <span class="op">@</span> beta_ols</span>
<span id="cb20-814"><a href="#cb20-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-815"><a href="#cb20-815" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute MSE, Bias, and Variance</span></span>
<span id="cb20-816"><a href="#cb20-816" aria-hidden="true" tabindex="-1"></a>        ols_mse <span class="op">+=</span> np.mean((y_train <span class="op">-</span> y_pred_ols_train) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-817"><a href="#cb20-817" aria-hidden="true" tabindex="-1"></a>        ols_bias <span class="op">+=</span> np.mean(beta_ols <span class="op">-</span> beta_true)</span>
<span id="cb20-818"><a href="#cb20-818" aria-hidden="true" tabindex="-1"></a>        ols_variance <span class="op">+=</span> np.var(beta_ols)</span>
<span id="cb20-819"><a href="#cb20-819" aria-hidden="true" tabindex="-1"></a>        out_sample_mse <span class="op">+=</span> np.mean((y_test <span class="op">-</span> y_pred_ols_test) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-820"><a href="#cb20-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-821"><a href="#cb20-821" aria-hidden="true" tabindex="-1"></a>    ols_mse_values[idx] <span class="op">=</span> ols_mse <span class="op">/</span> n_reps</span>
<span id="cb20-822"><a href="#cb20-822" aria-hidden="true" tabindex="-1"></a>    ols_bias_values[idx] <span class="op">=</span> ols_bias <span class="op">/</span> n_reps</span>
<span id="cb20-823"><a href="#cb20-823" aria-hidden="true" tabindex="-1"></a>    ols_variance_values[idx] <span class="op">=</span> ols_variance <span class="op">/</span> n_reps</span>
<span id="cb20-824"><a href="#cb20-824" aria-hidden="true" tabindex="-1"></a>    ols_out_sample_mse[idx] <span class="op">=</span> out_sample_mse <span class="op">/</span> n_reps</span>
<span id="cb20-825"><a href="#cb20-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-826"><a href="#cb20-826" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-829"><a href="#cb20-829" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-830"><a href="#cb20-830" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for Ridge Regression</span></span>
<span id="cb20-831"><a href="#cb20-831" aria-hidden="true" tabindex="-1"></a>ridge_mse_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-832"><a href="#cb20-832" aria-hidden="true" tabindex="-1"></a>ridge_bias_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-833"><a href="#cb20-833" aria-hidden="true" tabindex="-1"></a>ridge_variance_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-834"><a href="#cb20-834" aria-hidden="true" tabindex="-1"></a>ridge_mse_per_sample <span class="op">=</span> []</span>
<span id="cb20-835"><a href="#cb20-835" aria-hidden="true" tabindex="-1"></a>ridge_out_per_sample <span class="op">=</span> []</span>
<span id="cb20-836"><a href="#cb20-836" aria-hidden="true" tabindex="-1"></a>ridge_out_sample_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(sample_sizes))</span>
<span id="cb20-837"><a href="#cb20-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-838"><a href="#cb20-838" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation for Ridge</span></span>
<span id="cb20-839"><a href="#cb20-839" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, size <span class="kw">in</span> <span class="bu">enumerate</span>(sample_sizes):</span>
<span id="cb20-840"><a href="#cb20-840" aria-hidden="true" tabindex="-1"></a>    ridge_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb20-841"><a href="#cb20-841" aria-hidden="true" tabindex="-1"></a>    ridge_bias <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb20-842"><a href="#cb20-842" aria-hidden="true" tabindex="-1"></a>    ridge_variance <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb20-843"><a href="#cb20-843" aria-hidden="true" tabindex="-1"></a>    out_sample_mse <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb20-844"><a href="#cb20-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-845"><a href="#cb20-845" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, lmb <span class="kw">in</span> <span class="bu">enumerate</span>(lambda_grid):</span>
<span id="cb20-846"><a href="#cb20-846" aria-hidden="true" tabindex="-1"></a>        mse_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-847"><a href="#cb20-847" aria-hidden="true" tabindex="-1"></a>        bias_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-848"><a href="#cb20-848" aria-hidden="true" tabindex="-1"></a>        variance_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-849"><a href="#cb20-849" aria-hidden="true" tabindex="-1"></a>        out_mse_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-850"><a href="#cb20-850" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> np.random.multivariate_normal(np.zeros(p), cov_matrix, size<span class="op">=</span>size)</span>
<span id="cb20-851"><a href="#cb20-851" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb20-852"><a href="#cb20-852" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Generate new data for each repetition</span></span>
<span id="cb20-853"><a href="#cb20-853" aria-hidden="true" tabindex="-1"></a>            y_subsample <span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> np.random.randn(size)</span>
<span id="cb20-854"><a href="#cb20-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-855"><a href="#cb20-855" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Train-test split</span></span>
<span id="cb20-856"><a href="#cb20-856" aria-hidden="true" tabindex="-1"></a>            X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y_subsample, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-857"><a href="#cb20-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-858"><a href="#cb20-858" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Ridge estimators</span></span>
<span id="cb20-859"><a href="#cb20-859" aria-hidden="true" tabindex="-1"></a>            beta_ridge <span class="op">=</span> compute_estimators_Ridge(X_train, y_train, lmb)</span>
<span id="cb20-860"><a href="#cb20-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-861"><a href="#cb20-861" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Predictions</span></span>
<span id="cb20-862"><a href="#cb20-862" aria-hidden="true" tabindex="-1"></a>            y_pred_ridge_train <span class="op">=</span> X_train <span class="op">@</span> beta_ridge</span>
<span id="cb20-863"><a href="#cb20-863" aria-hidden="true" tabindex="-1"></a>            y_pred_ridge_test <span class="op">=</span> X_test <span class="op">@</span> beta_ridge</span>
<span id="cb20-864"><a href="#cb20-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-865"><a href="#cb20-865" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute MSE, Bias, and Variance</span></span>
<span id="cb20-866"><a href="#cb20-866" aria-hidden="true" tabindex="-1"></a>            mse_sum <span class="op">+=</span> np.mean((y_train <span class="op">-</span> y_pred_ridge_train) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-867"><a href="#cb20-867" aria-hidden="true" tabindex="-1"></a>            bias_sum <span class="op">+=</span> np.mean(beta_ridge <span class="op">-</span> beta_true)</span>
<span id="cb20-868"><a href="#cb20-868" aria-hidden="true" tabindex="-1"></a>            variance_sum <span class="op">+=</span> np.var(beta_ridge)</span>
<span id="cb20-869"><a href="#cb20-869" aria-hidden="true" tabindex="-1"></a>            out_mse_sum <span class="op">+=</span> np.mean((y_test <span class="op">-</span> y_pred_ridge_test) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-870"><a href="#cb20-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-871"><a href="#cb20-871" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store averaged values for Ridge</span></span>
<span id="cb20-872"><a href="#cb20-872" aria-hidden="true" tabindex="-1"></a>        ridge_mse[i] <span class="op">=</span> mse_sum <span class="op">/</span> n_reps</span>
<span id="cb20-873"><a href="#cb20-873" aria-hidden="true" tabindex="-1"></a>        ridge_bias[i] <span class="op">=</span> bias_sum <span class="op">/</span> n_reps</span>
<span id="cb20-874"><a href="#cb20-874" aria-hidden="true" tabindex="-1"></a>        ridge_variance[i] <span class="op">=</span> variance_sum <span class="op">/</span> n_reps</span>
<span id="cb20-875"><a href="#cb20-875" aria-hidden="true" tabindex="-1"></a>        out_sample_mse[i] <span class="op">=</span> out_mse_sum <span class="op">/</span> n_reps</span>
<span id="cb20-876"><a href="#cb20-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-877"><a href="#cb20-877" aria-hidden="true" tabindex="-1"></a>    ridge_mse_values[idx] <span class="op">=</span> np.mean(ridge_mse)</span>
<span id="cb20-878"><a href="#cb20-878" aria-hidden="true" tabindex="-1"></a>    ridge_bias_values[idx] <span class="op">=</span> np.mean(ridge_bias)</span>
<span id="cb20-879"><a href="#cb20-879" aria-hidden="true" tabindex="-1"></a>    ridge_variance_values[idx] <span class="op">=</span> np.mean(ridge_variance)</span>
<span id="cb20-880"><a href="#cb20-880" aria-hidden="true" tabindex="-1"></a>    ridge_out_sample_mse[idx] <span class="op">=</span> np.mean(out_sample_mse)</span>
<span id="cb20-881"><a href="#cb20-881" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-882"><a href="#cb20-882" aria-hidden="true" tabindex="-1"></a>The bias comparison plot reveals that OLS remains approximately unbiased across all sample sizes, as expected from its closed-form derivation that minimizes residuals without regularization. Ridge regression, however, introduces a small negative bias due to its penalty term that shrinks coefficients toward zero. Interestingly, this bias diminishes as the sample size increases, indicating that with more data, the regularization effect becomes less influential and the Ridge estimator begins to approach the OLS solution. This aligns with the theoretical understanding that regularization primarily helps in low-sample or high-collinearity scenarios.</span>
<span id="cb20-883"><a href="#cb20-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-884"><a href="#cb20-884" aria-hidden="true" tabindex="-1"></a>The variance comparison plot shows that Ridge regression consistently achieves slightly lower variance than OLS at every sample size. This is a direct consequence of the regularization term in Ridge, which stabilizes coefficient estimates by penalizing large weights, especially in scenarios with multicollinearity or limited data. Both OLS and Ridge exhibit a steep drop in variance as sample size increases, demonstrating that more data leads to more stable parameter estimation in both models.</span>
<span id="cb20-885"><a href="#cb20-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-886"><a href="#cb20-886" aria-hidden="true" tabindex="-1"></a>In the MSE comparison plot, we observe that Ridge regression has a higher in-sample MSE than OLS across all sample sizes, which is expected due to its bias penalty. However, Ridge achieves a consistently lower out-of-sample MSE than OLS for small to moderate sample sizes. This highlights the bias-variance tradeoff in action: while Ridge sacrifices some in-sample accuracy by introducing bias, it benefits from reduced variance, leading to better generalization on unseen data. As the sample size increases to 500, both models converge in their out-of-sample performance, with OLS catching up due to its decreasing variance and absence of bias.</span>
<span id="cb20-887"><a href="#cb20-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-888"><a href="#cb20-888" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb20-889"><a href="#cb20-889" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb20-890"><a href="#cb20-890" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Bias_Comparison.png)</span>{width=100%}</span>
<span id="cb20-891"><a href="#cb20-891" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Bias Comparison: OLS vs Ridge&lt;/center&gt;</span>
<span id="cb20-892"><a href="#cb20-892" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-893"><a href="#cb20-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-894"><a href="#cb20-894" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb20-895"><a href="#cb20-895" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Variance_Comparison.png)</span>{width=100%}</span>
<span id="cb20-896"><a href="#cb20-896" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Variance Comparison: OLS vs Ridge&lt;/center&gt;</span>
<span id="cb20-897"><a href="#cb20-897" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-898"><a href="#cb20-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-899"><a href="#cb20-899" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb20-900"><a href="#cb20-900" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/MSE_Comparison.png)</span>{width=100%}</span>
<span id="cb20-901"><a href="#cb20-901" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;MSE Comparison: OLS vs Ridge&lt;/center&gt;</span>
<span id="cb20-902"><a href="#cb20-902" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-903"><a href="#cb20-903" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-904"><a href="#cb20-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-905"><a href="#cb20-905" aria-hidden="true" tabindex="-1"></a>Ridge regression was evaluated across a grid of regularization parameters using k-fold cross-validation with repeated sampling. For each lambda value, the data was split into training and validation sets multiple times to compute stable estimates of out-of-sample mean squared error. The mean MSE across all folds and repetitions was recorded for each lambda, allowing for the identification of the value that yielded the lowest average validation error.</span>
<span id="cb20-906"><a href="#cb20-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-907"><a href="#cb20-907" aria-hidden="true" tabindex="-1"></a>This process resulted in an optimal lambda of approximately 0.1963, which minimized the out-of-sample MSE to 0.7268. In comparison, the corresponding out-of-sample MSE for ordinary least squares, which does not include any regularization, was slightly higher at 0.7505. This demonstrates that a modest amount of regularization can improve generalization performance by balancing bias and variance, especially in settings where noise or multicollinearity is present in the predictors.</span>
<span id="cb20-908"><a href="#cb20-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-909"><a href="#cb20-909" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-910"><a href="#cb20-910" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for Ridge Regression results</span></span>
<span id="cb20-911"><a href="#cb20-911" aria-hidden="true" tabindex="-1"></a>ridge_mse_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(lambda_grid))</span>
<span id="cb20-912"><a href="#cb20-912" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>k_folds, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-913"><a href="#cb20-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-914"><a href="#cb20-914" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation with Cross-Validation for Ridge</span></span>
<span id="cb20-915"><a href="#cb20-915" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, lmb <span class="kw">in</span> <span class="bu">enumerate</span>(lambda_grid):</span>
<span id="cb20-916"><a href="#cb20-916" aria-hidden="true" tabindex="-1"></a>    mse_sum <span class="op">=</span> np.zeros((k_folds, n_reps))</span>
<span id="cb20-917"><a href="#cb20-917" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> np.random.randn(n)</span>
<span id="cb20-918"><a href="#cb20-918" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold_idx, (train_index, val_index) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(np.arange(n))):</span>
<span id="cb20-919"><a href="#cb20-919" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> rep <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb20-920"><a href="#cb20-920" aria-hidden="true" tabindex="-1"></a>            X_train, X_val <span class="op">=</span> X[train_index], X[val_index]</span>
<span id="cb20-921"><a href="#cb20-921" aria-hidden="true" tabindex="-1"></a>            y_train, y_val <span class="op">=</span> y[train_index], y[val_index]</span>
<span id="cb20-922"><a href="#cb20-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-923"><a href="#cb20-923" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Ridge estimators manually</span></span>
<span id="cb20-924"><a href="#cb20-924" aria-hidden="true" tabindex="-1"></a>            beta_ridge <span class="op">=</span> compute_ridge_regression(X_train, y_train, lmb)</span>
<span id="cb20-925"><a href="#cb20-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-926"><a href="#cb20-926" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute Predictions &amp; MSE</span></span>
<span id="cb20-927"><a href="#cb20-927" aria-hidden="true" tabindex="-1"></a>            mse_sum[fold_idx, rep] <span class="op">=</span> np.mean((y_val <span class="op">-</span> X_val <span class="op">@</span> beta_ridge) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-928"><a href="#cb20-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-929"><a href="#cb20-929" aria-hidden="true" tabindex="-1"></a>    ridge_mse_values[i] <span class="op">=</span> mse_sum.mean()  <span class="co"># Average MSE across folds &amp; Monte Carlo repetitions</span></span>
<span id="cb20-930"><a href="#cb20-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-931"><a href="#cb20-931" aria-hidden="true" tabindex="-1"></a>optimal_lambda <span class="op">=</span> lambda_grid[np.argmin(ridge_mse_values)]</span>
<span id="cb20-932"><a href="#cb20-932" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-933"><a href="#cb20-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-934"><a href="#cb20-934" aria-hidden="true" tabindex="-1"></a><span class="in">                 Method  Optimal Lambda (Out-of-Sample)  Out-of-Sample MSE</span></span>
<span id="cb20-935"><a href="#cb20-935" aria-hidden="true" tabindex="-1"></a><span class="in">    0               OLS                             NaN           0.750544</span></span>
<span id="cb20-936"><a href="#cb20-936" aria-hidden="true" tabindex="-1"></a><span class="in">    1  Ridge Regression                        0.196304           0.726801</span></span>
<span id="cb20-937"><a href="#cb20-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-938"><a href="#cb20-938" aria-hidden="true" tabindex="-1"></a>The MSE existence theorem essentially states that for some optimal λ, Ridge Regression achieves a lower expected MSE than OLS. This graph shows how the out-of-sample mean squared error (MSE) for Ridge regression varies with different values of the regularization parameter lambda, plotted on a logarithmic scale. The blue curve represents Ridge’s MSE across a wide range of lambda values. As lambda increases from very small to very large values, the model transitions from low bias–high variance to high bias–low variance behavior. When lambda is too small, the model risks overfitting, while large lambdas overly shrink the coefficients, hurting predictive accuracy.</span>
<span id="cb20-939"><a href="#cb20-939" aria-hidden="true" tabindex="-1"></a>The curve dips at a lambda value around 0.1963, where the MSE is minimized. This value represents the optimal balance between bias and variance for this dataset. Beyond this point, the MSE rises rapidly as excessive regularization starts to dominate. The red dashed line indicates the out-of-sample MSE for OLS, which remains constant since OLS does not involve any regularization. The fact that the Ridge curve falls below this line around the optimal lambda shows that Ridge, when properly tuned, can outperform OLS in terms of generalization.</span>
<span id="cb20-940"><a href="#cb20-940" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Out_of_Sample_MSE.png)</span>{width=80%}</span>
<span id="cb20-941"><a href="#cb20-941" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Out-of-Sample MSE vs. Lambda (log scale) for Ridge Regression&lt;/center&gt;</span>
<span id="cb20-942"><a href="#cb20-942" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-943"><a href="#cb20-943" aria-hidden="true" tabindex="-1"></a><span class="fu"># 4. Bias and Variance under Non-Linear DGP</span></span>
<span id="cb20-944"><a href="#cb20-944" aria-hidden="true" tabindex="-1"></a>Non-Linear DGP:$$y = \frac{1}{1 + \exp(-X\beta)} + \varepsilon$$</span>
<span id="cb20-945"><a href="#cb20-945" aria-hidden="true" tabindex="-1"></a>This is a logistic transformation applied only to the first predictor(X0), meaning only X0 affects y. I’ve decided to only use the first predictor because if other predictors are included linearly, the function would become partially nonlinear and partially linear. This will make it harder to isolate the effects of nonlinearity when comparing OLS and Ridge Regression, essentially making it harder to understand when and why linear models fail. Additionally, the remaining predictors will act as noise, making it a good case for Ridge Regression to shrink irrelevant features.</span>
<span id="cb20-946"><a href="#cb20-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-947"><a href="#cb20-947" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb20-948"><a href="#cb20-948" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Non-Linear Data Generating Process (DGP)</span></span>
<span id="cb20-949"><a href="#cb20-949" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_nonlinear_data(n, p):</span>
<span id="cb20-950"><a href="#cb20-950" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.randn(n, p)</span>
<span id="cb20-951"><a href="#cb20-951" aria-hidden="true" tabindex="-1"></a>    y_raw <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="dv">1</span> <span class="op">+</span> X[:, <span class="dv">0</span>]))  <span class="co"># Compute logistic function using only the first predictor</span></span>
<span id="cb20-952"><a href="#cb20-952" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y_raw <span class="op">+</span> np.random.randn(n) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb20-953"><a href="#cb20-953" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb20-954"><a href="#cb20-954" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-955"><a href="#cb20-955" aria-hidden="true" tabindex="-1"></a>Aside from introducing the non-linear transformation through the sigmoid function in the data-generating process, the overall experimental setup remained consistent with the procedures described in Section 3. The same Monte Carlo simulation framework was applied to evaluate the performance of both OLS and Ridge regression under this non-linear setting. For each simulation run, synthetic datasets were generated, models were trained on a subset of the data, and evaluated on held-out test sets to compute key metrics such as bias, variance, and out-of-sample MSE. This allowed for a direct comparison between the linear and non-linear DGPs, highlighting how model assumptions interact with the underlying structure of the data.</span>
<span id="cb20-956"><a href="#cb20-956" aria-hidden="true" tabindex="-1"></a>The plot compares the mean squared error of OLS and Ridge regression across increasing sample sizes under the same non-linear setting. At all sample sizes, Ridge maintains a slight advantage over OLS, consistently producing lower MSE. This suggests that even in the presence of model misspecification due to non-linearity, regularization provides a stabilizing effect by reducing variance without dramatically increasing bias. Both models show increasing MSE with larger sample sizes, which is expected in this case, since the sigmoid-transformed targets are bounded and harder to approximate well with a linear model as more data exposes more of the non-linearity.</span>
<span id="cb20-957"><a href="#cb20-957" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Non_Linear_MSE_Comparison.png)</span>{width=80%}</span>
<span id="cb20-958"><a href="#cb20-958" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Comparison of OLS and Ridge MSE vs Sample Size (Non-Linear DGP)&lt;/center&gt;</span>
<span id="cb20-959"><a href="#cb20-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-960"><a href="#cb20-960" aria-hidden="true" tabindex="-1"></a>The plot shows how Ridge regression performs under a non-linear data-generating process where the response is a sigmoid transformation of a linear combination of predictors. The curve captures out-of-sample MSE across a wide range of lambda values on a logarithmic scale. A noticeable dip occurs around lambda = 0.0069, indicating the optimal regularization strength that minimizes prediction error. Outside this region, the MSE fluctuates more sharply, especially at higher lambda values, where the model likely overshrinks the coefficients. The overall noise in the curve reflects the added complexity of fitting a linear model to a non-linear signal, where small changes in lambda can lead to more volatile performance.</span>
<span id="cb20-961"><a href="#cb20-961" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Ridge_Out_of_Sample_MSE.png)</span>{width=80%}</span>
<span id="cb20-962"><a href="#cb20-962" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Out-of-Sample MSE vs Lambda (Log Scale) under Non-Linear DGP&lt;/center&gt;</span>
<span id="cb20-963"><a href="#cb20-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-964"><a href="#cb20-964" aria-hidden="true" tabindex="-1"></a>**B-spline Regression**: I used B-spline regression to get the “best approximate” linear regression coefficients. This cubic B-spline regression with 5 degrees of freedom allows the model to smoothly fit approximate the non-linear DGP, avoiding issues of high-degree polynomials. B-spline regression is considered an OLS model because it follows the same fundamental principles of linear regression, however with a transformed feature space.</span>
<span id="cb20-965"><a href="#cb20-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-968"><a href="#cb20-968" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-969"><a href="#cb20-969" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply splines to approximate the function and estimate coefficients</span></span>
<span id="cb20-970"><a href="#cb20-970" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_splines(X, y):</span>
<span id="cb20-971"><a href="#cb20-971" aria-hidden="true" tabindex="-1"></a>    X_spline <span class="op">=</span> dmatrix(<span class="ss">f"bs(X0, df=5, degree=3, include_intercept=True)"</span>,</span>
<span id="cb20-972"><a href="#cb20-972" aria-hidden="true" tabindex="-1"></a>                        {<span class="st">"X0"</span>: X[:, <span class="dv">0</span>]}, return_type<span class="op">=</span><span class="st">'matrix'</span>) <span class="co"># Only use the first predictor</span></span>
<span id="cb20-973"><a href="#cb20-973" aria-hidden="true" tabindex="-1"></a>    beta_spline <span class="op">=</span> np.linalg.pinv(X_spline.T <span class="op">@</span> X_spline) <span class="op">@</span> X_spline.T <span class="op">@</span> y</span>
<span id="cb20-974"><a href="#cb20-974" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta_spline</span>
<span id="cb20-975"><a href="#cb20-975" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-976"><a href="#cb20-976" aria-hidden="true" tabindex="-1"></a>The goal here is to test how well spline regression can model non-linear relationships in data. For each sample size, a synthetic dataset is generated where the relationship between the features and the response is curved or non-linear. Instead of fitting a model to all predictors, the simulation focuses on just the first predictor and tries to capture its non-linear effect using spline basis functions.</span>
<span id="cb20-977"><a href="#cb20-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-978"><a href="#cb20-978" aria-hidden="true" tabindex="-1"></a>In each repetition, the data is split into training and test sets. A spline regression model is fit on the training set by expressing the first predictor using a set of cubic spline basis functions. These basis functions break the input into smooth polynomial pieces connected at specific points called knots. Once the model is fit, it’s used to predict outcomes on the test set, and the accuracy of these predictions is measured using mean squared error (MSE).</span>
<span id="cb20-979"><a href="#cb20-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-980"><a href="#cb20-980" aria-hidden="true" tabindex="-1"></a>This process is repeated many times to reduce the effect of randomness from data splits. The final MSE for each sample size is the average over all repetitions. Additionally, the spline coefficients — which determine the shape of the fitted curve — are also averaged across repetitions. This gives a sense of the typical function shape learned by the spline model and helps assess how the fitted relationship evolves as more data becomes available.</span>
<span id="cb20-983"><a href="#cb20-983" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-984"><a href="#cb20-984" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo Simulation</span></span>
<span id="cb20-985"><a href="#cb20-985" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, size <span class="kw">in</span> <span class="bu">enumerate</span>(sample_sizes):</span>
<span id="cb20-986"><a href="#cb20-986" aria-hidden="true" tabindex="-1"></a>    splines_mse <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-987"><a href="#cb20-987" aria-hidden="true" tabindex="-1"></a>    spline_coef_sum <span class="op">=</span> np.zeros(<span class="dv">6</span>)</span>
<span id="cb20-988"><a href="#cb20-988" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> generate_nonlinear_data(size, p)</span>
<span id="cb20-989"><a href="#cb20-989" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_reps):</span>
<span id="cb20-990"><a href="#cb20-990" aria-hidden="true" tabindex="-1"></a>        X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb20-991"><a href="#cb20-991" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit splines using only the first predictor</span></span>
<span id="cb20-992"><a href="#cb20-992" aria-hidden="true" tabindex="-1"></a>        beta_spline <span class="op">=</span> fit_splines(X_train, y_train)</span>
<span id="cb20-993"><a href="#cb20-993" aria-hidden="true" tabindex="-1"></a>        X_test_spline <span class="op">=</span> dmatrix(<span class="ss">f"bs(X0, df=5, degree=3, include_intercept=True)"</span>, {<span class="st">"X0"</span>: X_test[:, <span class="dv">0</span>]}, return_type<span class="op">=</span><span class="st">'matrix'</span>)</span>
<span id="cb20-994"><a href="#cb20-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-995"><a href="#cb20-995" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict using splines</span></span>
<span id="cb20-996"><a href="#cb20-996" aria-hidden="true" tabindex="-1"></a>        y_pred_splines <span class="op">=</span> X_test_spline <span class="op">@</span> beta_spline</span>
<span id="cb20-997"><a href="#cb20-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-998"><a href="#cb20-998" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute MSE</span></span>
<span id="cb20-999"><a href="#cb20-999" aria-hidden="true" tabindex="-1"></a>        splines_mse <span class="op">+=</span> mean_squared_error(y_test, y_pred_splines)</span>
<span id="cb20-1000"><a href="#cb20-1000" aria-hidden="true" tabindex="-1"></a>        spline_coef_sum <span class="op">+=</span> beta_spline.ravel()</span>
<span id="cb20-1001"><a href="#cb20-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1002"><a href="#cb20-1002" aria-hidden="true" tabindex="-1"></a>    splines_mse_values[idx] <span class="op">=</span> splines_mse <span class="op">/</span> n_reps</span>
<span id="cb20-1003"><a href="#cb20-1003" aria-hidden="true" tabindex="-1"></a>    best_spline_coefficients[idx] <span class="op">=</span> spline_coef_sum <span class="op">/</span> n_reps</span>
<span id="cb20-1004"><a href="#cb20-1004" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1005"><a href="#cb20-1005" aria-hidden="true" tabindex="-1"></a>The first plot shows how spline regression is used to approximate a non-linear relationship between the response variable and the first predictor. The black dots represent the true data, while the red curve represents the fitted spline function. The shape of the red curve reflects the flexibility of spline basis functions, which are able to adapt to local changes in the curvature of the data. Compared to linear regression, this approach captures non-linear structure more accurately, especially in regions where the slope changes direction or flattens out.</span>
<span id="cb20-1006"><a href="#cb20-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1007"><a href="#cb20-1007" aria-hidden="true" tabindex="-1"></a>The second plot tracks the values of the spline basis coefficients as the sample size increases. Each colored line corresponds to one of the coefficients in the B-spline expansion. As the sample size grows, the coefficients begin to stabilize, indicating convergence toward a consistent underlying functional form. This pattern suggests that with more data, the estimated spline model becomes less sensitive to noise and better represents the true signal. The behavior also shows which parts of the spline basis are most influential in modeling the shape of the target curve.</span>
<span id="cb20-1008"><a href="#cb20-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1009"><a href="#cb20-1009" aria-hidden="true" tabindex="-1"></a>The third plot presents the mean squared error of the spline predictions across different sample sizes. There is a clear downward trend: as the sample size increases, the MSE decreases steadily. This confirms that the spline model generalizes better with more data and is able to capture the non-linear structure more accurately. Unlike linear models, which can suffer from persistent bias under non-linear DGPs, spline regression benefits directly from sample size by refining its fit to the curved underlying function.</span>
<span id="cb20-1010"><a href="#cb20-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1011"><a href="#cb20-1011" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb20-1012"><a href="#cb20-1012" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb20-1013"><a href="#cb20-1013" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Spline_approximation_to_Non_Linear_Function.png)</span>{width=100%}</span>
<span id="cb20-1014"><a href="#cb20-1014" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Spline Approximation to Non-Linear Function&lt;/center&gt;</span>
<span id="cb20-1015"><a href="#cb20-1015" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1016"><a href="#cb20-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1017"><a href="#cb20-1017" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb20-1018"><a href="#cb20-1018" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Convergence_of_spline_coeff.png)</span>{width=100%}</span>
<span id="cb20-1019"><a href="#cb20-1019" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;Convergence of Best Approximate Spline Coefficients&lt;/center&gt;</span>
<span id="cb20-1020"><a href="#cb20-1020" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1021"><a href="#cb20-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1022"><a href="#cb20-1022" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb20-1023"><a href="#cb20-1023" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/MSE_of_Splines_approx.png)</span>{width=100%}</span>
<span id="cb20-1024"><a href="#cb20-1024" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;MSE of Spline Approximation vs Sample Size&lt;/center&gt;</span>
<span id="cb20-1025"><a href="#cb20-1025" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1026"><a href="#cb20-1026" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1027"><a href="#cb20-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1028"><a href="#cb20-1028" aria-hidden="true" tabindex="-1"></a>References:</span>
<span id="cb20-1029"><a href="#cb20-1029" aria-hidden="true" tabindex="-1"></a>SNU AI. (n.d.). The bias-variance trade-off: A mathematical view. Medium. Retrieved from</span>
<span id="cb20-1030"><a href="#cb20-1030" aria-hidden="true" tabindex="-1"></a>https://medium.com/snu-ai/the-bias-variance-trade-off-a-mathematical-view-14ff9dfe5a3c</span>
<span id="cb20-1031"><a href="#cb20-1031" aria-hidden="true" tabindex="-1"></a>Shubham, S. (n.d.). All about Gauss-Markov theorem for ordinary least squares regression (OLS) &amp; BLUE properties of OLS estimators. Medium. Retrieved from https://medium.com/@shubhamsd100/all-about-gauss- markov-theorem-for-ordinary-least-squares-regression-ols-blue-properties-of-e1e1864fe087</span>
<span id="cb20-1032"><a href="#cb20-1032" aria-hidden="true" tabindex="-1"></a>Weylandt, M. (n.d.). STA9890 notes. Retrieved from https://michael-weylandt.com/STA9890/notes.html</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>