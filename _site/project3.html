<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STA 9890 Project: Property Valuation – Statisitical Learning for Data Mining Project Portfolio</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Statisitical Learning for Data Mining Project Portfolio</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./project1.qmd"> 
<span class="menu-text">Bias and Variance in Linear Regression</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./project2.qmd"> 
<span class="menu-text">Ensemble Learning Techniques for Fair Classification</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./project3.html" aria-current="page"> 
<span class="menu-text">Forecasting Property Valuations in a Mid-Sized U.S. City:A SHAP-Gain Feature Selection and ElasticNet-Ensembled Approach with Optuna-Tuned XGBoost</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background-on-property-valuation-and-machine-learning" id="toc-background-on-property-valuation-and-machine-learning" class="nav-link active" data-scroll-target="#background-on-property-valuation-and-machine-learning">1. Background on Property Valuation and Machine Learning</a>
  <ul class="collapse">
  <li><a href="#why-prediction-accuracy-and-interpretability-matter" id="toc-why-prediction-accuracy-and-interpretability-matter" class="nav-link" data-scroll-target="#why-prediction-accuracy-and-interpretability-matter">1.1 Why Prediction Accuracy and Interpretability Matter</a></li>
  </ul></li>
  <li><a href="#data-description-and-objective" id="toc-data-description-and-objective" class="nav-link" data-scroll-target="#data-description-and-objective">2. Data Description and Objective</a></li>
  <li><a href="#feature-engineering" id="toc-feature-engineering" class="nav-link" data-scroll-target="#feature-engineering">3. Feature Engineering</a>
  <ul class="collapse">
  <li><a href="#logic-driven-missing-value-handling-and-imputation" id="toc-logic-driven-missing-value-handling-and-imputation" class="nav-link" data-scroll-target="#logic-driven-missing-value-handling-and-imputation">3.1 Logic-Driven Missing Value Handling and Imputation</a></li>
  <li><a href="#frequency-encoding-of-high-cardinality-geographic-variables" id="toc-frequency-encoding-of-high-cardinality-geographic-variables" class="nav-link" data-scroll-target="#frequency-encoding-of-high-cardinality-geographic-variables">3.3 Frequency Encoding of High-Cardinality Geographic Variables</a></li>
  <li><a href="#boolean-and-ordinal-encoding" id="toc-boolean-and-ordinal-encoding" class="nav-link" data-scroll-target="#boolean-and-ordinal-encoding">3.4 Boolean and Ordinal Encoding</a></li>
  <li><a href="#target-encoding-of-nominal-categorical-variables-20152019" id="toc-target-encoding-of-nominal-categorical-variables-20152019" class="nav-link" data-scroll-target="#target-encoding-of-nominal-categorical-variables-20152019">3.5 Target Encoding of Nominal Categorical Variables (2015–2019)</a></li>
  <li><a href="#quantile-binning-of-features" id="toc-quantile-binning-of-features" class="nav-link" data-scroll-target="#quantile-binning-of-features">3.6 Quantile Binning of Features</a></li>
  <li><a href="#rare-frequency-suppression-in-spatial-encodings" id="toc-rare-frequency-suppression-in-spatial-encodings" class="nav-link" data-scroll-target="#rare-frequency-suppression-in-spatial-encodings">3.7 Rare Frequency Suppression in Spatial Encodings</a></li>
  <li><a href="#log-transformation-and-distribution-smoothing-for-ridge-regression" id="toc-log-transformation-and-distribution-smoothing-for-ridge-regression" class="nav-link" data-scroll-target="#log-transformation-and-distribution-smoothing-for-ridge-regression">3.8 Log Transformation and Distribution Smoothing for Ridge Regression</a></li>
  <li><a href="#adaptive-quantile-clipping-for-tree-based-models" id="toc-adaptive-quantile-clipping-for-tree-based-models" class="nav-link" data-scroll-target="#adaptive-quantile-clipping-for-tree-based-models">3.9 Adaptive Quantile Clipping for Tree-Based Models</a></li>
  <li><a href="#interaction-features-for-linear-and-nonlinear-models" id="toc-interaction-features-for-linear-and-nonlinear-models" class="nav-link" data-scroll-target="#interaction-features-for-linear-and-nonlinear-models">3.10 Interaction Features for Linear and Nonlinear Models</a></li>
  </ul></li>
  <li><a href="#model-development-and-tuning" id="toc-model-development-and-tuning" class="nav-link" data-scroll-target="#model-development-and-tuning">4. Model Development and Tuning</a>
  <ul class="collapse">
  <li><a href="#ridge-regression-with-cross-validation" id="toc-ridge-regression-with-cross-validation" class="nav-link" data-scroll-target="#ridge-regression-with-cross-validation">4.1 Ridge Regression with Cross-Validation</a></li>
  <li><a href="#tree-based-models-with-optuna-and-shap-gain-feature-selection" id="toc-tree-based-models-with-optuna-and-shap-gain-feature-selection" class="nav-link" data-scroll-target="#tree-based-models-with-optuna-and-shap-gain-feature-selection">4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection</a></li>
  </ul></li>
  <li><a href="#ensembling-strategy" id="toc-ensembling-strategy" class="nav-link" data-scroll-target="#ensembling-strategy">5. Ensembling Strategy</a>
  <ul class="collapse">
  <li><a href="#weighted-model-blending-with-optuna" id="toc-weighted-model-blending-with-optuna" class="nav-link" data-scroll-target="#weighted-model-blending-with-optuna">5.1 Weighted Model Blending with Optuna</a></li>
  <li><a href="#stacked-ensembling-with-elasticnetcv" id="toc-stacked-ensembling-with-elasticnetcv" class="nav-link" data-scroll-target="#stacked-ensembling-with-elasticnetcv">5.2 Stacked Ensembling with ElasticNetCV</a></li>
  </ul></li>
  <li><a href="#residual-error-analysis" id="toc-residual-error-analysis" class="nav-link" data-scroll-target="#residual-error-analysis">6. Residual Error Analysis</a></li>
  <li><a href="#shap-based-interpretability-and-insights" id="toc-shap-based-interpretability-and-insights" class="nav-link" data-scroll-target="#shap-based-interpretability-and-insights">7. SHAP-Based Interpretability and Insights</a>
  <ul class="collapse">
  <li><a href="#top-predictive-features" id="toc-top-predictive-features" class="nav-link" data-scroll-target="#top-predictive-features">Top Predictive Features</a></li>
  <li><a href="#low-impact-features" id="toc-low-impact-features" class="nav-link" data-scroll-target="#low-impact-features">Low-Impact Features</a></li>
  </ul></li>
  <li><a href="#reflections-and-lessons-learned" id="toc-reflections-and-lessons-learned" class="nav-link" data-scroll-target="#reflections-and-lessons-learned">8. Reflections and Lessons Learned</a>
  <ul class="collapse">
  <li><a href="#what-worked-well" id="toc-what-worked-well" class="nav-link" data-scroll-target="#what-worked-well">What Worked Well</a></li>
  <li><a href="#what-could-improve" id="toc-what-could-improve" class="nav-link" data-scroll-target="#what-could-improve">What Could Improve</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">STA 9890 Project: Property Valuation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="background-on-property-valuation-and-machine-learning" class="level1">
<h1>1. Background on Property Valuation and Machine Learning</h1>
<section id="why-prediction-accuracy-and-interpretability-matter" class="level2">
<h2 class="anchored" data-anchor-id="why-prediction-accuracy-and-interpretability-matter">1.1 Why Prediction Accuracy and Interpretability Matter</h2>
<p>Property assessment values directly influence individual tax obligations, urban development decisions, and housing affordability analyses. However, public datasets are often noisy and incomplete—featuring missing renovation records, outdated area measurements, or abrupt shifts in land valuation. Therefore, a high-performing ML model must not only minimize prediction error but also provide interpretable insights that help stakeholders understand, trust, and audit the predictions—especially when such predictions may inform public policy or fiscal planning.</p>
</section>
</section>
<section id="data-description-and-objective" class="level1">
<h1>2. Data Description and Objective</h1>
<p>The task is to predict 2019 assessed property values using historical records from 2015 to 2019. Each record includes detailed building features, land area and value, protested assessments, and neighborhood-level identifiers.</p>
<p>The final 2019 target is defined as:</p>
<p><strong>TARGET = building value 2019 + land value 2019</strong></p>
<p>This is a regression problem with approximately 600,000 training records and 400,000 test records. No external data was permitted.</p>
</section>
<section id="feature-engineering" class="level1">
<h1>3. Feature Engineering</h1>
<p>Feature engineering plays a critical role in unlocking predictive signals from raw data, particularly in structured datasets involving temporal and categorical variables. Our goal was to construct meaningful, leakage-free features that improve model performance while maintaining generalization to unseen data. Before initiating any feature transformations, we performed a strict comparison of columns between the training and test datasets to ensure no information advantage was present. This safeguards against data leakage and ensures that engineered features reflect patterns learnable at inference time.</p>
<section id="logic-driven-missing-value-handling-and-imputation" class="level2">
<h2 class="anchored" data-anchor-id="logic-driven-missing-value-handling-and-imputation">3.1 Logic-Driven Missing Value Handling and Imputation</h2>
<p><strong>Backfilling Year-Based Columns Across Feature Groups:</strong> To handle missing values in temporally structured features (e.g., building area, quality, full bath), we designed a consistent backfilling approach that uses older data to impute more recent years. Specifically, columns were ordered from newest to oldest (i.e., 2019, 2018, …, 2015), and we applied <code>bfill(axis=1)</code> across these columns. This setup causes older values (e.g., from 2015 or 2016) to be used to fill in newer year columns (e.g., 2018 or 2019), effectively implementing a forward fill in temporal logic. This approach assumes that older data reflects the property’s original state more accurately and helps prevent later-year anomalies or missing values from distorting long-term trends.</p>
<p>This logic was applied across multiple feature groups: - Residential count features: floors, full bath, half bath, bedrooms, total rooms - Area-based features: building area, land area - Valuation features: building value, land value, assessed - Categorical building attributes: foundation type, grade, building condition, quality, quality description, physical condition, exterior walls, has cooling, has heat</p>
<div id="cffb6de0" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>  echo: true</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  output: false</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  collapse: true</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backfill_categorical_year_features(df, features, years):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feature <span class="kw">in</span> features:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        year_cols <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> y <span class="kw">in</span> years <span class="cf">if</span> <span class="ss">f"</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span> <span class="kw">in</span> df.columns]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(year_cols) <span class="op">&gt;=</span> <span class="dv">2</span>:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>            df[year_cols] <span class="op">=</span> df[year_cols].bfill(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Backfilled: </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> across </span><span class="sc">{</span>year_cols<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Skipped: </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> — not enough year-based columns."</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Backfill from most recent year to oldest</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>years <span class="op">=</span> [<span class="st">'2019'</span>, <span class="st">'2018'</span>, <span class="st">'2017'</span>, <span class="st">'2016'</span>, <span class="st">'2015'</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'building_condition'</span>, <span class="st">'foundation_type'</span>, <span class="st">'grade'</span>, <span class="st">'has_cooling'</span>, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">'has_heat'</span>, <span class="st">'physical_condition'</span>, <span class="st">'exterior_walls'</span>]</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to train and test</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>train_merged <span class="op">=</span> backfill_categorical_year_features(train_merged, features, years)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>test_merged <span class="op">=</span> backfill_categorical_year_features(test_merged, features, years)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Zero-Aware Property Filtering:</strong> In cases where <code>floor_area_total_2019 = 0</code>, we treated the property as non-residential or commercial and applied domain-specific logic to avoid inappropriate imputations or distortions in the modeling process: - All related residential building features—such as full bath, total rooms, garage area, and porch area—were set to zero. - We also zeroed out all building area variables and the corresponding building value variables to reflect the absence of a residential structure.</p>
<p>These records were retained in the dataset rather than dropped, as they likely represent a distinct class of properties where valuation is driven primarily by land characteristics. Explicitly identifying and treating these properties allowed the model to better separate residential and non-residential valuation patterns.</p>
<div id="c7fc6bcb" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 0: Define base feature names ===</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>numeric_bases <span class="op">=</span> [</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'garage_area'</span>, <span class="st">'porch_area'</span>, <span class="st">'floors'</span>, <span class="st">'half_bath'</span>, <span class="st">'full_bath'</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'total_rooms'</span>, <span class="st">'bedrooms'</span>, <span class="st">'fireplaces'</span>, <span class="st">'building_area'</span>, <span class="st">'building_value'</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>categorical_fill_map <span class="op">=</span> {</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality_description'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'foundation_type'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grade'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'has_cooling'</span>: <span class="va">False</span>,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'has_heat'</span>: <span class="va">False</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'physical_condition'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'exterior_walls'</span>: <span class="st">'None'</span>,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'protested'</span>: <span class="va">False</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate full list of columns (2015–2019 only, no final columns)</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>numeric_cols_to_zero <span class="op">=</span> [</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> base <span class="kw">in</span> numeric_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>] <span class="op">+</span> [<span class="st">'building_value_growth'</span>]</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>categorical_cols_to_fill <span class="op">=</span> {</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">'</span>: val</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> base, val <span class="kw">in</span> categorical_fill_map.items()</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Apply imputation if floor_area_total_2019 == 0 ===</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df_name, df <span class="kw">in</span> [(<span class="st">'train_merged'</span>, train_merged), (<span class="st">'test_merged'</span>, test_merged)]:</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'floor_area_total_2019'</span> <span class="kw">in</span> df.columns:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        zero_floor_mask <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill numeric columns with 0</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> col <span class="kw">in</span> numeric_cols_to_zero:</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>                df.loc[zero_floor_mask, col] <span class="op">=</span> df.loc[zero_floor_mask, col].fillna(<span class="dv">0</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill categorical/boolean columns</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> col, fill_val <span class="kw">in</span> categorical_cols_to_fill.items():</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>                df.loc[zero_floor_mask, col] <span class="op">=</span> df.loc[zero_floor_mask, col].fillna(fill_val)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Filled structure-dependent missing values in </span><span class="sc">{</span>df_name<span class="sc">}</span><span class="ss"> for </span><span class="sc">{</span>zero_floor_mask<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> rows"</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" 'floor_area_total_2019' not found in </span><span class="sc">{</span>df_name<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Feature Drop Based on Sparse Signals:</strong> For features that appeared largely irrelevant or unused across the dataset, we calculated the percentage of zero values in columns such as mobile home area, deck area, and porch area. If a column contained over 90% zeros, it was considered non-informative and dropped from the modeling pipeline to reduce dimensionality and noise.</p>
<div id="ef84864a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>cols_to_drop <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> train_merged.columns <span class="cf">if</span> col.startswith(<span class="st">"mobile_home_area"</span>)]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop from both sets</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>train_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>test_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Dropped columns from train/test: </span><span class="sc">{</span>cols_to_drop<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Multi-Level Median and Mode Imputation:</strong> After applying logic-based pruning, we used a three-level median imputation strategy for continuous features (e.g., assessed value 2017, building area 2017) based on the following hierarchy: - Level 1: neighborhood-level median - Level 2: region-level median - Level 3: global median (fallback)</p>
<div id="52bab8c2" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># List of all assessed columns to impute</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>assessed_cols <span class="op">=</span> [<span class="st">'assessed_2015'</span>, <span class="st">'assessed_2016'</span>, <span class="st">'assessed_2017'</span>, <span class="st">'assessed_2018'</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> assessed_cols:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">not</span> <span class="kw">in</span> train_merged.columns:</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Compute medians from training data only</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    neigh_medians <span class="op">=</span> train_merged.groupby(<span class="st">'neighborhood'</span>)[col].median()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    region_medians <span class="op">=</span> train_merged.groupby(<span class="st">'region'</span>)[col].median()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    global_median <span class="op">=</span> train_merged[col].median()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Train set imputation</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    train_merged[col] <span class="op">=</span> train_merged.<span class="bu">apply</span>(</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> row: neigh_medians[row[<span class="st">'neighborhood'</span>]]</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="kw">and</span> row[<span class="st">'neighborhood'</span>] <span class="kw">in</span> neigh_medians <span class="cf">else</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        region_medians[row[<span class="st">'region'</span>]]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="kw">and</span> row[<span class="st">'region'</span>] <span class="kw">in</span> region_medians <span class="cf">else</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        global_median</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="cf">else</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        row[col],</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Test set imputation (using train medians only)</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    test_merged[col] <span class="op">=</span> test_merged.<span class="bu">apply</span>(</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> row: neigh_medians.get(row[<span class="st">'neighborhood'</span>], np.nan)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="cf">else</span> row[col],</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    test_merged[col] <span class="op">=</span> test_merged.<span class="bu">apply</span>(</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> row: region_medians.get(row[<span class="st">'region'</span>], np.nan)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="cf">else</span> row[col],</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    test_merged[col].fillna(global_median, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Imputed '</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">' using neighborhood → region → global medians (from training data)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For categorical variables such as foundation type or building condition, we applied single-level mode imputation using the most frequent category within the training data. While this approach is less localized, it provided a simple and stable method for handling missing values in features with low cardinality. ## 3.2 Neighborhood and Region-Level Statistical Features</p>
<p>To capture localized pricing dynamics and identify anomalies in property assessments, we engineered a suite of statistical features using the 2018 assessed values as a proxy for prior valuation context. We first computed neighborhood-level metrics including the mean, median, standard deviation, and interquartile range (IQR) of assessed 2018, grouped by neighborhood. Similarly, region-level statistics were computed using the region variable.</p>
<p>Using the merged and imputed stats, we computed derived features such as: - <strong>assess minus neigh mean</strong>: the raw deviation of a property’s 2018 assessed value from its neighborhood mean - <strong>assess ratio neigh mean</strong>: a normalized ratio of a property’s value to its local average - <strong>z score assess neigh</strong>: a z-score based on neighborhood-level variation - <strong>Corresponding region-level counterparts</strong>: assess minus region mean, assess ratio region mean, and z score assess region</p>
<p>These features helped contextualize each property’s assessed value relative to other properties within the same neighborhood or region. They proved useful in capturing outliers and potentially undervalued homes that deviated from local valuation patterns.</p>
<div id="a2cb81de" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Compute neighborhood-level stats ===</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>neigh_stats <span class="op">=</span> train_merged.groupby(<span class="st">'neighborhood'</span>)[<span class="st">'assessed_2018'</span>].agg([</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_mean'</span>, <span class="st">'mean'</span>),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_median'</span>, <span class="st">'median'</span>),</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_std'</span>, <span class="st">'std'</span>),</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_q1'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.25</span>)),</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_q3'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.75</span>)),</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>]).reset_index()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>neigh_stats[<span class="st">'neigh_assess_iqr'</span>] <span class="op">=</span> neigh_stats[<span class="st">'neigh_assess_q3'</span>] <span class="op">-</span> neigh_stats[<span class="st">'neigh_assess_q1'</span>]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Compute region-level stats ===</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>region_stats <span class="op">=</span> train_merged.groupby(<span class="st">'region'</span>)[<span class="st">'assessed_2018'</span>].agg([</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_mean'</span>, <span class="st">'mean'</span>),</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_median'</span>, <span class="st">'median'</span>),</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_std'</span>, <span class="st">'std'</span>),</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_q1'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.25</span>)),</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_q3'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.75</span>)),</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>]).reset_index()</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>region_stats[<span class="st">'region_assess_iqr'</span>] <span class="op">=</span> region_stats[<span class="st">'region_assess_q3'</span>] <span class="op">-</span> region_stats[<span class="st">'region_assess_q1'</span>]</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Fallback std maps from training data ===</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co"># For neighborhood fallback, group region medians of neighborhood std</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>neigh_std_by_region <span class="op">=</span> neigh_stats.merge(train_merged[[<span class="st">'neighborhood'</span>, <span class="st">'region'</span>]], on<span class="op">=</span><span class="st">'neighborhood'</span>, how<span class="op">=</span><span class="st">'left'</span>) <span class="op">\</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>                                  .groupby(<span class="st">'region'</span>)[<span class="st">'neigh_assess_std'</span>].median()</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>global_neigh_std <span class="op">=</span> neigh_stats[<span class="st">'neigh_assess_std'</span>].median()</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>region_std_by_neigh <span class="op">=</span> region_stats.merge(train_merged[[<span class="st">'neighborhood'</span>, <span class="st">'region'</span>]], on<span class="op">=</span><span class="st">'region'</span>, how<span class="op">=</span><span class="st">'left'</span>) <span class="op">\</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>                                   .groupby(<span class="st">'neighborhood'</span>)[<span class="st">'region_assess_std'</span>].median()</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>global_region_std <span class="op">=</span> region_stats[<span class="st">'region_assess_std'</span>].median()</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 4: Merge into train/test and compute features ===</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df_name, df <span class="kw">in</span> [(<span class="st">'train_merged'</span>, train_merged), (<span class="st">'test_merged'</span>, test_merged)]:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.merge(neigh_stats, on<span class="op">=</span><span class="st">'neighborhood'</span>, how<span class="op">=</span><span class="st">'left'</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.merge(region_stats, on<span class="op">=</span><span class="st">'region'</span>, how<span class="op">=</span><span class="st">'left'</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fill missing std values via fallback</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'neigh_assess_std'</span>] <span class="op">=</span> df[<span class="st">'neigh_assess_std'</span>].fillna(</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">'region'</span>].<span class="bu">map</span>(neigh_std_by_region)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    ).fillna(global_neigh_std)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'region_assess_std'</span>] <span class="op">=</span> df[<span class="st">'region_assess_std'</span>].fillna(</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">'neighborhood'</span>].<span class="bu">map</span>(region_std_by_neigh)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    ).fillna(global_region_std)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute derived features</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_minus_neigh_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">-</span> df[<span class="st">'neigh_assess_mean'</span>]</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_ratio_neigh_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_mean'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'z_score_assess_neigh'</span>] <span class="op">=</span> df[<span class="st">'assess_minus_neigh_mean'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_std'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_minus_region_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">-</span> df[<span class="st">'region_assess_mean'</span>]</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_ratio_region_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'region_assess_mean'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'z_score_assess_region'</span>] <span class="op">=</span> df[<span class="st">'assess_minus_region_mean'</span>] <span class="op">/</span> (df[<span class="st">'region_assess_std'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save back</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> df_name <span class="op">==</span> <span class="st">'train_merged'</span>:</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        train_merged <span class="op">=</span> df</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        test_merged <span class="op">=</span> df</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Completed: Stats merge + std fallback + z-score computation."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="frequency-encoding-of-high-cardinality-geographic-variables" class="level2">
<h2 class="anchored" data-anchor-id="frequency-encoding-of-high-cardinality-geographic-variables">3.3 Frequency Encoding of High-Cardinality Geographic Variables</h2>
<p>To convert high-cardinality categorical variables into numerical features while preserving signal strength, we applied frequency encoding to four key geographic identifiers: neighborhood, region, zone, and subneighborhood.</p>
<p>This encoding strategy served two purposes: 1. It allowed the model to retain information about how common or rare a spatial unit was. A frequently occurring neighborhood (i.e., one with high frequency) likely has more properties, which implies greater residential or commercial development in that area. 2. Areas with more properties are also likely to have more consistent and well-understood assessment patterns—the government has “seen” more properties there, which may reduce valuation volatility. These areas are more visible or prioritized in municipal processes.</p>
<p>Finally, frequency encoding avoids the dimensional explosion caused by one-hot encoding, which is especially problematic for variables with high cardinality like neighborhood or subneighborhood.</p>
<div id="408b9bab" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> [<span class="st">'neighborhood'</span>, <span class="st">'region'</span>,<span class="st">'zone'</span>,<span class="st">'subneighborhood'</span>]:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Compute frequency from training data</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        freq_map <span class="op">=</span> train_merged[col].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Apply to both datasets</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        train_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_freq'</span>] <span class="op">=</span> train_merged[col].<span class="bu">map</span>(freq_map)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        test_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_freq'</span>] <span class="op">=</span> test_merged[col].<span class="bu">map</span>(freq_map)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Frequency encoded: </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> → </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_freq (based on training set)"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Column '</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">' not found in training set"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="boolean-and-ordinal-encoding" class="level2">
<h2 class="anchored" data-anchor-id="boolean-and-ordinal-encoding">3.4 Boolean and Ordinal Encoding</h2>
<p><strong>Boolean Encoding:</strong> We focused on three boolean variables: has cooling, has heat, and protested. These features were encoded across all five years using binary values (0/1).</p>
<p><strong>Ordinal Encoding:</strong> For ordinal features such as quality, quality description, grade, building condition, and physical condition, we performed domain-informed cleaning and then applied ordinal encoding based on defined category hierarchies. Prior to encoding, raw values were standardized through column-specific replacements. For instance, extreme or ambiguous values like X, None, or overly granular subgrades (e.g., X-, E+) were either mapped to more interpretable categories or treated as missing. Some detailed conditions like Unsound and Very Poor were collapsed into broader categories such as Poor. Unknown values were handled gracefully by assigning an encoded fallback of -1. This process ensured that ordinal information was preserved in a numerically meaningful way, allowing models to leverage the ordered nature of these features without exploding dimensionality as one-hot encoding would.</p>
<div id="081f6ba1" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OrdinalEncoder</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Clear specific variables</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> [<span class="st">'ordinal_cols_all'</span>, <span class="st">'bool_cols_all'</span>]:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> var <span class="kw">in</span> <span class="bu">locals</span>():</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> <span class="bu">globals</span>()[var]</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Boolean Encoding (2015–2019 only) ===</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>bool_bases <span class="op">=</span> [<span class="st">'has_cooling'</span>, <span class="st">'has_heat'</span>, <span class="st">'protested'</span>]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>bool_cols_all <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> bool_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> bool_cols_all:</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        mode_val <span class="op">=</span> train_merged[col].mode(dropna<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        train_merged[col] <span class="op">=</span> train_merged[col].fillna(mode_val).astype(<span class="bu">int</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        test_merged[col] <span class="op">=</span> test_merged[col].fillna(mode_val).astype(<span class="bu">int</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Ordinal Cleaning and Encoding (2015–2019 only) ===</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>ordinal_bases <span class="op">=</span> [</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>, <span class="st">'quality_description'</span>, <span class="st">'grade'</span>,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>, <span class="st">'physical_condition'</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>ordinal_cols_all <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> ordinal_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Column-specific replacements</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>replacement_maps <span class="op">=</span> {</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>: {<span class="st">'E'</span>: <span class="st">'D'</span>, <span class="st">'F'</span>: <span class="st">'D'</span>, <span class="st">'X'</span>: np.nan, <span class="st">'None'</span>: np.nan},</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality_description'</span>: {<span class="st">'Poor'</span>: <span class="st">'Very Low'</span>, <span class="st">'None'</span>: np.nan},</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grade'</span>: {<span class="st">'X'</span>: <span class="st">'F'</span>, <span class="st">'X-'</span>: <span class="st">'F'</span>, <span class="st">'X+'</span>: <span class="st">'F'</span>, <span class="st">'E'</span>: <span class="st">'D'</span>, <span class="st">'E-'</span>: <span class="st">'D-'</span>, <span class="st">'E+'</span>: <span class="st">'D+'</span>, <span class="st">'None'</span>: np.nan},</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>: {<span class="st">'Very Poor'</span>: <span class="st">'Poor'</span>, <span class="st">'Unsound'</span>: <span class="st">'Poor'</span>, <span class="st">'None'</span>: np.nan},</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'physical_condition'</span>: {<span class="st">'Very Poor'</span>: <span class="st">'Poor'</span>, <span class="st">'Unsound'</span>: <span class="st">'Poor'</span>, <span class="st">'None'</span>: np.nan}</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinal category order</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>ord_categories <span class="op">=</span> {</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>: [<span class="st">'D'</span>, <span class="st">'C'</span>, <span class="st">'B'</span>, <span class="st">'A'</span>],</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality_description'</span>: [<span class="st">'Very Low'</span>, <span class="st">'Low'</span>, <span class="st">'Average'</span>, <span class="st">'Good'</span>, <span class="st">'Excellent'</span>, <span class="st">'Superior'</span>],</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grade'</span>: [<span class="st">'F'</span>, <span class="st">'D-'</span>, <span class="st">'D'</span>, <span class="st">'D+'</span>, <span class="st">'C-'</span>, <span class="st">'C'</span>, <span class="st">'C+'</span>, <span class="st">'B-'</span>, <span class="st">'B'</span>, <span class="st">'B+'</span>, <span class="st">'A-'</span>, <span class="st">'A'</span>, <span class="st">'A+'</span>],</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>: [<span class="st">'Poor'</span>, <span class="st">'Fair'</span>, <span class="st">'Average'</span>, <span class="st">'Good'</span>, <span class="st">'Very Good'</span>, <span class="st">'Excellent'</span>],</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">'physical_condition'</span>: [<span class="st">'Poor'</span>, <span class="st">'Fair'</span>, <span class="st">'Average'</span>, <span class="st">'Good'</span>, <span class="st">'Very Good'</span>, <span class="st">'Excellent'</span>]</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean and encode</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> base <span class="kw">in</span> ordinal_bases:</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>):</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        col <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>            replacements <span class="op">=</span> replacement_maps.get(base, {})</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>            train_merged[col] <span class="op">=</span> train_merged[col].replace(replacements)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>            test_merged[col] <span class="op">=</span> test_merged[col].replace(replacements)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>            mode_val <span class="op">=</span> train_merged[col].mode(dropna<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>            train_merged[col] <span class="op">=</span> train_merged[col].fillna(mode_val)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>            test_merged[col] <span class="op">=</span> test_merged[col].fillna(mode_val)</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>            encoder <span class="op">=</span> OrdinalEncoder(categories<span class="op">=</span>[ord_categories[base]], handle_unknown<span class="op">=</span><span class="st">'use_encoded_value'</span>, unknown_value<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>            train_merged[[col]] <span class="op">=</span> encoder.fit_transform(train_merged[[col]])</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>            test_merged[[col]] <span class="op">=</span> encoder.transform(test_merged[[col]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="target-encoding-of-nominal-categorical-variables-20152019" class="level2">
<h2 class="anchored" data-anchor-id="target-encoding-of-nominal-categorical-variables-20152019">3.5 Target Encoding of Nominal Categorical Variables (2015–2019)</h2>
<p>For certain nominal features that lacked ordinal structure but exhibited high cardinality, such as foundation type and exterior walls, we applied target encoding across all years from 2015 to 2019. This encoding replaces each category with a smoothed version of the mean target value (assessed 2018) observed for that category.</p>
<p>To avoid overfitting and data leakage, we implemented a 5-fold cross-validated target encoding procedure. For each fold, the mean target value was computed from the training portion and mapped to the validation fold. We used a smoothing parameter of 10 to balance the influence of the global mean versus the category-specific mean, especially for infrequent categories.</p>
<p>This method enabled us to capture predictive signal from nominal features without creating high-dimensional one-hot encodings or imposing artificial ordinal structure.</p>
<div id="75b39f9e" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: Target Encoding (2015–2019 only) ===</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> group_and_target_encode_cv(train_df, test_df, target_name, column, rare_threshold<span class="op">=</span><span class="fl">0.001</span>, smoothing<span class="op">=</span><span class="dv">10</span>, n_splits<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    freq <span class="op">=</span> train_df[column].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    rare_cats <span class="op">=</span> freq[freq <span class="op">&lt;</span> rare_threshold].index</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    train_df[column] <span class="op">=</span> train_df[column].replace(rare_cats, <span class="st">'Other'</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    test_df[column] <span class="op">=</span> test_df[column].replace(rare_cats, <span class="st">'Other'</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    global_mean <span class="op">=</span> train_df[target_name].mean()</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    oof_encoded <span class="op">=</span> pd.Series(index<span class="op">=</span>train_df.index, dtype<span class="op">=</span><span class="st">'float64'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>n_splits, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> train_idx, val_idx <span class="kw">in</span> kf.split(train_df):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        X_tr, X_val <span class="op">=</span> train_df.iloc[train_idx], train_df.iloc[val_idx]</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        stats <span class="op">=</span> X_tr.groupby(column)[target_name].agg([<span class="st">'mean'</span>, <span class="st">'count'</span>])</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        smooth <span class="op">=</span> (stats[<span class="st">'mean'</span>] <span class="op">*</span> stats[<span class="st">'count'</span>] <span class="op">+</span> global_mean <span class="op">*</span> smoothing) <span class="op">/</span> (stats[<span class="st">'count'</span>] <span class="op">+</span> smoothing)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        oof_encoded.iloc[val_idx] <span class="op">=</span> X_val[column].<span class="bu">map</span>(smooth).fillna(global_mean)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    final_stats <span class="op">=</span> train_df.groupby(column)[target_name].agg([<span class="st">'mean'</span>, <span class="st">'count'</span>])</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    final_smooth <span class="op">=</span> (final_stats[<span class="st">'mean'</span>] <span class="op">*</span> final_stats[<span class="st">'count'</span>] <span class="op">+</span> global_mean <span class="op">*</span> smoothing) <span class="op">/</span> (final_stats[<span class="st">'count'</span>] <span class="op">+</span> smoothing)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    test_encoded <span class="op">=</span> test_df[column].<span class="bu">map</span>(final_smooth).fillna(global_mean)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> oof_encoded, test_encoded</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Target-encodable nominal columns</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>target_encodable_bases <span class="op">=</span> [<span class="st">'foundation_type'</span>, <span class="st">'exterior_walls'</span>]</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>target_encodable_cols_all <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> target_encodable_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply target encoding</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> target_encodable_cols_all:</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        mode_val <span class="op">=</span> train_merged[col].mode(dropna<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        train_merged[col] <span class="op">=</span> train_merged[col].fillna(mode_val)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        test_merged[col] <span class="op">=</span> test_merged[col].fillna(mode_val)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        train_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_te'</span>], test_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_te'</span>] <span class="op">=</span> group_and_target_encode_cv(</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>            train_merged, test_merged, target_name<span class="op">=</span><span class="st">'assessed_2018'</span>, column<span class="op">=</span>col,</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>            rare_threshold<span class="op">=</span><span class="fl">0.001</span>, smoothing<span class="op">=</span><span class="dv">10</span>, n_splits<span class="op">=</span><span class="dv">5</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        train_merged.drop(columns<span class="op">=</span>[col], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        test_merged.drop(columns<span class="op">=</span>[col], inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="quantile-binning-of-features" class="level2">
<h2 class="anchored" data-anchor-id="quantile-binning-of-features">3.6 Quantile Binning of Features</h2>
<p>To enhance robustness and reduce sensitivity to outliers, we converted few continuous features into categorical bins using quantile-based binning. Growth metrics such as land value growth, building value growth, and assessed growth were binned into four quantiles, with thresholds computed only on the training data to prevent information leakage. If quantile binning failed due to low cardinality (e.g., repeated values), we defaulted to equal-width binning. All binned variables were explicitly cast as categorical to ensure compatibility with tree-based models.</p>
<p>Additionally, we binned year built final into five quantiles to capture generational differences in construction periods. This replaced raw year values with interpretable ordinal categories. Original continuous features were removed after binning to avoid redundancy and reduce multicollinearity.</p>
<div id="bf43eb3d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: List your growth features ===</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>growth_features <span class="op">=</span> [<span class="st">'land_value_growth'</span>, <span class="st">'building_value_growth'</span>, <span class="st">'assessed_growth'</span>]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Binning Function (train-based binning) ===</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bin_growth_feature_safe(train_df, test_df, feature, bins<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quantile binning on train only</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>], bin_edges <span class="op">=</span> pd.qcut(train_df[feature], q<span class="op">=</span>bins, labels<span class="op">=</span><span class="va">False</span>, retbins<span class="op">=</span><span class="va">True</span>, duplicates<span class="op">=</span><span class="st">'drop'</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> pd.cut(test_df[feature], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fallback: Equal-width binning</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        min_val <span class="op">=</span> train_df[feature].<span class="bu">min</span>()</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        max_val <span class="op">=</span> train_df[feature].<span class="bu">max</span>()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        bin_edges <span class="op">=</span> np.linspace(min_val, max_val, bins <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> pd.cut(train_df[feature], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> pd.cut(test_df[feature], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to category</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_df, test_df</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Apply to train_merged and test_merged ===</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature <span class="kw">in</span> growth_features:</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    train_merged, test_merged <span class="op">=</span> bin_growth_feature_safe(train_merged, test_merged, feature)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 4: Bin year_built_final using train-based quantiles ===</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>train_merged[<span class="st">'year_built_bin'</span>], bin_edges <span class="op">=</span> pd.qcut(</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    train_merged[<span class="st">'year_built_final'</span>], q<span class="op">=</span><span class="dv">5</span>, retbins<span class="op">=</span><span class="va">True</span>, labels<span class="op">=</span><span class="va">False</span>, duplicates<span class="op">=</span><span class="st">'drop'</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>test_merged[<span class="st">'year_built_bin'</span>] <span class="op">=</span> pd.cut(</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    test_merged[<span class="st">'year_built_final'</span>], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to category</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>train_merged[<span class="st">'year_built_bin'</span>] <span class="op">=</span> train_merged[<span class="st">'year_built_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>test_merged[<span class="st">'year_built_bin'</span>] <span class="op">=</span> test_merged[<span class="st">'year_built_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 5: Drop original continuous columns ===</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>cols_to_drop <span class="op">=</span> growth_features <span class="op">+</span> [<span class="st">'year_built_final'</span>]</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>train_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>test_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Binned growth &amp; year_built features safely with no leakage."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="rare-frequency-suppression-in-spatial-encodings" class="level2">
<h2 class="anchored" data-anchor-id="rare-frequency-suppression-in-spatial-encodings">3.7 Rare Frequency Suppression in Spatial Encodings</h2>
<p>Following frequency encoding of high-cardinality spatial variables (region, neighborhood, zone, subneighborhood), we applied a rare-value suppression step to mitigate the noise introduced by sparsely represented categories. For each frequency-encoded column, we identified values that occurred in less than 0.1% of the training data and replaced them with a neutral value of zero in both the training and test sets. This was done using thresholds derived solely from the training distribution to prevent data leakage.</p>
<p>The intuition behind this strategy is that extremely rare spatial groupings may not provide reliable or generalizable signals to the model. Treating them as a common fallback class (i.e., assigning them a frequency of zero) improves model stability and reduces overfitting to idiosyncratic, low-support locations. This transformation preserves the informativeness of frequent categories while smoothing out sparse tail behavior in the feature space.</p>
<div id="3a53bf60" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define frequency columns and threshold</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>freq_cols <span class="op">=</span> [<span class="st">'region_freq'</span>, <span class="st">'neighborhood_freq'</span>, <span class="st">'zone_freq'</span>, <span class="st">'subneighborhood_freq'</span>]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>rare_thresh <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply rare value replacement for each frequency column</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> freq_cols:</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        rare_vals <span class="op">=</span> train_merged[col].value_counts(normalize<span class="op">=</span><span class="va">True</span>)[<span class="kw">lambda</span> x: x <span class="op">&lt;</span> rare_thresh].index</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        train_merged[col] <span class="op">=</span> train_merged[col].replace(rare_vals, <span class="dv">0</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        test_merged[col] <span class="op">=</span> test_merged[col].replace(rare_vals, <span class="dv">0</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Replaced rare values in </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> using train_merged threshold &lt; </span><span class="sc">{</span>rare_thresh<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Column </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> not found in train_merged — skipping."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="log-transformation-and-distribution-smoothing-for-ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="log-transformation-and-distribution-smoothing-for-ridge-regression">3.8 Log Transformation and Distribution Smoothing for Ridge Regression</h2>
<p>To satisfy linear model assumptions and reduce skew-related distortion in Ridge regression, we applied a targeted log transformation to select continuous features. Specifically, we identified variables related to building size, land area, and valuation (e.g., building value 2019, land area 2018, neigh assess mean) whose skewness exceeded a threshold of 2.0 in the training set. For these features, we applied a log1p transformation, which effectively stabilized variance, compressed long-tailed distributions, and improved linear fit potential.</p>
<p>This transformation was particularly useful for the Ridge regression model, which benefits from normally distributed inputs and is sensitive to extreme values. By selectively applying log1p only to features with high skew, we preserved model interpretability while enhancing numerical stability and predictive performance.</p>
<div id="2645be05" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Skew-based Log Transformation (2015–2019 only) ===</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>log_bases <span class="op">=</span> [</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'floor_area_total'</span>, <span class="st">'porch_area'</span>, <span class="st">'building_area'</span>, <span class="st">'land_area'</span>,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_value'</span>, <span class="st">'land_value'</span>, <span class="st">'assessed'</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>neigh_stat_cols <span class="op">=</span> [</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'neigh_assess_mean'</span>, <span class="st">'neigh_assess_std'</span>, <span class="st">'neigh_assess_median'</span>,</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'neigh_assess_q1'</span>, <span class="st">'neigh_assess_q3'</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect log-transformable columns (2015–2019 + neighborhood stats)</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>log_transform_cols <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> log_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>log_transform_cols <span class="op">+=</span> neigh_stat_cols</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute skewness on train and apply log1p only if skew &gt; 2</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> log_transform_cols:</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        skew <span class="op">=</span> train_merged[col].skew()</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> skew <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> df <span class="kw">in</span> [train_merged, test_merged]:</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>                df[<span class="ss">f"log_</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> np.log1p(df[col])</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Log-transformed: </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> (skew=</span><span class="sc">{</span>skew<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"ℹ Skipped: </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> (skew=</span><span class="sc">{</span>skew<span class="sc">:.2f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="adaptive-quantile-clipping-for-tree-based-models" class="level2">
<h2 class="anchored" data-anchor-id="adaptive-quantile-clipping-for-tree-based-models">3.9 Adaptive Quantile Clipping for Tree-Based Models</h2>
<p>To further control the influence of extreme values in tree-based models, we implemented an adaptive quantile clipping strategy informed by skewness severity. Using a precomputed skewness report, we categorized numeric features (excluding binary and target-encoded variables) into two groups: ultra-skewed (skewness &gt; 100) and moderately-skewed (2 &lt; skewness ≤ 100). Features were considered only if they had more than ten unique values and were not binary.</p>
<p>For ultra-skewed features, we applied clipping at the 0.5th and 99.5th percentiles. For moderately skewed features, we clipped at the 0.1st and 99.9th percentiles. All thresholds were derived solely from the training data and applied to both training and test sets to ensure leakage-free transformations. This clipping procedure helped suppress extreme values that might otherwise dominate decision paths or split criteria in tree-based learners.</p>
<p>These transformations were specifically designed for use with XGBoost and LightGBM, where reducing the influence of outliers improves model generalization and enhances interpretability in leaf-based decision structures.</p>
<div id="0ca4fd98" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Categorize features by skew level ===</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>ultra_skewed <span class="op">=</span> []</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>moderately_skewed <span class="op">=</span> []</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> skew_df.iterrows():</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    feature <span class="op">=</span> row[<span class="st">'feature'</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    skew <span class="op">=</span> row[<span class="st">'skewness'</span>]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature <span class="kw">not</span> <span class="kw">in</span> train_merged.columns:</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    unique_vals <span class="op">=</span> train_merged[feature].nunique()</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    is_binary <span class="op">=</span> <span class="bu">set</span>(train_merged[feature].dropna().unique()).issubset({<span class="dv">0</span>, <span class="dv">1</span>})</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> unique_vals <span class="op">&gt;</span> <span class="dv">10</span> <span class="kw">and</span> <span class="kw">not</span> is_binary <span class="kw">and</span> <span class="kw">not</span> feature.endswith(<span class="st">'_te'</span>):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> skew <span class="op">&gt;</span> <span class="dv">100</span>:</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>            ultra_skewed.append(feature)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="dv">2</span><span class="op">&lt;</span> skew <span class="op">&lt;=</span> <span class="dv">100</span>:</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>            moderately_skewed.append(feature)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" </span><span class="sc">{</span><span class="bu">len</span>(ultra_skewed)<span class="sc">}</span><span class="ss"> ultra-skewed features to clip at 0.995."</span>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" </span><span class="sc">{</span><span class="bu">len</span>(moderately_skewed)<span class="sc">}</span><span class="ss"> moderately-skewed features to clip at 0.999."</span>)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Compute quantile clipping bounds ===</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>clip_bounds <span class="op">=</span> {}</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> ultra_skewed:</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    clip_bounds[col] <span class="op">=</span> (</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.005</span>),</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.995</span>)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> moderately_skewed:</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    clip_bounds[col] <span class="op">=</span> (</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.001</span>),</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.999</span>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Apply clipping to both train and test ===</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df_name, df <span class="kw">in</span> [(<span class="st">'train_merged'</span>, train_merged), (<span class="st">'test_merged'</span>, test_merged)]:</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col, (lower, upper) <span class="kw">in</span> clip_bounds.items():</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>            df[col] <span class="op">=</span> df[col].clip(lower, upper)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Adaptive clipping applied: 0.995 for ultra-skewed, 0.999 for moderately-skewed features."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="interaction-features-for-linear-and-nonlinear-models" class="level2">
<h2 class="anchored" data-anchor-id="interaction-features-for-linear-and-nonlinear-models">3.10 Interaction Features for Linear and Nonlinear Models</h2>
<p>To enrich model expressiveness, we engineered a comprehensive set of interaction features used across both Ridge regression and tree-based models (XGBoost and LightGBM). These included multiplicative and ratio-based terms such as grade quality index, value per age, area x quality, and assess to neigh mean, capturing relationships between physical dimensions, valuation, quality, and neighborhood context. For Ridge regression, these features acted as implicit basis expansions—effectively enabling the linear model to capture non-additive effects by introducing new combinations of input features.</p>
<p>Additionally, we created a specialized set of log-transformed interaction terms—such as log area x grade, log assess x age, and log value diff—used exclusively in the Ridge pipeline. These features helped linearize multiplicative relationships and reduce skew, improving fit under Ridge’s sensitivity to input distribution. Log-based interactions were excluded from tree models, which are inherently robust to skew and insensitive to monotonic transformations like log, as they rely only on the relative ordering of feature values when making splits.</p>
<div id="6d95d0cb" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === Interaction Features for Ridge Regression ===</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_features(df):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Ratio features ===</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'area_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'porch_ratio'</span>] <span class="op">=</span> df[<span class="st">'porch_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'floor_density'</span>] <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_build_density'</span>] <span class="op">=</span> df[<span class="st">'log_building_area_2019'</span>] <span class="op">-</span> df[<span class="st">'log_land_area_2019'</span>]</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_land_to_build_ratio'</span>] <span class="op">=</span> df[<span class="st">'log_land_area_2019'</span>] <span class="op">-</span> df[<span class="st">'log_building_area_2019'</span>]</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'land_value_2018'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_value_diff'</span>] <span class="op">=</span> df[<span class="st">'log_building_value_2018'</span>] <span class="op">-</span> df[<span class="st">'log_land_value_2018'</span>]</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'price_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Bathroom &amp; room structure ===</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_score'</span>] <span class="op">=</span> df[<span class="st">'full_bath_2019'</span>] <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> df[<span class="st">'half_bath_2019'</span>]</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_density'</span>] <span class="op">=</span> df[<span class="st">'bathroom_score'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedroom_ratio'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_per_floor'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'floors_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Core interactions ===</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedrooms_x_floors'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">*</span> df[<span class="st">'floors_2019'</span>]</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_x_quality'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_area_x_grade'</span>] <span class="op">=</span> df[<span class="st">'log_building_area_2019'</span>] <span class="op">*</span> df[<span class="st">'grade_2019'</span>]</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_assess_x_age'</span>] <span class="op">=</span> df[<span class="st">'log_assessed_2018'</span>] <span class="op">*</span> df[<span class="st">'building_age'</span>]</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_spread_neigh'</span>] <span class="op">=</span> df[<span class="st">'log_neigh_assess_q3'</span>] <span class="op">-</span> df[<span class="st">'log_neigh_assess_q1'</span>]</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'grade_quality_index'</span>] <span class="op">=</span> df[<span class="st">'grade_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Clean up ===</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.replace([np.inf, <span class="op">-</span>np.inf], np.nan)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.fillna(<span class="dv">0</span>)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co"># === Apply to train and test ===</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>train_merged <span class="op">=</span> add_features(train_merged)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>test_merged <span class="op">=</span> add_features(test_merged)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1ee6c5bc" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === Interaction Features for Tree models ===</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_features(df):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Ratio features ===</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'area_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'porch_ratio'</span>] <span class="op">=</span> df[<span class="st">'porch_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'floor_density'</span>] <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'land_value_2018'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'price_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Bathroom &amp; room structure ===</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_score'</span>] <span class="op">=</span> df[<span class="st">'full_bath_2019'</span>] <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> df[<span class="st">'half_bath_2019'</span>]</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_density'</span>] <span class="op">=</span> df[<span class="st">'bathroom_score'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedroom_ratio'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_per_floor'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'floors_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Core interactions ===</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedrooms_x_floors'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">*</span> df[<span class="st">'floors_2019'</span>]</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_x_quality'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_x_age'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">*</span> df[<span class="st">'building_age'</span>]</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'grade_quality_index'</span>] <span class="op">=</span> df[<span class="st">'grade_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Selected high-signal interactions ===</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'area_x_quality'</span>] <span class="op">=</span> df[<span class="st">'building_area_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'floor_area_x_grade'</span>] <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">*</span> df[<span class="st">'grade_2019'</span>]</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_to_neigh_median'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_median'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_to_neigh_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_mean'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_per_age'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_age'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Clean up ===</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.replace([np.inf, <span class="op">-</span>np.inf], np.nan)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.fillna(<span class="dv">0</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="co"># === Apply to train and test sets ===</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>train_merged <span class="op">=</span> add_features(train_merged)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>test_merged <span class="op">=</span> add_features(test_merged)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="model-development-and-tuning" class="level1">
<h1>4. Model Development and Tuning</h1>
<section id="ridge-regression-with-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-with-cross-validation">4.1 Ridge Regression with Cross-Validation</h2>
<p>We implemented a Ridge regression model using RidgeCV to automatically select the regularization strength α through nested cross-validation. A 3-fold outer loop was used for estimating out-of-fold (OOF) performance, while each inner fold evaluated a grid of α values ranging from 10⁻³ to 10² on a logarithmic scale. Input features were standardized within a Pipeline using StandardScaler to ensure scale-invariant regression coefficients. The model selected a different optimal α for each fold, reflecting local variance in validation behavior:</p>
<ul>
<li>Fold 1 RMSE: 42,050.33—Best α: 2.1544</li>
<li>Fold 2 RMSE: 41,036.52—Best α: 27.8256</li>
<li>Fold 3 RMSE: 40,619.40—Best α: 0.5995</li>
</ul>
<p>The final out-of-fold RMSE across all folds was 41,239.79, with an average best α of approximately 10.1932. This indicates that moderate regularization consistently improved generalization across different training splits.</p>
<p>We saved both OOF predictions and test forecasts as NumPy arrays—<code>ridgecv_oof_preds.npy</code> and <code>ridgecv_test_preds.npy</code>—for later use in model ensembling. These stored outputs served as reliable building blocks for downstream blending and stacking strategies.</p>
<div id="275b6977" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> RidgeCV</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Prepare training/test matrices ===</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> train_merged.copy()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_merged.copy()</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.Series(y_train).values <span class="co"># use raw target (not log)</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: RidgeCV pipeline ===</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>ridge_oof <span class="op">=</span> np.zeros(<span class="bu">len</span>(X))</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>ridge_test_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_test))</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>best_alphas <span class="op">=</span> []</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X)):</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> Fold </span><span class="sc">{</span>fold<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/5"</span>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    X_train, y_train_fold <span class="op">=</span> X.iloc[train_idx], y[train_idx]</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    X_val, y_val <span class="op">=</span> X.iloc[val_idx], y[val_idx]</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_pipeline(</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        StandardScaler(),</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        RidgeCV(alphas<span class="op">=</span>alphas, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train_fold)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    ridge_oof[val_idx] <span class="op">=</span> model.predict(X_val)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    ridge_test_preds <span class="op">+=</span> model.predict(X_test) <span class="op">/</span> kf.get_n_splits()</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    best_alpha <span class="op">=</span> model.named_steps[<span class="st">'ridgecv'</span>].alpha_</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    best_alphas.append(best_alpha)</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>    fold_rmse <span class="op">=</span> root_mean_squared_error(y_val, ridge_oof[val_idx])</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Fold </span><span class="sc">{</span>fold<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> RMSE: </span><span class="sc">{</span>fold_rmse<span class="sc">:,.2f}</span><span class="ss"> | Best alpha: </span><span class="sc">{</span>best_alpha<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 4: Final RMSE ===</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>final_rmse <span class="op">=</span> root_mean_squared_error(y, ridge_oof)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> Final OOF RMSE (RidgeCV): </span><span class="sc">{</span>final_rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Average best alpha across folds: </span><span class="sc">{</span>np<span class="sc">.</span>mean(best_alphas)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 5: Save predictions ===</span></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ACCOUNT"</span>: acct_test.values.ravel(),</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TARGET"</span>: ridge_test_preds</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_ridgecv_pipeline.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st"> Saved: submission_ridgecv_pipeline.csv"</span>)</span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a><span class="co"># === Optional: Save OOF &amp; test preds for stacking or analysis ===</span></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"ridgecv_oof_preds.npy"</span>, ridge_oof)</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"ridgecv_test_preds.npy"</span>, ridge_test_preds)</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: ridgecv_oof_preds.npy and ridgecv_test_preds.npy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tree-based-models-with-optuna-and-shap-gain-feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="tree-based-models-with-optuna-and-shap-gain-feature-selection">4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection</h2>
<p>To capture nonlinear interactions and leverage automatic handling of missing values and categorical splits, we trained two gradient boosting models: LightGBM and XGBoost. Both models followed a structured pipeline consisting of hyperparameter optimization using Optuna, followed by SHAP- and gain-based feature selection, and a final retraining on the selected features.</p>
<p><strong>Step 1: Hyperparameter Tuning with Optuna.</strong> For each model, we defined an Optuna objective that trained 3-fold cross-validated models using early stopping. We explored hyperparameter ranges tailored to each algorithm, with LightGBM using a native pruning callback and XGBoost leveraging <code>XGBoostPruningCallback</code>. During tuning, we stored the best out-of-fold (OOF) predictions across all trials to later use in ensembling.</p>
<div id="d7d16a48" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm <span class="im">as</span> lgb</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.integration <span class="im">import</span> LightGBMPruningCallback</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.pruners <span class="im">import</span> SuccessiveHalvingPruner</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightgbm <span class="im">import</span> log_evaluation, early_stopping</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 0: Setup Data ===</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>X_full <span class="op">=</span> train_merged.copy()</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>y_full <span class="op">=</span> pd.Series(y_train)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_merged.copy()</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Detect categorical columns</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>cat_cols <span class="op">=</span> X_full.select_dtypes(include<span class="op">=</span>[<span class="st">'category'</span>, <span class="st">'object'</span>]).columns.tolist()</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> cat_cols:</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    X_full[col] <span class="op">=</span> X_full[col].astype(<span class="st">"category"</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    X_test[col] <span class="op">=</span> X_test[col].astype(<span class="st">"category"</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>global_oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Define Optuna Objective ===</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> global_oof_preds, best_score</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">"objective"</span>: <span class="st">"regression"</span>,</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        <span class="st">"metric"</span>: <span class="st">"rmse"</span>,</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="st">"boosting_type"</span>: <span class="st">"gbdt"</span>,</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: trial.suggest_float(<span class="st">"learning_rate"</span>, <span class="fl">0.025</span>, <span class="fl">0.04</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">"num_leaves"</span>: trial.suggest_int(<span class="st">"num_leaves"</span>, <span class="dv">160</span>, <span class="dv">220</span>),</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="st">"max_depth"</span>: trial.suggest_int(<span class="st">"max_depth"</span>, <span class="dv">7</span>, <span class="dv">11</span>),</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        <span class="st">"min_child_samples"</span>: trial.suggest_int(<span class="st">"min_child_samples"</span>, <span class="dv">18</span>, <span class="dv">30</span>),</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">"subsample"</span>: trial.suggest_float(<span class="st">"subsample"</span>, <span class="fl">0.65</span>, <span class="fl">0.88</span>),</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">"colsample_bytree"</span>: trial.suggest_float(<span class="st">"colsample_bytree"</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>),</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_alpha"</span>: trial.suggest_float(<span class="st">"reg_alpha"</span>, <span class="fl">1.0</span>, <span class="fl">5.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_lambda"</span>: trial.suggest_float(<span class="st">"reg_lambda"</span>, <span class="fl">1.0</span>, <span class="fl">4.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        <span class="st">"min_split_gain"</span>: trial.suggest_float(<span class="st">"min_split_gain"</span>, <span class="fl">0.15</span>, <span class="fl">0.25</span>),</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">"verbose"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"n_jobs"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> []</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>    oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>        X_train, X_val <span class="op">=</span> X_full.iloc[train_idx], X_full.iloc[val_idx]</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>        y_train_fold, y_val <span class="op">=</span> y_full.iloc[train_idx], y_full.iloc[val_idx]</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>        dtrain <span class="op">=</span> lgb.Dataset(X_train, label<span class="op">=</span>y_train_fold, categorical_feature<span class="op">=</span>cat_cols)</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        dvalid <span class="op">=</span> lgb.Dataset(X_val, label<span class="op">=</span>y_val, categorical_feature<span class="op">=</span>cat_cols)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> lgb.train(</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>            params,</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>            dtrain,</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>            valid_sets<span class="op">=</span>[dvalid],</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>            num_boost_round<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>            callbacks<span class="op">=</span>[</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>                early_stopping(stopping_rounds<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>                log_evaluation(period<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>                LightGBMPruningCallback(trial, <span class="st">"rmse"</span>)</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>        val_pred <span class="op">=</span> model.predict(X_val, num_iteration<span class="op">=</span>model.best_iteration)</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>        oof_preds[val_idx] <span class="op">=</span> val_pred</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>        val_rmse.append(root_mean_squared_error(y_val, val_pred))</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>    mean_rmse <span class="op">=</span> np.mean(val_rmse)</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>    trial.set_user_attr(<span class="st">"cv_rmse"</span>, mean_rmse)</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mean_rmse <span class="op">&lt;</span> best_score:</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> mean_rmse</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>        global_oof_preds[:] <span class="op">=</span> oof_preds</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> | CV RMSE: </span><span class="sc">{</span>mean_rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_rmse</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Run Optuna ===</span></span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>    direction<span class="op">=</span><span class="st">'minimize'</span>,</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">'lgbm_study_final_with_shap'</span>,</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span><span class="st">'sqlite:///lgbm_study_final_with_shap.db'</span>,</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>SuccessiveHalvingPruner(min_resource<span class="op">=</span><span class="dv">100</span>, reduction_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">25</span>, show_progress_bar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best RMSE:"</span>, study.best_value)</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best Parameters:"</span>, study.best_params)</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"oof_preds_lgbm.npy"</span>, global_oof_preds)</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: oof_preds_lgbm.npy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="75581870" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.integration <span class="im">import</span> XGBoostPruningCallback</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> shap <span class="im">import</span> TreeExplainer</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 0: Prepare Data ===</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>X_full <span class="op">=</span> train_merged.copy()</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>y_full <span class="op">=</span> pd.Series(y_train)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_merged.copy()</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>bin_cols <span class="op">=</span> [</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_value_growth_bin'</span>,</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'assessed_growth_bin'</span>,</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'land_value_growth_bin'</span>,<span class="st">'year_built_bin'</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> bin_cols:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    X_full[col] <span class="op">=</span> X_full[col].cat.codes</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    X_test[col] <span class="op">=</span> X_test[col].cat.codes</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>categorical_cols <span class="op">=</span> X_full.select_dtypes(include<span class="op">=</span><span class="st">'object'</span>).columns.tolist()</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>X_full[categorical_cols] <span class="op">=</span> X_full[categorical_cols].astype(<span class="st">'category'</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>X_test[categorical_cols] <span class="op">=</span> X_test[categorical_cols].astype(<span class="st">'category'</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co"># === Global OOF Tracker ===</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>global_oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="bu">float</span>(<span class="st">"inf"</span>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Optuna Objective Function (No SHAP during tuning) ===</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> global_oof_preds, best_score</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        <span class="st">"objective"</span>: <span class="st">"reg:squarederror"</span>,</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>        <span class="st">"eval_metric"</span>: <span class="st">"rmse"</span>,</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        <span class="st">"tree_method"</span>: <span class="st">"hist"</span>,</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: trial.suggest_float(<span class="st">"learning_rate"</span>, <span class="fl">0.047</span>, <span class="fl">0.05</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        <span class="st">"max_depth"</span>: <span class="dv">6</span>,</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        <span class="st">"min_child_weight"</span>: trial.suggest_int(<span class="st">"min_child_weight"</span>, <span class="dv">11</span>, <span class="dv">12</span>),</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>        <span class="st">"subsample"</span>: trial.suggest_float(<span class="st">"subsample"</span>, <span class="fl">0.87</span>, <span class="fl">0.89</span>),</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        <span class="st">"colsample_bytree"</span>: trial.suggest_float(<span class="st">"colsample_bytree"</span>, <span class="fl">0.7</span>, <span class="fl">0.74</span>),</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_alpha"</span>: trial.suggest_float(<span class="st">"reg_alpha"</span>, <span class="fl">0.30</span>, <span class="fl">0.56</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_lambda"</span>: trial.suggest_float(<span class="st">"reg_lambda"</span>, <span class="fl">0.05</span>, <span class="fl">0.11</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        <span class="st">"gamma"</span>: trial.suggest_float(<span class="st">"gamma"</span>, <span class="fl">1.1</span>, <span class="fl">4.3</span>),</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">"n_estimators"</span>: <span class="dv">1000</span>,</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>        <span class="st">"n_jobs"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>        <span class="st">"enable_categorical"</span>: <span class="va">True</span>,</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>    fold_rmse <span class="op">=</span> []</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>        X_train, y_train_fold <span class="op">=</span> X_full.iloc[train_idx], y_full.iloc[train_idx]</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>        X_val, y_val <span class="op">=</span> X_full.iloc[val_idx], y_full.iloc[val_idx]</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> XGBRegressor(</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>params,</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>            early_stopping_rounds<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>            callbacks<span class="op">=</span>[XGBoostPruningCallback(trial, <span class="st">"validation_0-rmse"</span>),</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>                       ]</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>        model.fit(X_train, y_train_fold, eval_set<span class="op">=</span>[(X_val, y_val)], verbose<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>        val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>        oof_preds[val_idx] <span class="op">=</span> val_pred</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>        fold_rmse.append(root_mean_squared_error(y_val, val_pred))</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>    mean_rmse <span class="op">=</span> np.mean(fold_rmse)</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>    trial.set_user_attr(<span class="st">"cv_rmse"</span>, mean_rmse)</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mean_rmse <span class="op">&lt;</span> best_score:</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> mean_rmse</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>        global_oof_preds[:] <span class="op">=</span> oof_preds</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> | CV RMSE: </span><span class="sc">{</span>mean_rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_rmse</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Run Optuna ===</span></span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>    direction<span class="op">=</span><span class="st">'minimize'</span>,</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">'xgbreg_optuna_final_no_shap'</span>,</span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>optuna.pruners.SuccessiveHalvingPruner(min_resource<span class="op">=</span><span class="dv">100</span>, reduction_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">25</span>, show_progress_bar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best RMSE:"</span>, study.best_value)</span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best Parameters:"</span>, study.best_params)</span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"oof_preds_xgbreg.npy"</span>, global_oof_preds)</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: oof_preds_xgbreg.npy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Step 2: SHAP and Gain-Based Feature Selection.</strong> After tuning, we trained new LightGBM and XGBoost models using the best parameters on each fold of the training data. For each fold, we computed SHAP importance values and LightGBM/XGBoost gain importances. We retained features that collectively accounted for 95% of total importance in either SHAP or gain, and constructed a union of these high-signal features across all folds. This union was used to define the final reduced feature space for retraining.</p>
<div id="8dff1de2" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: SHAP + GAIN Feature Selection for LGBM ===</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>selected_feature_sets <span class="op">=</span> []</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    X_train_raw, y_train_fold <span class="op">=</span> X_full.iloc[train_idx], y_full.iloc[train_idx]</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> lgb.Dataset(X_train_raw, label<span class="op">=</span>y_train_fold, categorical_feature<span class="op">=</span>cat_cols)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    model_temp <span class="op">=</span> lgb.train(</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        study.best_params,</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        train_dataset,</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        num_boost_round<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        valid_sets<span class="op">=</span>[train_dataset],</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        callbacks<span class="op">=</span>[log_evaluation(period<span class="op">=</span><span class="dv">100</span>)] </span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># SHAP importance</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    explainer <span class="op">=</span> shap.TreeExplainer(model_temp)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    shap_values <span class="op">=</span> explainer.shap_values(X_train_raw)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    shap_df <span class="op">=</span> pd.DataFrame(np.<span class="bu">abs</span>(shap_values), columns<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    shap_importance <span class="op">=</span> shap_df.mean().sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    shap_cumsum <span class="op">=</span> shap_importance.cumsum() <span class="op">/</span> shap_importance.<span class="bu">sum</span>()</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    top_shap <span class="op">=</span> shap_cumsum[shap_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gain importance</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    gain_importance <span class="op">=</span> pd.Series(model_temp.feature_importance(importance_type<span class="op">=</span><span class="st">'gain'</span>), index<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    gain_sorted <span class="op">=</span> gain_importance.sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    gain_cumsum <span class="op">=</span> gain_sorted.cumsum() <span class="op">/</span> gain_sorted.<span class="bu">sum</span>()</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    top_gain <span class="op">=</span> gain_cumsum[gain_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    selected_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(top_shap).union(<span class="bu">set</span>(top_gain)))</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    selected_feature_sets.append(selected_features)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 4: Final Feature Union ===</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>final_union_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>().union(<span class="op">*</span>selected_feature_sets))</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Final Union Feature Count:"</span>, <span class="bu">len</span>(final_union_features))</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter only those categorical columns that are in final features</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>filtered_cat_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> cat_cols <span class="cf">if</span> col <span class="kw">in</span> final_union_features]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7fadbf5c" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: Post-Optuna SHAP + Gain Feature Selection for XGBoost ===</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>selected_feature_sets <span class="op">=</span> []</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    X_train_raw, y_train_fold <span class="op">=</span> X_full.iloc[train_idx], y_full.iloc[train_idx]</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    model_temp <span class="op">=</span> XGBRegressor(<span class="op">**</span>study.best_params, n_estimators<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    model_temp.fit(X_train_raw, y_train_fold)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === SHAP Importance ===</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    explainer <span class="op">=</span> TreeExplainer(model_temp)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    shap_values <span class="op">=</span> explainer.shap_values(X_train_raw)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    shap_df <span class="op">=</span> pd.DataFrame(np.<span class="bu">abs</span>(shap_values), columns<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    shap_importance <span class="op">=</span> shap_df.mean().sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    shap_cumsum <span class="op">=</span> shap_importance.cumsum() <span class="op">/</span> shap_importance.<span class="bu">sum</span>()</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    top_shap <span class="op">=</span> shap_cumsum[shap_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Gain Importance ===</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    gain_importance <span class="op">=</span> pd.Series(model_temp.feature_importances_, index<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    gain_sorted <span class="op">=</span> gain_importance.sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    gain_cumsum <span class="op">=</span> gain_sorted.cumsum() <span class="op">/</span> gain_sorted.<span class="bu">sum</span>()</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    top_gain <span class="op">=</span> gain_cumsum[gain_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    selected_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(top_shap).union(<span class="bu">set</span>(top_gain)))</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    selected_feature_sets.append(selected_features)</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 4: Final Feature Union ===</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>final_union_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>().union(<span class="op">*</span>selected_feature_sets))</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Final Union Feature Count:"</span>, <span class="bu">len</span>(final_union_features))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Step 3: Final Model Training and Inference.</strong> Each final model was retrained on the full training set using only the selected features, with early stopping enabled to prevent overfitting. Predictions on the test set were generated using the best iteration count. All model outputs—including OOF predictions and test forecasts—were saved for submission and later use in ensemble blending.</p>
<div id="f6632733" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 5: Final Model on Selected Features for LGBM ===</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>X_full_selected <span class="op">=</span> X_full[final_union_features]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>X_test_selected <span class="op">=</span> X_test[final_union_features]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>final_dataset <span class="op">=</span> lgb.Dataset(X_full_selected, label<span class="op">=</span>y_full, categorical_feature<span class="op">=</span>filtered_cat_cols)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> lgb.train(</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    study.best_params,</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    final_dataset,</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    num_boost_round<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    valid_sets<span class="op">=</span>[final_dataset],</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    valid_names<span class="op">=</span>[<span class="st">"train"</span>],</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[log_evaluation(period<span class="op">=</span><span class="dv">100</span>)]</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 6: Predict on Test Set ===</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>test_preds <span class="op">=</span> final_model.predict(X_test_selected, num_iteration<span class="op">=</span>final_model.best_iteration)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_lgbm_shap.npy"</span>, test_preds)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: test_preds_lgbm_shap.npy"</span>)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 7: Save Submission ===</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ACCOUNT'</span>: acct_test.values.ravel(),  <span class="co"># Replace with your ID col</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'TARGET'</span>: test_preds</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_lgbm_shap.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Submission saved: submission_lgbm_shap.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e24122b2" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 5: Final Model on Selected Features for XGBoost ===</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>X_full_selected <span class="op">=</span> X_full[final_union_features]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>X_test_selected <span class="op">=</span> X_test[final_union_features]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> XGBRegressor(<span class="op">**</span>study.best_params)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>final_model.set_params(n_estimators<span class="op">=</span><span class="dv">1000</span>, verbosity<span class="op">=</span><span class="dv">1</span>, early_stopping_rounds<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>final_model.fit(X_full_selected, y_full, eval_set<span class="op">=</span>[(X_full_selected, y_full)], verbose<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 6: Predict on Test Set ===</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>test_preds <span class="op">=</span> final_model.predict(X_test_selected)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_xgbreg.npy"</span>, test_preds)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: test_preds_xgbreg.npy"</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 7: Create Submission File ===</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>account_ids <span class="op">=</span> acct_test.values.ravel()  <span class="co"># Replace with actual ID column</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ACCOUNT'</span>: account_ids,</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'TARGET'</span>: test_preds</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_xgbreg.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Submission saved: submission_xgbreg.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This hybrid approach—combining Optuna-based tuning with SHAP-driven interpretability—allowed us to retain only high-impact features, thereby improving generalization and reducing overfitting without sacrificing performance. The best out-of-fold RMSE achieved was 40,925.29 with XGBoost and 41,641.42 with LightGBM, confirming the robustness of both pipelines.</p>
</section>
</section>
<section id="ensembling-strategy" class="level1">
<h1>5. Ensembling Strategy</h1>
<section id="weighted-model-blending-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="weighted-model-blending-with-optuna">5.1 Weighted Model Blending with Optuna</h2>
<p>To consolidate the strengths of our top-performing base models—XGBoost, RidgeCV, and LightGBM—we employed a weighted blending strategy optimized using Optuna. This approach directly searched for the optimal linear combination of model predictions that minimized RMSE on a holdout set.</p>
<p>We first constructed a meta-training set consisting of out-of-fold (OOF) predictions from each base model. A corresponding test matrix was constructed from each model’s final test predictions. The blending weights were constrained to be non-negative and normalized to sum to one.</p>
<p>An Optuna study was run for 100 trials, where each trial proposed a new set of blending weights and evaluated their performance via RMSE on the holdout split. The final optimized weights were:</p>
<ul>
<li>XGBoost: w₀ = 25.98%</li>
<li>RidgeCV: w₁ = 33.53%</li>
<li>LightGBM: w₂ = 40.49%</li>
</ul>
<p>These weights were then used to produce a final blended prediction for the test set. The resulting predictions achieved an RMSE of 36,239.91 on the holdout set—outperforming all individual base models and demonstrating the value of combining linear and tree-based perspectives.</p>
<div id="3d47499c" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co"># === Setup Logging ===</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.INFO, <span class="bu">format</span><span class="op">=</span><span class="st">"</span><span class="sc">%(asctime)s</span><span class="st"> [</span><span class="sc">%(levelname)s</span><span class="st">] </span><span class="sc">%(message)s</span><span class="st">"</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>logger <span class="op">=</span> logging.getLogger(<span class="st">"OptunaBlender"</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>optuna.logging.set_verbosity(optuna.logging.INFO)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load base model predictions ===</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>oof_xgb <span class="op">=</span> np.load(<span class="st">"oof_preds_xgbreg.npy"</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>ridge_oof <span class="op">=</span> np.load(<span class="st">"ridgecv_oof_preds.npy"</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>oof_lgb<span class="op">=</span>np.load(<span class="st">"oof_preds_lgbm.npy"</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>test_xgb <span class="op">=</span> np.load(<span class="st">"test_preds_xgbreg.npy"</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>ridge_test_preds <span class="op">=</span> np.load(<span class="st">"ridgecv_test_preds.npy"</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>test_lgb<span class="op">=</span>np.load(<span class="st">"test_preds_lgbm_shap.npy"</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="co"># === Targets and prediction stack ===</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>y_meta <span class="op">=</span> train[<span class="st">'TARGET'</span>].values</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>X_base <span class="op">=</span> np.vstack([oof_xgb, ridge_oof,oof_lgb]).T</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>X_test_base <span class="op">=</span> np.vstack([test_xgb, ridge_test_preds,test_lgb]).T</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a><span class="co"># === Holdout split ===</span></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>X_train, X_holdout, y_train, y_holdout <span class="op">=</span> train_test_split(X_base, y_meta, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="co"># === Objective Function ===</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> [trial.suggest_float(<span class="ss">f"w</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, <span class="dv">0</span>, <span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X_train.shape[<span class="dv">1</span>])]</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.array(weights)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">/=</span> weights.<span class="bu">sum</span>()  <span class="co"># normalize</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> X_holdout <span class="op">@</span> weights</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">=</span> root_mean_squared_error(y_holdout, preds)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> | Weights: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(weights, <span class="dv">3</span>)<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> | RMSE: </span><span class="sc">{</span>rmse<span class="sc">:,.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rmse</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a><span class="co"># === Run Study ===</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="st">" Starting Optuna optimization for weighted blending..."</span>)</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(direction<span class="op">=</span><span class="st">"minimize"</span>)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a><span class="co"># === Best weights ===</span></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>best_weights <span class="op">=</span> np.array([study.best_trial.params[<span class="ss">f"w</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X_base.shape[<span class="dv">1</span>])])</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>best_weights <span class="op">/=</span> best_weights.<span class="bu">sum</span>()</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="ss">f" Best weights: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(best_weights, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="ss">f" Best RMSE: </span><span class="sc">{</span>study<span class="sc">.</span>best_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a><span class="co"># === Final test prediction ===</span></span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>meta_preds <span class="op">=</span> X_test_base <span class="op">@</span> best_weights</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a><span class="co"># === Save predictions ===</span></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_optuna_blended.npy"</span>, meta_preds)</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>account_ids <span class="op">=</span> acct_test.values.ravel()</span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ACCOUNT"</span>: account_ids,</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TARGET"</span>: meta_preds</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_optuna_blended.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="st">" Saved: test_preds_optuna_blended.npy and submission_optuna_blended.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="stacked-ensembling-with-elasticnetcv" class="level2">
<h2 class="anchored" data-anchor-id="stacked-ensembling-with-elasticnetcv">5.2 Stacked Ensembling with ElasticNetCV</h2>
<p>To complement our Optuna-based weighted average ensemble, we implemented a stacked generalization approach using ElasticNetCV as a meta-learner. This method treats out-of-fold (OOF) predictions from the base models—XGBoost, RidgeCV, and LightGBM—as features in a second-level regression model. By learning how to optimally combine base predictions, the meta-model can capture nonlinear inter-model relationships while applying regularization to prevent overfitting.</p>
<p><strong>Meta-Model Training.</strong> We concatenated the OOF predictions into a 3-column meta-feature matrix and used it to fit an ElasticNetCV model wrapped in a StandardScaler pipeline. The meta-model searched over a grid of <code>l1_ratio</code> and <code>alpha</code> values, using 3-fold cross-validation to identify the optimal regularization configuration.</p>
<p><strong>Holdout Evaluation.</strong> For evaluation, we trained the meta-learner on an 80% split and evaluated on a 20% holdout. The resulting RMSE was 36,344.64, closely aligned with the Optuna-weighted blend. The selected <code>alpha</code> was 0.01, indicating strong regularization and robust coefficient shrinkage.</p>
<p><strong>Final Test Predictions.</strong> The final model was retrained on the full meta-feature set and used to predict the test set. This stacked approach provided a robust and regularized alternative to linear averaging, automatically downweighting weaker models while maintaining interpretability and reproducibility.</p>
<p>On the competition leaderboard, the ElasticNetCV ensemble—combining Ridge, XGBoost, and LightGBM predictions—secured the top rank with a private leaderboard RMSE of 36,021, just 2 points above the public leaderboard score of 36,019. Interestingly, while an Optuna-weighted linear blend achieved a lower public RMSE, it ranked below the ElasticNet ensemble on the final evaluation, underscoring the latter’s generalization strength.</p>
<div id="167f8891" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> ElasticNetCV</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load OOF + Test Predictions ===</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>oof_xgb <span class="op">=</span> np.load(<span class="st">"oof_preds_xgbreg.npy"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>test_xgb <span class="op">=</span> np.load(<span class="st">"test_preds_xgbreg.npy"</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>ridge_oof<span class="op">=</span>np.load(<span class="st">"ridgecv_oof_preds.npy"</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>ridge_test_preds<span class="op">=</span>np.load(<span class="st">"ridgecv_test_preds.npy"</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>oof_lgb<span class="op">=</span>np.load(<span class="st">"oof_preds_lgbm.npy"</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>test_lgb<span class="op">=</span>np.load(<span class="st">"test_preds_lgbm_shap.npy"</span>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co"># === 3. Combine full meta-input feature set ===</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>X_meta <span class="op">=</span> np.hstack([</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    oof_xgb.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    ridge_oof.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    oof_lgb.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>y_meta <span class="op">=</span> train[<span class="st">'TARGET'</span>].values</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>X_meta_test <span class="op">=</span> np.hstack([</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    test_xgb.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    ridge_test_preds.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    test_lgb.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="co"># === 4. Train ElasticNetCV meta-learner ===</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>meta_model <span class="op">=</span> make_pipeline(</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    StandardScaler(),</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>    ElasticNetCV(</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>        l1_ratio<span class="op">=</span>[<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="dv">1</span>],</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        alphas<span class="op">=</span>np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span><span class="dv">5000</span>,</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>meta_model.fit(X_meta, y_meta)</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a><span class="co"># === 5. Predict and evaluate (optional holdout split) ===</span></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="co"># You can skip this section if you're blending on full train</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>X_train, X_holdout, y_train, y_holdout <span class="op">=</span> train_test_split(X_meta, y_meta, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>meta_model.fit(X_train, y_train)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>holdout_preds <span class="op">=</span> meta_model.predict(X_holdout)</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> root_mean_squared_error(y_holdout, holdout_preds)</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ElasticNetCV Blended Meta Holdout RMSE: </span><span class="sc">{</span>rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> meta_model.named_steps[<span class="st">'elasticnetcv'</span>].alpha_</span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Best alpha selected: </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a><span class="co"># === 6. Final predictions for test set ===</span></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>meta_preds <span class="op">=</span> meta_model.predict(X_meta_test)</span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a><span class="co"># === 7. Save blended test predictions ===</span></span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_elasticnet_blended.npy"</span>, meta_preds)</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>account_ids <span class="op">=</span> acct_test.values.ravel() </span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ACCOUNT"</span>: account_ids,  <span class="co"># Replace with your actual ID column</span></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TARGET"</span>: meta_preds</span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_elasticnet_blended.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" ElasticNetCV blended stacking submission saved."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="residual-error-analysis" class="level1">
<h1>6. Residual Error Analysis</h1>
<p>To evaluate model robustness, we conducted residual analysis across key dimensions. Three consistent patterns emerged:</p>
<ol type="1">
<li><strong>Underprediction for High-Value Properties:</strong> Residuals increased with actual property value, particularly above $5M. This indicates systematic underestimation of high-end properties, likely due to their rarity and unique characteristics.</li>
<li><strong>Volatility in Protested Properties:</strong> Properties with multiple protests between 2015–2018 exhibited larger residual variance, despite median residuals near zero. This suggests that frequently disputed properties are harder to predict, potentially due to unrecorded structural changes or historical valuation disagreements not captured in the dataset.</li>
<li><strong>Poor Generalization in Sparse Neighborhoods:</strong> Residuals were more variable and extreme in neighborhoods with low frequency in the training data. This suggests weaker model generalization in underrepresented regions. Frequency-aware encodings or collapsing sparse neighborhoods into broader groups may enhance stability and reduce prediction noise in these areas.</li>
</ol>
<div class="columns">
<div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/residuals_vs_neighborhood_freq.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Neighborhood Frequency</figcaption>
</figure>
</div>
</div><div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/residuals_by_protest_count.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Protest Count</figcaption>
</figure>
</div>
</div><div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/residuals_vs_actual_value.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Actual Value</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="shap-based-interpretability-and-insights" class="level1">
<h1>7. SHAP-Based Interpretability and Insights</h1>
<section id="top-predictive-features" class="level2">
<h2 class="anchored" data-anchor-id="top-predictive-features">Top Predictive Features</h2>
<p>SHAP analysis revealed that features like assessed 2018, building value 2018, and land value 2018 were primary drivers of the model’s predictions. Structural attributes such as building area 2019, floor area x grade, and grade 2019 also carried strong explanatory power. These results confirmed the value of engineering ratio and interaction terms that encode economic density, build quality, and age-adjusted valuation. Neighborhood-level variables, especially neigh assess std and frequency encodings, further demonstrated the importance of local context in real estate assessment.</p>
</section>
<section id="low-impact-features" class="level2">
<h2 class="anchored" data-anchor-id="low-impact-features">Low-Impact Features</h2>
<p>Although SHAP-ranked bottom 30 features showed limited average contribution, they were retained via the 95% SHAP + Gain union due to their potential complementary value. Examples include early-year indicators like quality description 2015, fireplaces 2016, and spatial ratios like porch ratio. Their inclusion likely enhanced generalization by supporting edge cases, and their low impact helped confirm that more aggressive feature pruning would have offered little gain. ::: {.columns} ::: {.column width=“50%”} <img src="images/shap_summary_bottom30_union.png" class="img-fluid" style="width:100.0%" data-fig-cap="Bottom 30 SHAP Features" alt="SHAP Bottom 30"> :::</p>
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/shap_summary_top30_union.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>SHAP Top 30</figcaption>
</figure>
</div>
</div>
<p>:::</p>
</section>
</section>
<section id="reflections-and-lessons-learned" class="level1">
<h1>8. Reflections and Lessons Learned</h1>
<p>Our modeling pipeline blended SHAP-driven interpretability with ensemble-based prediction to achieve both transparency and predictive strength. Key lessons emerged across three dimensions:</p>
<section id="what-worked-well" class="level2">
<h2 class="anchored" data-anchor-id="what-worked-well">What Worked Well</h2>
<ul>
<li>The SHAP + Gain union was highly effective in denoising the feature space and avoiding overfitting, retaining only high-impact predictors across folds.</li>
<li>The ElasticNet ensemble combined linear and non-linear model strengths to capture nuanced patterns in both well-represented and sparse regions.</li>
<li>Optuna tuning reduced manual trial-and-error and consistently improved generalization across Ridge, LGBM, and XGBoost pipelines.</li>
</ul>
</section>
<section id="what-could-improve" class="level2">
<h2 class="anchored" data-anchor-id="what-could-improve">What Could Improve</h2>
<ul>
<li>We did not model temporal dependencies across yearly features—treating them as time-series sequences or using transformer architectures may better account for evolving valuation dynamics over time.</li>
<li>Geographical variation was only partially captured using frequency encoding. Future work could explore residual stacking or blended stacking techniques with subgroup-aware features (e.g., neighborhood frequency, protest history) to correct localized error patterns more explicitly.</li>
</ul>
<p>Overall, the final ElasticNet ensemble—integrating Ridge, SHAP-informed LGBM, and Optuna-tuned XGBoost—delivered a strong performance on the private leaderboard. Future extensions may benefit from incorporating time-aware modeling, subgroup-specific residual correction, or spatially informed representations to further improve accuracy and fairness in municipal assessment systems.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb24" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "STA 9890 Project: Property Valuation"</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">  output: true</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co">  eval: false</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="fu"># 1. Background on Property Valuation and Machine Learning</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## 1.1 Why Prediction Accuracy and Interpretability Matter</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>Property assessment values directly influence individual tax obligations, urban development decisions, and housing affordability analyses. However, public datasets are often noisy and incomplete—featuring missing renovation records, outdated area measurements, or abrupt shifts in land valuation. Therefore, a high-performing ML model must not only minimize prediction error but also provide interpretable insights that help stakeholders understand, trust, and audit the predictions—especially when such predictions may inform public policy or fiscal planning.</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="fu"># 2. Data Description and Objective</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>The task is to predict 2019 assessed property values using historical records from 2015 to 2019. Each record includes detailed building features, land area and value, protested assessments, and neighborhood-level identifiers.</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>The final 2019 target is defined as:</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>**TARGET = building value 2019 + land value 2019**</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>This is a regression problem with approximately 600,000 training records and 400,000 test records. No external data was permitted.</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="fu"># 3. Feature Engineering</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>Feature engineering plays a critical role in unlocking predictive signals from raw data, particularly in structured datasets involving temporal and categorical variables. Our goal was to construct meaningful, leakage-free features that improve model performance while maintaining generalization to unseen data. Before initiating any feature transformations, we performed a strict comparison of columns between the training and test datasets to ensure no information advantage was present. This safeguards against data leakage and ensures that engineered features reflect patterns learnable at inference time.</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.1 Logic-Driven Missing Value Handling and Imputation</span></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>**Backfilling Year-Based Columns Across Feature Groups:**</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>To handle missing values in temporally structured features (e.g., building area, quality, full bath), we designed a consistent backfilling approach that uses older data to impute more recent years. Specifically, columns were ordered from newest to oldest (i.e., 2019, 2018, ..., 2015), and we applied <span class="in">`bfill(axis=1)`</span> across these columns. This setup causes older values (e.g., from 2015 or 2016) to be used to fill in newer year columns (e.g., 2018 or 2019), effectively implementing a forward fill in temporal logic. This approach assumes that older data reflects the property’s original state more accurately and helps prevent later-year anomalies or missing values from distorting long-term trends.</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>This logic was applied across multiple feature groups:</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Residential count features: floors, full bath, half bath, bedrooms, total rooms</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Area-based features: building area, land area</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Valuation features: building value, land value, assessed</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Categorical building attributes: foundation type, grade, building condition, quality, quality description, physical condition, exterior walls, has cooling, has heat</span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>  echo: true</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>  output: false</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>  collapse: true</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backfill_categorical_year_features(df, features, years):</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feature <span class="kw">in</span> features:</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>        year_cols <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> y <span class="kw">in</span> years <span class="cf">if</span> <span class="ss">f"</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">"</span> <span class="kw">in</span> df.columns]</span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(year_cols) <span class="op">&gt;=</span> <span class="dv">2</span>:</span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a>            df[year_cols] <span class="op">=</span> df[year_cols].bfill(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Backfilled: </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> across </span><span class="sc">{</span>year_cols<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Skipped: </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> — not enough year-based columns."</span>)</span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Backfill from most recent year to oldest</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>years <span class="op">=</span> [<span class="st">'2019'</span>, <span class="st">'2018'</span>, <span class="st">'2017'</span>, <span class="st">'2016'</span>, <span class="st">'2015'</span>]</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'building_condition'</span>, <span class="st">'foundation_type'</span>, <span class="st">'grade'</span>, <span class="st">'has_cooling'</span>, </span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a>            <span class="st">'has_heat'</span>, <span class="st">'physical_condition'</span>, <span class="st">'exterior_walls'</span>]</span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to train and test</span></span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>train_merged <span class="op">=</span> backfill_categorical_year_features(train_merged, features, years)</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>test_merged <span class="op">=</span> backfill_categorical_year_features(test_merged, features, years)</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>**Zero-Aware Property Filtering:**</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a>In cases where <span class="in">`floor_area_total_2019 = 0`</span>, we treated the property as non-residential or commercial and applied domain-specific logic to avoid inappropriate imputations or distortions in the modeling process:</span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>All related residential building features—such as full bath, total rooms, garage area, and porch area—were set to zero.</span>
<span id="cb24-71"><a href="#cb24-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We also zeroed out all building area variables and the corresponding building value variables to reflect the absence of a residential structure.</span>
<span id="cb24-72"><a href="#cb24-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-73"><a href="#cb24-73" aria-hidden="true" tabindex="-1"></a>These records were retained in the dataset rather than dropped, as they likely represent a distinct class of properties where valuation is driven primarily by land characteristics. Explicitly identifying and treating these properties allowed the model to better separate residential and non-residential valuation patterns.</span>
<span id="cb24-76"><a href="#cb24-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-77"><a href="#cb24-77" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-78"><a href="#cb24-78" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-79"><a href="#cb24-79" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-80"><a href="#cb24-80" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 0: Define base feature names ===</span></span>
<span id="cb24-81"><a href="#cb24-81" aria-hidden="true" tabindex="-1"></a>numeric_bases <span class="op">=</span> [</span>
<span id="cb24-82"><a href="#cb24-82" aria-hidden="true" tabindex="-1"></a>    <span class="st">'garage_area'</span>, <span class="st">'porch_area'</span>, <span class="st">'floors'</span>, <span class="st">'half_bath'</span>, <span class="st">'full_bath'</span>,</span>
<span id="cb24-83"><a href="#cb24-83" aria-hidden="true" tabindex="-1"></a>    <span class="st">'total_rooms'</span>, <span class="st">'bedrooms'</span>, <span class="st">'fireplaces'</span>, <span class="st">'building_area'</span>, <span class="st">'building_value'</span></span>
<span id="cb24-84"><a href="#cb24-84" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-85"><a href="#cb24-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-86"><a href="#cb24-86" aria-hidden="true" tabindex="-1"></a>categorical_fill_map <span class="op">=</span> {</span>
<span id="cb24-87"><a href="#cb24-87" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-88"><a href="#cb24-88" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality_description'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-89"><a href="#cb24-89" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-90"><a href="#cb24-90" aria-hidden="true" tabindex="-1"></a>    <span class="st">'foundation_type'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-91"><a href="#cb24-91" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grade'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-92"><a href="#cb24-92" aria-hidden="true" tabindex="-1"></a>    <span class="st">'has_cooling'</span>: <span class="va">False</span>,</span>
<span id="cb24-93"><a href="#cb24-93" aria-hidden="true" tabindex="-1"></a>    <span class="st">'has_heat'</span>: <span class="va">False</span>,</span>
<span id="cb24-94"><a href="#cb24-94" aria-hidden="true" tabindex="-1"></a>    <span class="st">'physical_condition'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-95"><a href="#cb24-95" aria-hidden="true" tabindex="-1"></a>    <span class="st">'exterior_walls'</span>: <span class="st">'None'</span>,</span>
<span id="cb24-96"><a href="#cb24-96" aria-hidden="true" tabindex="-1"></a>    <span class="st">'protested'</span>: <span class="va">False</span></span>
<span id="cb24-97"><a href="#cb24-97" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-98"><a href="#cb24-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-99"><a href="#cb24-99" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate full list of columns (2015–2019 only, no final columns)</span></span>
<span id="cb24-100"><a href="#cb24-100" aria-hidden="true" tabindex="-1"></a>numeric_cols_to_zero <span class="op">=</span> [</span>
<span id="cb24-101"><a href="#cb24-101" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> base <span class="kw">in</span> numeric_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)</span>
<span id="cb24-102"><a href="#cb24-102" aria-hidden="true" tabindex="-1"></a>] <span class="op">+</span> [<span class="st">'building_value_growth'</span>]</span>
<span id="cb24-103"><a href="#cb24-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-104"><a href="#cb24-104" aria-hidden="true" tabindex="-1"></a>categorical_cols_to_fill <span class="op">=</span> {</span>
<span id="cb24-105"><a href="#cb24-105" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f'</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">'</span>: val</span>
<span id="cb24-106"><a href="#cb24-106" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> base, val <span class="kw">in</span> categorical_fill_map.items()</span>
<span id="cb24-107"><a href="#cb24-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)</span>
<span id="cb24-108"><a href="#cb24-108" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-109"><a href="#cb24-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-110"><a href="#cb24-110" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Apply imputation if floor_area_total_2019 == 0 ===</span></span>
<span id="cb24-111"><a href="#cb24-111" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df_name, df <span class="kw">in</span> [(<span class="st">'train_merged'</span>, train_merged), (<span class="st">'test_merged'</span>, test_merged)]:</span>
<span id="cb24-112"><a href="#cb24-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'floor_area_total_2019'</span> <span class="kw">in</span> df.columns:</span>
<span id="cb24-113"><a href="#cb24-113" aria-hidden="true" tabindex="-1"></a>        zero_floor_mask <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb24-114"><a href="#cb24-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-115"><a href="#cb24-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill numeric columns with 0</span></span>
<span id="cb24-116"><a href="#cb24-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> col <span class="kw">in</span> numeric_cols_to_zero:</span>
<span id="cb24-117"><a href="#cb24-117" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb24-118"><a href="#cb24-118" aria-hidden="true" tabindex="-1"></a>                df.loc[zero_floor_mask, col] <span class="op">=</span> df.loc[zero_floor_mask, col].fillna(<span class="dv">0</span>)</span>
<span id="cb24-119"><a href="#cb24-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-120"><a href="#cb24-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fill categorical/boolean columns</span></span>
<span id="cb24-121"><a href="#cb24-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> col, fill_val <span class="kw">in</span> categorical_cols_to_fill.items():</span>
<span id="cb24-122"><a href="#cb24-122" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb24-123"><a href="#cb24-123" aria-hidden="true" tabindex="-1"></a>                df.loc[zero_floor_mask, col] <span class="op">=</span> df.loc[zero_floor_mask, col].fillna(fill_val)</span>
<span id="cb24-124"><a href="#cb24-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-125"><a href="#cb24-125" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Filled structure-dependent missing values in </span><span class="sc">{</span>df_name<span class="sc">}</span><span class="ss"> for </span><span class="sc">{</span>zero_floor_mask<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> rows"</span>)</span>
<span id="cb24-126"><a href="#cb24-126" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-127"><a href="#cb24-127" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" 'floor_area_total_2019' not found in </span><span class="sc">{</span>df_name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-128"><a href="#cb24-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-129"><a href="#cb24-129" aria-hidden="true" tabindex="-1"></a>**Feature Drop Based on Sparse Signals:**</span>
<span id="cb24-130"><a href="#cb24-130" aria-hidden="true" tabindex="-1"></a>For features that appeared largely irrelevant or unused across the dataset, we calculated the percentage of zero values in columns such as mobile home area, deck area, and porch area. If a column contained over 90% zeros, it was considered non-informative and dropped from the modeling pipeline to reduce dimensionality and noise.</span>
<span id="cb24-133"><a href="#cb24-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-134"><a href="#cb24-134" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-135"><a href="#cb24-135" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-136"><a href="#cb24-136" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-137"><a href="#cb24-137" aria-hidden="true" tabindex="-1"></a>cols_to_drop <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> train_merged.columns <span class="cf">if</span> col.startswith(<span class="st">"mobile_home_area"</span>)]</span>
<span id="cb24-138"><a href="#cb24-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-139"><a href="#cb24-139" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop from both sets</span></span>
<span id="cb24-140"><a href="#cb24-140" aria-hidden="true" tabindex="-1"></a>train_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-141"><a href="#cb24-141" aria-hidden="true" tabindex="-1"></a>test_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-142"><a href="#cb24-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-143"><a href="#cb24-143" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Dropped columns from train/test: </span><span class="sc">{</span>cols_to_drop<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-144"><a href="#cb24-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-145"><a href="#cb24-145" aria-hidden="true" tabindex="-1"></a>**Multi-Level Median and Mode Imputation:**</span>
<span id="cb24-146"><a href="#cb24-146" aria-hidden="true" tabindex="-1"></a>After applying logic-based pruning, we used a three-level median imputation strategy for continuous features (e.g., assessed value 2017, building area 2017) based on the following hierarchy:</span>
<span id="cb24-147"><a href="#cb24-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Level 1: neighborhood-level median</span>
<span id="cb24-148"><a href="#cb24-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Level 2: region-level median</span>
<span id="cb24-149"><a href="#cb24-149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Level 3: global median (fallback)</span>
<span id="cb24-150"><a href="#cb24-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-153"><a href="#cb24-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-154"><a href="#cb24-154" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-155"><a href="#cb24-155" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-156"><a href="#cb24-156" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-157"><a href="#cb24-157" aria-hidden="true" tabindex="-1"></a><span class="co"># List of all assessed columns to impute</span></span>
<span id="cb24-158"><a href="#cb24-158" aria-hidden="true" tabindex="-1"></a>assessed_cols <span class="op">=</span> [<span class="st">'assessed_2015'</span>, <span class="st">'assessed_2016'</span>, <span class="st">'assessed_2017'</span>, <span class="st">'assessed_2018'</span>]</span>
<span id="cb24-159"><a href="#cb24-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-160"><a href="#cb24-160" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> assessed_cols:</span>
<span id="cb24-161"><a href="#cb24-161" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">not</span> <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-162"><a href="#cb24-162" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb24-163"><a href="#cb24-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-164"><a href="#cb24-164" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Compute medians from training data only</span></span>
<span id="cb24-165"><a href="#cb24-165" aria-hidden="true" tabindex="-1"></a>    neigh_medians <span class="op">=</span> train_merged.groupby(<span class="st">'neighborhood'</span>)[col].median()</span>
<span id="cb24-166"><a href="#cb24-166" aria-hidden="true" tabindex="-1"></a>    region_medians <span class="op">=</span> train_merged.groupby(<span class="st">'region'</span>)[col].median()</span>
<span id="cb24-167"><a href="#cb24-167" aria-hidden="true" tabindex="-1"></a>    global_median <span class="op">=</span> train_merged[col].median()</span>
<span id="cb24-168"><a href="#cb24-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-169"><a href="#cb24-169" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Train set imputation</span></span>
<span id="cb24-170"><a href="#cb24-170" aria-hidden="true" tabindex="-1"></a>    train_merged[col] <span class="op">=</span> train_merged.<span class="bu">apply</span>(</span>
<span id="cb24-171"><a href="#cb24-171" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> row: neigh_medians[row[<span class="st">'neighborhood'</span>]]</span>
<span id="cb24-172"><a href="#cb24-172" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="kw">and</span> row[<span class="st">'neighborhood'</span>] <span class="kw">in</span> neigh_medians <span class="cf">else</span></span>
<span id="cb24-173"><a href="#cb24-173" aria-hidden="true" tabindex="-1"></a>        region_medians[row[<span class="st">'region'</span>]]</span>
<span id="cb24-174"><a href="#cb24-174" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="kw">and</span> row[<span class="st">'region'</span>] <span class="kw">in</span> region_medians <span class="cf">else</span></span>
<span id="cb24-175"><a href="#cb24-175" aria-hidden="true" tabindex="-1"></a>        global_median</span>
<span id="cb24-176"><a href="#cb24-176" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="cf">else</span></span>
<span id="cb24-177"><a href="#cb24-177" aria-hidden="true" tabindex="-1"></a>        row[col],</span>
<span id="cb24-178"><a href="#cb24-178" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb24-179"><a href="#cb24-179" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-180"><a href="#cb24-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-181"><a href="#cb24-181" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Test set imputation (using train medians only)</span></span>
<span id="cb24-182"><a href="#cb24-182" aria-hidden="true" tabindex="-1"></a>    test_merged[col] <span class="op">=</span> test_merged.<span class="bu">apply</span>(</span>
<span id="cb24-183"><a href="#cb24-183" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> row: neigh_medians.get(row[<span class="st">'neighborhood'</span>], np.nan)</span>
<span id="cb24-184"><a href="#cb24-184" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="cf">else</span> row[col],</span>
<span id="cb24-185"><a href="#cb24-185" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb24-186"><a href="#cb24-186" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-187"><a href="#cb24-187" aria-hidden="true" tabindex="-1"></a>    test_merged[col] <span class="op">=</span> test_merged.<span class="bu">apply</span>(</span>
<span id="cb24-188"><a href="#cb24-188" aria-hidden="true" tabindex="-1"></a>        <span class="kw">lambda</span> row: region_medians.get(row[<span class="st">'region'</span>], np.nan)</span>
<span id="cb24-189"><a href="#cb24-189" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pd.isna(row[col]) <span class="cf">else</span> row[col],</span>
<span id="cb24-190"><a href="#cb24-190" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb24-191"><a href="#cb24-191" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-192"><a href="#cb24-192" aria-hidden="true" tabindex="-1"></a>    test_merged[col].fillna(global_median, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-193"><a href="#cb24-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-194"><a href="#cb24-194" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Imputed '</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">' using neighborhood → region → global medians (from training data)"</span>)</span>
<span id="cb24-195"><a href="#cb24-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-196"><a href="#cb24-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-197"><a href="#cb24-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-198"><a href="#cb24-198" aria-hidden="true" tabindex="-1"></a>For categorical variables such as foundation type or building condition, we applied single-level mode imputation using the most frequent category within the training data. While this approach is less localized, it provided a simple and stable method for handling missing values in features with low cardinality.</span>
<span id="cb24-199"><a href="#cb24-199" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.2 Neighborhood and Region-Level Statistical Features</span></span>
<span id="cb24-200"><a href="#cb24-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-201"><a href="#cb24-201" aria-hidden="true" tabindex="-1"></a>To capture localized pricing dynamics and identify anomalies in property assessments, we engineered a suite of statistical features using the 2018 assessed values as a proxy for prior valuation context. We first computed neighborhood-level metrics including the mean, median, standard deviation, and interquartile range (IQR) of assessed 2018, grouped by neighborhood. Similarly, region-level statistics were computed using the region variable.</span>
<span id="cb24-202"><a href="#cb24-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-203"><a href="#cb24-203" aria-hidden="true" tabindex="-1"></a>Using the merged and imputed stats, we computed derived features such as:</span>
<span id="cb24-204"><a href="#cb24-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**assess minus neigh mean**: the raw deviation of a property’s 2018 assessed value from its neighborhood mean</span>
<span id="cb24-205"><a href="#cb24-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**assess ratio neigh mean**: a normalized ratio of a property’s value to its local average</span>
<span id="cb24-206"><a href="#cb24-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**z score assess neigh**: a z-score based on neighborhood-level variation</span>
<span id="cb24-207"><a href="#cb24-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Corresponding region-level counterparts**: assess minus region mean, assess ratio region mean, and z score assess region</span>
<span id="cb24-208"><a href="#cb24-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-209"><a href="#cb24-209" aria-hidden="true" tabindex="-1"></a>These features helped contextualize each property’s assessed value relative to other properties within the same neighborhood or region. They proved useful in capturing outliers and potentially undervalued homes that deviated from local valuation patterns.</span>
<span id="cb24-212"><a href="#cb24-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-213"><a href="#cb24-213" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-214"><a href="#cb24-214" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-215"><a href="#cb24-215" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-216"><a href="#cb24-216" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Compute neighborhood-level stats ===</span></span>
<span id="cb24-217"><a href="#cb24-217" aria-hidden="true" tabindex="-1"></a>neigh_stats <span class="op">=</span> train_merged.groupby(<span class="st">'neighborhood'</span>)[<span class="st">'assessed_2018'</span>].agg([</span>
<span id="cb24-218"><a href="#cb24-218" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_mean'</span>, <span class="st">'mean'</span>),</span>
<span id="cb24-219"><a href="#cb24-219" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_median'</span>, <span class="st">'median'</span>),</span>
<span id="cb24-220"><a href="#cb24-220" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_std'</span>, <span class="st">'std'</span>),</span>
<span id="cb24-221"><a href="#cb24-221" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_q1'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.25</span>)),</span>
<span id="cb24-222"><a href="#cb24-222" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'neigh_assess_q3'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.75</span>)),</span>
<span id="cb24-223"><a href="#cb24-223" aria-hidden="true" tabindex="-1"></a>]).reset_index()</span>
<span id="cb24-224"><a href="#cb24-224" aria-hidden="true" tabindex="-1"></a>neigh_stats[<span class="st">'neigh_assess_iqr'</span>] <span class="op">=</span> neigh_stats[<span class="st">'neigh_assess_q3'</span>] <span class="op">-</span> neigh_stats[<span class="st">'neigh_assess_q1'</span>]</span>
<span id="cb24-225"><a href="#cb24-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-226"><a href="#cb24-226" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Compute region-level stats ===</span></span>
<span id="cb24-227"><a href="#cb24-227" aria-hidden="true" tabindex="-1"></a>region_stats <span class="op">=</span> train_merged.groupby(<span class="st">'region'</span>)[<span class="st">'assessed_2018'</span>].agg([</span>
<span id="cb24-228"><a href="#cb24-228" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_mean'</span>, <span class="st">'mean'</span>),</span>
<span id="cb24-229"><a href="#cb24-229" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_median'</span>, <span class="st">'median'</span>),</span>
<span id="cb24-230"><a href="#cb24-230" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_std'</span>, <span class="st">'std'</span>),</span>
<span id="cb24-231"><a href="#cb24-231" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_q1'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.25</span>)),</span>
<span id="cb24-232"><a href="#cb24-232" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'region_assess_q3'</span>, <span class="kw">lambda</span> x: x.quantile(<span class="fl">0.75</span>)),</span>
<span id="cb24-233"><a href="#cb24-233" aria-hidden="true" tabindex="-1"></a>]).reset_index()</span>
<span id="cb24-234"><a href="#cb24-234" aria-hidden="true" tabindex="-1"></a>region_stats[<span class="st">'region_assess_iqr'</span>] <span class="op">=</span> region_stats[<span class="st">'region_assess_q3'</span>] <span class="op">-</span> region_stats[<span class="st">'region_assess_q1'</span>]</span>
<span id="cb24-235"><a href="#cb24-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-236"><a href="#cb24-236" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Fallback std maps from training data ===</span></span>
<span id="cb24-237"><a href="#cb24-237" aria-hidden="true" tabindex="-1"></a><span class="co"># For neighborhood fallback, group region medians of neighborhood std</span></span>
<span id="cb24-238"><a href="#cb24-238" aria-hidden="true" tabindex="-1"></a>neigh_std_by_region <span class="op">=</span> neigh_stats.merge(train_merged[[<span class="st">'neighborhood'</span>, <span class="st">'region'</span>]], on<span class="op">=</span><span class="st">'neighborhood'</span>, how<span class="op">=</span><span class="st">'left'</span>) <span class="op">\</span></span>
<span id="cb24-239"><a href="#cb24-239" aria-hidden="true" tabindex="-1"></a>                                  .groupby(<span class="st">'region'</span>)[<span class="st">'neigh_assess_std'</span>].median()</span>
<span id="cb24-240"><a href="#cb24-240" aria-hidden="true" tabindex="-1"></a>global_neigh_std <span class="op">=</span> neigh_stats[<span class="st">'neigh_assess_std'</span>].median()</span>
<span id="cb24-241"><a href="#cb24-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-242"><a href="#cb24-242" aria-hidden="true" tabindex="-1"></a>region_std_by_neigh <span class="op">=</span> region_stats.merge(train_merged[[<span class="st">'neighborhood'</span>, <span class="st">'region'</span>]], on<span class="op">=</span><span class="st">'region'</span>, how<span class="op">=</span><span class="st">'left'</span>) <span class="op">\</span></span>
<span id="cb24-243"><a href="#cb24-243" aria-hidden="true" tabindex="-1"></a>                                   .groupby(<span class="st">'neighborhood'</span>)[<span class="st">'region_assess_std'</span>].median()</span>
<span id="cb24-244"><a href="#cb24-244" aria-hidden="true" tabindex="-1"></a>global_region_std <span class="op">=</span> region_stats[<span class="st">'region_assess_std'</span>].median()</span>
<span id="cb24-245"><a href="#cb24-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-246"><a href="#cb24-246" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 4: Merge into train/test and compute features ===</span></span>
<span id="cb24-247"><a href="#cb24-247" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df_name, df <span class="kw">in</span> [(<span class="st">'train_merged'</span>, train_merged), (<span class="st">'test_merged'</span>, test_merged)]:</span>
<span id="cb24-248"><a href="#cb24-248" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.merge(neigh_stats, on<span class="op">=</span><span class="st">'neighborhood'</span>, how<span class="op">=</span><span class="st">'left'</span>)</span>
<span id="cb24-249"><a href="#cb24-249" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.merge(region_stats, on<span class="op">=</span><span class="st">'region'</span>, how<span class="op">=</span><span class="st">'left'</span>)</span>
<span id="cb24-250"><a href="#cb24-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-251"><a href="#cb24-251" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fill missing std values via fallback</span></span>
<span id="cb24-252"><a href="#cb24-252" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'neigh_assess_std'</span>] <span class="op">=</span> df[<span class="st">'neigh_assess_std'</span>].fillna(</span>
<span id="cb24-253"><a href="#cb24-253" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">'region'</span>].<span class="bu">map</span>(neigh_std_by_region)</span>
<span id="cb24-254"><a href="#cb24-254" aria-hidden="true" tabindex="-1"></a>    ).fillna(global_neigh_std)</span>
<span id="cb24-255"><a href="#cb24-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-256"><a href="#cb24-256" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'region_assess_std'</span>] <span class="op">=</span> df[<span class="st">'region_assess_std'</span>].fillna(</span>
<span id="cb24-257"><a href="#cb24-257" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">'neighborhood'</span>].<span class="bu">map</span>(region_std_by_neigh)</span>
<span id="cb24-258"><a href="#cb24-258" aria-hidden="true" tabindex="-1"></a>    ).fillna(global_region_std)</span>
<span id="cb24-259"><a href="#cb24-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-260"><a href="#cb24-260" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute derived features</span></span>
<span id="cb24-261"><a href="#cb24-261" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_minus_neigh_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">-</span> df[<span class="st">'neigh_assess_mean'</span>]</span>
<span id="cb24-262"><a href="#cb24-262" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_ratio_neigh_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_mean'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb24-263"><a href="#cb24-263" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'z_score_assess_neigh'</span>] <span class="op">=</span> df[<span class="st">'assess_minus_neigh_mean'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_std'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb24-264"><a href="#cb24-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-265"><a href="#cb24-265" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_minus_region_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">-</span> df[<span class="st">'region_assess_mean'</span>]</span>
<span id="cb24-266"><a href="#cb24-266" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_ratio_region_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'region_assess_mean'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb24-267"><a href="#cb24-267" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'z_score_assess_region'</span>] <span class="op">=</span> df[<span class="st">'assess_minus_region_mean'</span>] <span class="op">/</span> (df[<span class="st">'region_assess_std'</span>] <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb24-268"><a href="#cb24-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-269"><a href="#cb24-269" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save back</span></span>
<span id="cb24-270"><a href="#cb24-270" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> df_name <span class="op">==</span> <span class="st">'train_merged'</span>:</span>
<span id="cb24-271"><a href="#cb24-271" aria-hidden="true" tabindex="-1"></a>        train_merged <span class="op">=</span> df</span>
<span id="cb24-272"><a href="#cb24-272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-273"><a href="#cb24-273" aria-hidden="true" tabindex="-1"></a>        test_merged <span class="op">=</span> df</span>
<span id="cb24-274"><a href="#cb24-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-275"><a href="#cb24-275" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Completed: Stats merge + std fallback + z-score computation."</span>)</span>
<span id="cb24-276"><a href="#cb24-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-277"><a href="#cb24-277" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-278"><a href="#cb24-278" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.3 Frequency Encoding of High-Cardinality Geographic Variables</span></span>
<span id="cb24-279"><a href="#cb24-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-280"><a href="#cb24-280" aria-hidden="true" tabindex="-1"></a>To convert high-cardinality categorical variables into numerical features while preserving signal strength, we applied frequency encoding to four key geographic identifiers: neighborhood, region, zone, and subneighborhood.</span>
<span id="cb24-281"><a href="#cb24-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-282"><a href="#cb24-282" aria-hidden="true" tabindex="-1"></a>This encoding strategy served two purposes:</span>
<span id="cb24-283"><a href="#cb24-283" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>It allowed the model to retain information about how common or rare a spatial unit was. A frequently occurring neighborhood (i.e., one with high frequency) likely has more properties, which implies greater residential or commercial development in that area.</span>
<span id="cb24-284"><a href="#cb24-284" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Areas with more properties are also likely to have more consistent and well-understood assessment patterns—the government has “seen” more properties there, which may reduce valuation volatility. These areas are more visible or prioritized in municipal processes.</span>
<span id="cb24-285"><a href="#cb24-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-286"><a href="#cb24-286" aria-hidden="true" tabindex="-1"></a>Finally, frequency encoding avoids the dimensional explosion caused by one-hot encoding, which is especially problematic for variables with high cardinality like neighborhood or subneighborhood.</span>
<span id="cb24-289"><a href="#cb24-289" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-290"><a href="#cb24-290" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-291"><a href="#cb24-291" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-292"><a href="#cb24-292" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-293"><a href="#cb24-293" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> [<span class="st">'neighborhood'</span>, <span class="st">'region'</span>,<span class="st">'zone'</span>,<span class="st">'subneighborhood'</span>]:</span>
<span id="cb24-294"><a href="#cb24-294" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-295"><a href="#cb24-295" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Compute frequency from training data</span></span>
<span id="cb24-296"><a href="#cb24-296" aria-hidden="true" tabindex="-1"></a>        freq_map <span class="op">=</span> train_merged[col].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-297"><a href="#cb24-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-298"><a href="#cb24-298" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Apply to both datasets</span></span>
<span id="cb24-299"><a href="#cb24-299" aria-hidden="true" tabindex="-1"></a>        train_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_freq'</span>] <span class="op">=</span> train_merged[col].<span class="bu">map</span>(freq_map)</span>
<span id="cb24-300"><a href="#cb24-300" aria-hidden="true" tabindex="-1"></a>        test_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_freq'</span>] <span class="op">=</span> test_merged[col].<span class="bu">map</span>(freq_map)</span>
<span id="cb24-301"><a href="#cb24-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-302"><a href="#cb24-302" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Frequency encoded: </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> → </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_freq (based on training set)"</span>)</span>
<span id="cb24-303"><a href="#cb24-303" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-304"><a href="#cb24-304" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Column '</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">' not found in training set"</span>)</span>
<span id="cb24-305"><a href="#cb24-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-306"><a href="#cb24-306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-307"><a href="#cb24-307" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.4 Boolean and Ordinal Encoding</span></span>
<span id="cb24-308"><a href="#cb24-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-309"><a href="#cb24-309" aria-hidden="true" tabindex="-1"></a>**Boolean Encoding:** We focused on three boolean variables: has cooling, has heat, and protested. These features were encoded across all five years using binary values (0/1).</span>
<span id="cb24-310"><a href="#cb24-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-311"><a href="#cb24-311" aria-hidden="true" tabindex="-1"></a>**Ordinal Encoding:** For ordinal features such as quality, quality description, grade, building condition, and physical condition, we performed domain-informed cleaning and then applied ordinal encoding based on defined category hierarchies. Prior to encoding, raw values were standardized through column-specific replacements. For instance, extreme or ambiguous values like X, None, or overly granular subgrades (e.g., X-, E+) were either mapped to more interpretable categories or treated as missing. Some detailed conditions like Unsound and Very Poor were collapsed into broader categories such as Poor. Unknown values were handled gracefully by assigning an encoded fallback of -1. This process ensured that ordinal information was preserved in a numerically meaningful way, allowing models to leverage the ordered nature of these features without exploding dimensionality as one-hot encoding would.</span>
<span id="cb24-312"><a href="#cb24-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-315"><a href="#cb24-315" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-316"><a href="#cb24-316" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-317"><a href="#cb24-317" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-318"><a href="#cb24-318" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-319"><a href="#cb24-319" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-320"><a href="#cb24-320" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-321"><a href="#cb24-321" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OrdinalEncoder</span>
<span id="cb24-322"><a href="#cb24-322" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb24-323"><a href="#cb24-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-324"><a href="#cb24-324" aria-hidden="true" tabindex="-1"></a><span class="co"># Clear specific variables</span></span>
<span id="cb24-325"><a href="#cb24-325" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> var <span class="kw">in</span> [<span class="st">'ordinal_cols_all'</span>, <span class="st">'bool_cols_all'</span>]:</span>
<span id="cb24-326"><a href="#cb24-326" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> var <span class="kw">in</span> <span class="bu">locals</span>():</span>
<span id="cb24-327"><a href="#cb24-327" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> <span class="bu">globals</span>()[var]</span>
<span id="cb24-328"><a href="#cb24-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-329"><a href="#cb24-329" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Boolean Encoding (2015–2019 only) ===</span></span>
<span id="cb24-330"><a href="#cb24-330" aria-hidden="true" tabindex="-1"></a>bool_bases <span class="op">=</span> [<span class="st">'has_cooling'</span>, <span class="st">'has_heat'</span>, <span class="st">'protested'</span>]</span>
<span id="cb24-331"><a href="#cb24-331" aria-hidden="true" tabindex="-1"></a>bool_cols_all <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> bool_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb24-332"><a href="#cb24-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-333"><a href="#cb24-333" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> bool_cols_all:</span>
<span id="cb24-334"><a href="#cb24-334" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-335"><a href="#cb24-335" aria-hidden="true" tabindex="-1"></a>        mode_val <span class="op">=</span> train_merged[col].mode(dropna<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb24-336"><a href="#cb24-336" aria-hidden="true" tabindex="-1"></a>        train_merged[col] <span class="op">=</span> train_merged[col].fillna(mode_val).astype(<span class="bu">int</span>)</span>
<span id="cb24-337"><a href="#cb24-337" aria-hidden="true" tabindex="-1"></a>        test_merged[col] <span class="op">=</span> test_merged[col].fillna(mode_val).astype(<span class="bu">int</span>)</span>
<span id="cb24-338"><a href="#cb24-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-339"><a href="#cb24-339" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Ordinal Cleaning and Encoding (2015–2019 only) ===</span></span>
<span id="cb24-340"><a href="#cb24-340" aria-hidden="true" tabindex="-1"></a>ordinal_bases <span class="op">=</span> [</span>
<span id="cb24-341"><a href="#cb24-341" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>, <span class="st">'quality_description'</span>, <span class="st">'grade'</span>,</span>
<span id="cb24-342"><a href="#cb24-342" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>, <span class="st">'physical_condition'</span></span>
<span id="cb24-343"><a href="#cb24-343" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-344"><a href="#cb24-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-345"><a href="#cb24-345" aria-hidden="true" tabindex="-1"></a>ordinal_cols_all <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> ordinal_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb24-346"><a href="#cb24-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-347"><a href="#cb24-347" aria-hidden="true" tabindex="-1"></a><span class="co"># Column-specific replacements</span></span>
<span id="cb24-348"><a href="#cb24-348" aria-hidden="true" tabindex="-1"></a>replacement_maps <span class="op">=</span> {</span>
<span id="cb24-349"><a href="#cb24-349" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>: {<span class="st">'E'</span>: <span class="st">'D'</span>, <span class="st">'F'</span>: <span class="st">'D'</span>, <span class="st">'X'</span>: np.nan, <span class="st">'None'</span>: np.nan},</span>
<span id="cb24-350"><a href="#cb24-350" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality_description'</span>: {<span class="st">'Poor'</span>: <span class="st">'Very Low'</span>, <span class="st">'None'</span>: np.nan},</span>
<span id="cb24-351"><a href="#cb24-351" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grade'</span>: {<span class="st">'X'</span>: <span class="st">'F'</span>, <span class="st">'X-'</span>: <span class="st">'F'</span>, <span class="st">'X+'</span>: <span class="st">'F'</span>, <span class="st">'E'</span>: <span class="st">'D'</span>, <span class="st">'E-'</span>: <span class="st">'D-'</span>, <span class="st">'E+'</span>: <span class="st">'D+'</span>, <span class="st">'None'</span>: np.nan},</span>
<span id="cb24-352"><a href="#cb24-352" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>: {<span class="st">'Very Poor'</span>: <span class="st">'Poor'</span>, <span class="st">'Unsound'</span>: <span class="st">'Poor'</span>, <span class="st">'None'</span>: np.nan},</span>
<span id="cb24-353"><a href="#cb24-353" aria-hidden="true" tabindex="-1"></a>    <span class="st">'physical_condition'</span>: {<span class="st">'Very Poor'</span>: <span class="st">'Poor'</span>, <span class="st">'Unsound'</span>: <span class="st">'Poor'</span>, <span class="st">'None'</span>: np.nan}</span>
<span id="cb24-354"><a href="#cb24-354" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-355"><a href="#cb24-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-356"><a href="#cb24-356" aria-hidden="true" tabindex="-1"></a><span class="co"># Ordinal category order</span></span>
<span id="cb24-357"><a href="#cb24-357" aria-hidden="true" tabindex="-1"></a>ord_categories <span class="op">=</span> {</span>
<span id="cb24-358"><a href="#cb24-358" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality'</span>: [<span class="st">'D'</span>, <span class="st">'C'</span>, <span class="st">'B'</span>, <span class="st">'A'</span>],</span>
<span id="cb24-359"><a href="#cb24-359" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quality_description'</span>: [<span class="st">'Very Low'</span>, <span class="st">'Low'</span>, <span class="st">'Average'</span>, <span class="st">'Good'</span>, <span class="st">'Excellent'</span>, <span class="st">'Superior'</span>],</span>
<span id="cb24-360"><a href="#cb24-360" aria-hidden="true" tabindex="-1"></a>    <span class="st">'grade'</span>: [<span class="st">'F'</span>, <span class="st">'D-'</span>, <span class="st">'D'</span>, <span class="st">'D+'</span>, <span class="st">'C-'</span>, <span class="st">'C'</span>, <span class="st">'C+'</span>, <span class="st">'B-'</span>, <span class="st">'B'</span>, <span class="st">'B+'</span>, <span class="st">'A-'</span>, <span class="st">'A'</span>, <span class="st">'A+'</span>],</span>
<span id="cb24-361"><a href="#cb24-361" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_condition'</span>: [<span class="st">'Poor'</span>, <span class="st">'Fair'</span>, <span class="st">'Average'</span>, <span class="st">'Good'</span>, <span class="st">'Very Good'</span>, <span class="st">'Excellent'</span>],</span>
<span id="cb24-362"><a href="#cb24-362" aria-hidden="true" tabindex="-1"></a>    <span class="st">'physical_condition'</span>: [<span class="st">'Poor'</span>, <span class="st">'Fair'</span>, <span class="st">'Average'</span>, <span class="st">'Good'</span>, <span class="st">'Very Good'</span>, <span class="st">'Excellent'</span>]</span>
<span id="cb24-363"><a href="#cb24-363" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-364"><a href="#cb24-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-365"><a href="#cb24-365" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean and encode</span></span>
<span id="cb24-366"><a href="#cb24-366" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> base <span class="kw">in</span> ordinal_bases:</span>
<span id="cb24-367"><a href="#cb24-367" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>):</span>
<span id="cb24-368"><a href="#cb24-368" aria-hidden="true" tabindex="-1"></a>        col <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb24-369"><a href="#cb24-369" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-370"><a href="#cb24-370" aria-hidden="true" tabindex="-1"></a>            replacements <span class="op">=</span> replacement_maps.get(base, {})</span>
<span id="cb24-371"><a href="#cb24-371" aria-hidden="true" tabindex="-1"></a>            train_merged[col] <span class="op">=</span> train_merged[col].replace(replacements)</span>
<span id="cb24-372"><a href="#cb24-372" aria-hidden="true" tabindex="-1"></a>            test_merged[col] <span class="op">=</span> test_merged[col].replace(replacements)</span>
<span id="cb24-373"><a href="#cb24-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-374"><a href="#cb24-374" aria-hidden="true" tabindex="-1"></a>            mode_val <span class="op">=</span> train_merged[col].mode(dropna<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb24-375"><a href="#cb24-375" aria-hidden="true" tabindex="-1"></a>            train_merged[col] <span class="op">=</span> train_merged[col].fillna(mode_val)</span>
<span id="cb24-376"><a href="#cb24-376" aria-hidden="true" tabindex="-1"></a>            test_merged[col] <span class="op">=</span> test_merged[col].fillna(mode_val)</span>
<span id="cb24-377"><a href="#cb24-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-378"><a href="#cb24-378" aria-hidden="true" tabindex="-1"></a>            encoder <span class="op">=</span> OrdinalEncoder(categories<span class="op">=</span>[ord_categories[base]], handle_unknown<span class="op">=</span><span class="st">'use_encoded_value'</span>, unknown_value<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb24-379"><a href="#cb24-379" aria-hidden="true" tabindex="-1"></a>            train_merged[[col]] <span class="op">=</span> encoder.fit_transform(train_merged[[col]])</span>
<span id="cb24-380"><a href="#cb24-380" aria-hidden="true" tabindex="-1"></a>            test_merged[[col]] <span class="op">=</span> encoder.transform(test_merged[[col]])</span>
<span id="cb24-381"><a href="#cb24-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-382"><a href="#cb24-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-383"><a href="#cb24-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-384"><a href="#cb24-384" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.5 Target Encoding of Nominal Categorical Variables (2015–2019)</span></span>
<span id="cb24-385"><a href="#cb24-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-386"><a href="#cb24-386" aria-hidden="true" tabindex="-1"></a>For certain nominal features that lacked ordinal structure but exhibited high cardinality, such as foundation type and exterior walls, we applied target encoding across all years from 2015 to 2019. This encoding replaces each category with a smoothed version of the mean target value (assessed 2018) observed for that category.</span>
<span id="cb24-387"><a href="#cb24-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-388"><a href="#cb24-388" aria-hidden="true" tabindex="-1"></a>To avoid overfitting and data leakage, we implemented a 5-fold cross-validated target encoding procedure. For each fold, the mean target value was computed from the training portion and mapped to the validation fold. We used a smoothing parameter of 10 to balance the influence of the global mean versus the category-specific mean, especially for infrequent categories.</span>
<span id="cb24-389"><a href="#cb24-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-390"><a href="#cb24-390" aria-hidden="true" tabindex="-1"></a>This method enabled us to capture predictive signal from nominal features without creating high-dimensional one-hot encodings or imposing artificial ordinal structure.</span>
<span id="cb24-393"><a href="#cb24-393" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-394"><a href="#cb24-394" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-395"><a href="#cb24-395" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-396"><a href="#cb24-396" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-397"><a href="#cb24-397" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: Target Encoding (2015–2019 only) ===</span></span>
<span id="cb24-398"><a href="#cb24-398" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> group_and_target_encode_cv(train_df, test_df, target_name, column, rare_threshold<span class="op">=</span><span class="fl">0.001</span>, smoothing<span class="op">=</span><span class="dv">10</span>, n_splits<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb24-399"><a href="#cb24-399" aria-hidden="true" tabindex="-1"></a>    freq <span class="op">=</span> train_df[column].value_counts(normalize<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-400"><a href="#cb24-400" aria-hidden="true" tabindex="-1"></a>    rare_cats <span class="op">=</span> freq[freq <span class="op">&lt;</span> rare_threshold].index</span>
<span id="cb24-401"><a href="#cb24-401" aria-hidden="true" tabindex="-1"></a>    train_df[column] <span class="op">=</span> train_df[column].replace(rare_cats, <span class="st">'Other'</span>)</span>
<span id="cb24-402"><a href="#cb24-402" aria-hidden="true" tabindex="-1"></a>    test_df[column] <span class="op">=</span> test_df[column].replace(rare_cats, <span class="st">'Other'</span>)</span>
<span id="cb24-403"><a href="#cb24-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-404"><a href="#cb24-404" aria-hidden="true" tabindex="-1"></a>    global_mean <span class="op">=</span> train_df[target_name].mean()</span>
<span id="cb24-405"><a href="#cb24-405" aria-hidden="true" tabindex="-1"></a>    oof_encoded <span class="op">=</span> pd.Series(index<span class="op">=</span>train_df.index, dtype<span class="op">=</span><span class="st">'float64'</span>)</span>
<span id="cb24-406"><a href="#cb24-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-407"><a href="#cb24-407" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>n_splits, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-408"><a href="#cb24-408" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> train_idx, val_idx <span class="kw">in</span> kf.split(train_df):</span>
<span id="cb24-409"><a href="#cb24-409" aria-hidden="true" tabindex="-1"></a>        X_tr, X_val <span class="op">=</span> train_df.iloc[train_idx], train_df.iloc[val_idx]</span>
<span id="cb24-410"><a href="#cb24-410" aria-hidden="true" tabindex="-1"></a>        stats <span class="op">=</span> X_tr.groupby(column)[target_name].agg([<span class="st">'mean'</span>, <span class="st">'count'</span>])</span>
<span id="cb24-411"><a href="#cb24-411" aria-hidden="true" tabindex="-1"></a>        smooth <span class="op">=</span> (stats[<span class="st">'mean'</span>] <span class="op">*</span> stats[<span class="st">'count'</span>] <span class="op">+</span> global_mean <span class="op">*</span> smoothing) <span class="op">/</span> (stats[<span class="st">'count'</span>] <span class="op">+</span> smoothing)</span>
<span id="cb24-412"><a href="#cb24-412" aria-hidden="true" tabindex="-1"></a>        oof_encoded.iloc[val_idx] <span class="op">=</span> X_val[column].<span class="bu">map</span>(smooth).fillna(global_mean)</span>
<span id="cb24-413"><a href="#cb24-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-414"><a href="#cb24-414" aria-hidden="true" tabindex="-1"></a>    final_stats <span class="op">=</span> train_df.groupby(column)[target_name].agg([<span class="st">'mean'</span>, <span class="st">'count'</span>])</span>
<span id="cb24-415"><a href="#cb24-415" aria-hidden="true" tabindex="-1"></a>    final_smooth <span class="op">=</span> (final_stats[<span class="st">'mean'</span>] <span class="op">*</span> final_stats[<span class="st">'count'</span>] <span class="op">+</span> global_mean <span class="op">*</span> smoothing) <span class="op">/</span> (final_stats[<span class="st">'count'</span>] <span class="op">+</span> smoothing)</span>
<span id="cb24-416"><a href="#cb24-416" aria-hidden="true" tabindex="-1"></a>    test_encoded <span class="op">=</span> test_df[column].<span class="bu">map</span>(final_smooth).fillna(global_mean)</span>
<span id="cb24-417"><a href="#cb24-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-418"><a href="#cb24-418" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> oof_encoded, test_encoded</span>
<span id="cb24-419"><a href="#cb24-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-420"><a href="#cb24-420" aria-hidden="true" tabindex="-1"></a><span class="co"># Target-encodable nominal columns</span></span>
<span id="cb24-421"><a href="#cb24-421" aria-hidden="true" tabindex="-1"></a>target_encodable_bases <span class="op">=</span> [<span class="st">'foundation_type'</span>, <span class="st">'exterior_walls'</span>]</span>
<span id="cb24-422"><a href="#cb24-422" aria-hidden="true" tabindex="-1"></a>target_encodable_cols_all <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> target_encodable_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb24-423"><a href="#cb24-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-424"><a href="#cb24-424" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply target encoding</span></span>
<span id="cb24-425"><a href="#cb24-425" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> target_encodable_cols_all:</span>
<span id="cb24-426"><a href="#cb24-426" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-427"><a href="#cb24-427" aria-hidden="true" tabindex="-1"></a>        mode_val <span class="op">=</span> train_merged[col].mode(dropna<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb24-428"><a href="#cb24-428" aria-hidden="true" tabindex="-1"></a>        train_merged[col] <span class="op">=</span> train_merged[col].fillna(mode_val)</span>
<span id="cb24-429"><a href="#cb24-429" aria-hidden="true" tabindex="-1"></a>        test_merged[col] <span class="op">=</span> test_merged[col].fillna(mode_val)</span>
<span id="cb24-430"><a href="#cb24-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-431"><a href="#cb24-431" aria-hidden="true" tabindex="-1"></a>        train_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_te'</span>], test_merged[<span class="ss">f'</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">_te'</span>] <span class="op">=</span> group_and_target_encode_cv(</span>
<span id="cb24-432"><a href="#cb24-432" aria-hidden="true" tabindex="-1"></a>            train_merged, test_merged, target_name<span class="op">=</span><span class="st">'assessed_2018'</span>, column<span class="op">=</span>col,</span>
<span id="cb24-433"><a href="#cb24-433" aria-hidden="true" tabindex="-1"></a>            rare_threshold<span class="op">=</span><span class="fl">0.001</span>, smoothing<span class="op">=</span><span class="dv">10</span>, n_splits<span class="op">=</span><span class="dv">5</span></span>
<span id="cb24-434"><a href="#cb24-434" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-435"><a href="#cb24-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-436"><a href="#cb24-436" aria-hidden="true" tabindex="-1"></a>        train_merged.drop(columns<span class="op">=</span>[col], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-437"><a href="#cb24-437" aria-hidden="true" tabindex="-1"></a>        test_merged.drop(columns<span class="op">=</span>[col], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-438"><a href="#cb24-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-439"><a href="#cb24-439" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-440"><a href="#cb24-440" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.6 Quantile Binning of Features</span></span>
<span id="cb24-441"><a href="#cb24-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-442"><a href="#cb24-442" aria-hidden="true" tabindex="-1"></a>To enhance robustness and reduce sensitivity to outliers, we converted few continuous features into categorical bins using quantile-based binning. Growth metrics such as land value growth, building value growth, and assessed growth were binned into four quantiles, with thresholds computed only on the training data to prevent information leakage. If quantile binning failed due to low cardinality (e.g., repeated values), we defaulted to equal-width binning. All binned variables were explicitly cast as categorical to ensure compatibility with tree-based models.</span>
<span id="cb24-443"><a href="#cb24-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-444"><a href="#cb24-444" aria-hidden="true" tabindex="-1"></a>Additionally, we binned year built final into five quantiles to capture generational differences in construction periods. This replaced raw year values with interpretable ordinal categories. Original continuous features were removed after binning to avoid redundancy and reduce multicollinearity.</span>
<span id="cb24-447"><a href="#cb24-447" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-448"><a href="#cb24-448" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-449"><a href="#cb24-449" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-450"><a href="#cb24-450" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-451"><a href="#cb24-451" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: List your growth features ===</span></span>
<span id="cb24-452"><a href="#cb24-452" aria-hidden="true" tabindex="-1"></a>growth_features <span class="op">=</span> [<span class="st">'land_value_growth'</span>, <span class="st">'building_value_growth'</span>, <span class="st">'assessed_growth'</span>]</span>
<span id="cb24-453"><a href="#cb24-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-454"><a href="#cb24-454" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Binning Function (train-based binning) ===</span></span>
<span id="cb24-455"><a href="#cb24-455" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bin_growth_feature_safe(train_df, test_df, feature, bins<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb24-456"><a href="#cb24-456" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb24-457"><a href="#cb24-457" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quantile binning on train only</span></span>
<span id="cb24-458"><a href="#cb24-458" aria-hidden="true" tabindex="-1"></a>        train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>], bin_edges <span class="op">=</span> pd.qcut(train_df[feature], q<span class="op">=</span>bins, labels<span class="op">=</span><span class="va">False</span>, retbins<span class="op">=</span><span class="va">True</span>, duplicates<span class="op">=</span><span class="st">'drop'</span>)</span>
<span id="cb24-459"><a href="#cb24-459" aria-hidden="true" tabindex="-1"></a>        test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> pd.cut(test_df[feature], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-460"><a href="#cb24-460" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">ValueError</span>:</span>
<span id="cb24-461"><a href="#cb24-461" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fallback: Equal-width binning</span></span>
<span id="cb24-462"><a href="#cb24-462" aria-hidden="true" tabindex="-1"></a>        min_val <span class="op">=</span> train_df[feature].<span class="bu">min</span>()</span>
<span id="cb24-463"><a href="#cb24-463" aria-hidden="true" tabindex="-1"></a>        max_val <span class="op">=</span> train_df[feature].<span class="bu">max</span>()</span>
<span id="cb24-464"><a href="#cb24-464" aria-hidden="true" tabindex="-1"></a>        bin_edges <span class="op">=</span> np.linspace(min_val, max_val, bins <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-465"><a href="#cb24-465" aria-hidden="true" tabindex="-1"></a>        train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> pd.cut(train_df[feature], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-466"><a href="#cb24-466" aria-hidden="true" tabindex="-1"></a>        test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> pd.cut(test_df[feature], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-467"><a href="#cb24-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-468"><a href="#cb24-468" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to category</span></span>
<span id="cb24-469"><a href="#cb24-469" aria-hidden="true" tabindex="-1"></a>    train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> train_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb24-470"><a href="#cb24-470" aria-hidden="true" tabindex="-1"></a>    test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>] <span class="op">=</span> test_df[<span class="ss">f'</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb24-471"><a href="#cb24-471" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_df, test_df</span>
<span id="cb24-472"><a href="#cb24-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-473"><a href="#cb24-473" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Apply to train_merged and test_merged ===</span></span>
<span id="cb24-474"><a href="#cb24-474" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature <span class="kw">in</span> growth_features:</span>
<span id="cb24-475"><a href="#cb24-475" aria-hidden="true" tabindex="-1"></a>    train_merged, test_merged <span class="op">=</span> bin_growth_feature_safe(train_merged, test_merged, feature)</span>
<span id="cb24-476"><a href="#cb24-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-477"><a href="#cb24-477" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 4: Bin year_built_final using train-based quantiles ===</span></span>
<span id="cb24-478"><a href="#cb24-478" aria-hidden="true" tabindex="-1"></a>train_merged[<span class="st">'year_built_bin'</span>], bin_edges <span class="op">=</span> pd.qcut(</span>
<span id="cb24-479"><a href="#cb24-479" aria-hidden="true" tabindex="-1"></a>    train_merged[<span class="st">'year_built_final'</span>], q<span class="op">=</span><span class="dv">5</span>, retbins<span class="op">=</span><span class="va">True</span>, labels<span class="op">=</span><span class="va">False</span>, duplicates<span class="op">=</span><span class="st">'drop'</span></span>
<span id="cb24-480"><a href="#cb24-480" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-481"><a href="#cb24-481" aria-hidden="true" tabindex="-1"></a>test_merged[<span class="st">'year_built_bin'</span>] <span class="op">=</span> pd.cut(</span>
<span id="cb24-482"><a href="#cb24-482" aria-hidden="true" tabindex="-1"></a>    test_merged[<span class="st">'year_built_final'</span>], bins<span class="op">=</span>bin_edges, labels<span class="op">=</span><span class="va">False</span>, include_lowest<span class="op">=</span><span class="va">True</span></span>
<span id="cb24-483"><a href="#cb24-483" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-484"><a href="#cb24-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-485"><a href="#cb24-485" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to category</span></span>
<span id="cb24-486"><a href="#cb24-486" aria-hidden="true" tabindex="-1"></a>train_merged[<span class="st">'year_built_bin'</span>] <span class="op">=</span> train_merged[<span class="st">'year_built_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb24-487"><a href="#cb24-487" aria-hidden="true" tabindex="-1"></a>test_merged[<span class="st">'year_built_bin'</span>] <span class="op">=</span> test_merged[<span class="st">'year_built_bin'</span>].astype(<span class="st">'category'</span>)</span>
<span id="cb24-488"><a href="#cb24-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-489"><a href="#cb24-489" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 5: Drop original continuous columns ===</span></span>
<span id="cb24-490"><a href="#cb24-490" aria-hidden="true" tabindex="-1"></a>cols_to_drop <span class="op">=</span> growth_features <span class="op">+</span> [<span class="st">'year_built_final'</span>]</span>
<span id="cb24-491"><a href="#cb24-491" aria-hidden="true" tabindex="-1"></a>train_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-492"><a href="#cb24-492" aria-hidden="true" tabindex="-1"></a>test_merged.drop(columns<span class="op">=</span>cols_to_drop, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-493"><a href="#cb24-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-494"><a href="#cb24-494" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Binned growth &amp; year_built features safely with no leakage."</span>)</span>
<span id="cb24-495"><a href="#cb24-495" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-496"><a href="#cb24-496" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.7 Rare Frequency Suppression in Spatial Encodings</span></span>
<span id="cb24-497"><a href="#cb24-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-498"><a href="#cb24-498" aria-hidden="true" tabindex="-1"></a>Following frequency encoding of high-cardinality spatial variables (region, neighborhood, zone, subneighborhood), we applied a rare-value suppression step to mitigate the noise introduced by sparsely represented categories. For each frequency-encoded column, we identified values that occurred in less than 0.1% of the training data and replaced them with a neutral value of zero in both the training and test sets. This was done using thresholds derived solely from the training distribution to prevent data leakage.</span>
<span id="cb24-499"><a href="#cb24-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-500"><a href="#cb24-500" aria-hidden="true" tabindex="-1"></a>The intuition behind this strategy is that extremely rare spatial groupings may not provide reliable or generalizable signals to the model. Treating them as a common fallback class (i.e., assigning them a frequency of zero) improves model stability and reduces overfitting to idiosyncratic, low-support locations. This transformation preserves the informativeness of frequent categories while smoothing out sparse tail behavior in the feature space.</span>
<span id="cb24-503"><a href="#cb24-503" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-504"><a href="#cb24-504" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-505"><a href="#cb24-505" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-506"><a href="#cb24-506" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-507"><a href="#cb24-507" aria-hidden="true" tabindex="-1"></a><span class="co"># Define frequency columns and threshold</span></span>
<span id="cb24-508"><a href="#cb24-508" aria-hidden="true" tabindex="-1"></a>freq_cols <span class="op">=</span> [<span class="st">'region_freq'</span>, <span class="st">'neighborhood_freq'</span>, <span class="st">'zone_freq'</span>, <span class="st">'subneighborhood_freq'</span>]</span>
<span id="cb24-509"><a href="#cb24-509" aria-hidden="true" tabindex="-1"></a>rare_thresh <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb24-510"><a href="#cb24-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-511"><a href="#cb24-511" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply rare value replacement for each frequency column</span></span>
<span id="cb24-512"><a href="#cb24-512" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> freq_cols:</span>
<span id="cb24-513"><a href="#cb24-513" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-514"><a href="#cb24-514" aria-hidden="true" tabindex="-1"></a>        rare_vals <span class="op">=</span> train_merged[col].value_counts(normalize<span class="op">=</span><span class="va">True</span>)[<span class="kw">lambda</span> x: x <span class="op">&lt;</span> rare_thresh].index</span>
<span id="cb24-515"><a href="#cb24-515" aria-hidden="true" tabindex="-1"></a>        train_merged[col] <span class="op">=</span> train_merged[col].replace(rare_vals, <span class="dv">0</span>)</span>
<span id="cb24-516"><a href="#cb24-516" aria-hidden="true" tabindex="-1"></a>        test_merged[col] <span class="op">=</span> test_merged[col].replace(rare_vals, <span class="dv">0</span>)</span>
<span id="cb24-517"><a href="#cb24-517" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Replaced rare values in </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> using train_merged threshold &lt; </span><span class="sc">{</span>rare_thresh<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-518"><a href="#cb24-518" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-519"><a href="#cb24-519" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f" Column </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> not found in train_merged — skipping."</span>)</span>
<span id="cb24-520"><a href="#cb24-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-521"><a href="#cb24-521" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-522"><a href="#cb24-522" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.8 Log Transformation and Distribution Smoothing for Ridge Regression</span></span>
<span id="cb24-523"><a href="#cb24-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-524"><a href="#cb24-524" aria-hidden="true" tabindex="-1"></a>To satisfy linear model assumptions and reduce skew-related distortion in Ridge regression, we applied a targeted log transformation to select continuous features. Specifically, we identified variables related to building size, land area, and valuation (e.g., building value 2019, land area 2018, neigh assess mean) whose skewness exceeded a threshold of 2.0 in the training set. For these features, we applied a log1p transformation, which effectively stabilized variance, compressed long-tailed distributions, and improved linear fit potential.</span>
<span id="cb24-525"><a href="#cb24-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-526"><a href="#cb24-526" aria-hidden="true" tabindex="-1"></a>This transformation was particularly useful for the Ridge regression model, which benefits from normally distributed inputs and is sensitive to extreme values. By selectively applying log1p only to features with high skew, we preserved model interpretability while enhancing numerical stability and predictive performance.</span>
<span id="cb24-529"><a href="#cb24-529" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-530"><a href="#cb24-530" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-531"><a href="#cb24-531" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-532"><a href="#cb24-532" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-533"><a href="#cb24-533" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-534"><a href="#cb24-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-535"><a href="#cb24-535" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Skew-based Log Transformation (2015–2019 only) ===</span></span>
<span id="cb24-536"><a href="#cb24-536" aria-hidden="true" tabindex="-1"></a>log_bases <span class="op">=</span> [</span>
<span id="cb24-537"><a href="#cb24-537" aria-hidden="true" tabindex="-1"></a>    <span class="st">'floor_area_total'</span>, <span class="st">'porch_area'</span>, <span class="st">'building_area'</span>, <span class="st">'land_area'</span>,</span>
<span id="cb24-538"><a href="#cb24-538" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_value'</span>, <span class="st">'land_value'</span>, <span class="st">'assessed'</span></span>
<span id="cb24-539"><a href="#cb24-539" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-540"><a href="#cb24-540" aria-hidden="true" tabindex="-1"></a>neigh_stat_cols <span class="op">=</span> [</span>
<span id="cb24-541"><a href="#cb24-541" aria-hidden="true" tabindex="-1"></a>    <span class="st">'neigh_assess_mean'</span>, <span class="st">'neigh_assess_std'</span>, <span class="st">'neigh_assess_median'</span>,</span>
<span id="cb24-542"><a href="#cb24-542" aria-hidden="true" tabindex="-1"></a>    <span class="st">'neigh_assess_q1'</span>, <span class="st">'neigh_assess_q3'</span></span>
<span id="cb24-543"><a href="#cb24-543" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-544"><a href="#cb24-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-545"><a href="#cb24-545" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect log-transformable columns (2015–2019 + neighborhood stats)</span></span>
<span id="cb24-546"><a href="#cb24-546" aria-hidden="true" tabindex="-1"></a>log_transform_cols <span class="op">=</span> [<span class="ss">f"</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss">_</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> base <span class="kw">in</span> log_bases <span class="cf">for</span> year <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2015</span>, <span class="dv">2020</span>)]</span>
<span id="cb24-547"><a href="#cb24-547" aria-hidden="true" tabindex="-1"></a>log_transform_cols <span class="op">+=</span> neigh_stat_cols</span>
<span id="cb24-548"><a href="#cb24-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-549"><a href="#cb24-549" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute skewness on train and apply log1p only if skew &gt; 2</span></span>
<span id="cb24-550"><a href="#cb24-550" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> log_transform_cols:</span>
<span id="cb24-551"><a href="#cb24-551" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> col <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-552"><a href="#cb24-552" aria-hidden="true" tabindex="-1"></a>        skew <span class="op">=</span> train_merged[col].skew()</span>
<span id="cb24-553"><a href="#cb24-553" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> skew <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb24-554"><a href="#cb24-554" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> df <span class="kw">in</span> [train_merged, test_merged]:</span>
<span id="cb24-555"><a href="#cb24-555" aria-hidden="true" tabindex="-1"></a>                df[<span class="ss">f"log_</span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">"</span>] <span class="op">=</span> np.log1p(df[col])</span>
<span id="cb24-556"><a href="#cb24-556" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f" Log-transformed: </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> (skew=</span><span class="sc">{</span>skew<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb24-557"><a href="#cb24-557" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb24-558"><a href="#cb24-558" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"ℹ Skipped: </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss"> (skew=</span><span class="sc">{</span>skew<span class="sc">:.2f}</span><span class="ss">)"</span>)</span>
<span id="cb24-559"><a href="#cb24-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-560"><a href="#cb24-560" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-561"><a href="#cb24-561" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.9 Adaptive Quantile Clipping for Tree-Based Models</span></span>
<span id="cb24-562"><a href="#cb24-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-563"><a href="#cb24-563" aria-hidden="true" tabindex="-1"></a>To further control the influence of extreme values in tree-based models, we implemented an adaptive quantile clipping strategy informed by skewness severity. Using a precomputed skewness report, we categorized numeric features (excluding binary and target-encoded variables) into two groups: ultra-skewed (skewness &gt; 100) and moderately-skewed (2 &lt; skewness ≤ 100). Features were considered only if they had more than ten unique values and were not binary.</span>
<span id="cb24-564"><a href="#cb24-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-565"><a href="#cb24-565" aria-hidden="true" tabindex="-1"></a>For ultra-skewed features, we applied clipping at the 0.5th and 99.5th percentiles. For moderately skewed features, we clipped at the 0.1st and 99.9th percentiles. All thresholds were derived solely from the training data and applied to both training and test sets to ensure leakage-free transformations. This clipping procedure helped suppress extreme values that might otherwise dominate decision paths or split criteria in tree-based learners.</span>
<span id="cb24-566"><a href="#cb24-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-567"><a href="#cb24-567" aria-hidden="true" tabindex="-1"></a>These transformations were specifically designed for use with XGBoost and LightGBM, where reducing the influence of outliers improves model generalization and enhances interpretability in leaf-based decision structures.</span>
<span id="cb24-568"><a href="#cb24-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-571"><a href="#cb24-571" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-572"><a href="#cb24-572" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-573"><a href="#cb24-573" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-574"><a href="#cb24-574" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-575"><a href="#cb24-575" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-576"><a href="#cb24-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-577"><a href="#cb24-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-578"><a href="#cb24-578" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Categorize features by skew level ===</span></span>
<span id="cb24-579"><a href="#cb24-579" aria-hidden="true" tabindex="-1"></a>ultra_skewed <span class="op">=</span> []</span>
<span id="cb24-580"><a href="#cb24-580" aria-hidden="true" tabindex="-1"></a>moderately_skewed <span class="op">=</span> []</span>
<span id="cb24-581"><a href="#cb24-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-582"><a href="#cb24-582" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> skew_df.iterrows():</span>
<span id="cb24-583"><a href="#cb24-583" aria-hidden="true" tabindex="-1"></a>    feature <span class="op">=</span> row[<span class="st">'feature'</span>]</span>
<span id="cb24-584"><a href="#cb24-584" aria-hidden="true" tabindex="-1"></a>    skew <span class="op">=</span> row[<span class="st">'skewness'</span>]</span>
<span id="cb24-585"><a href="#cb24-585" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-586"><a href="#cb24-586" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature <span class="kw">not</span> <span class="kw">in</span> train_merged.columns:</span>
<span id="cb24-587"><a href="#cb24-587" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb24-588"><a href="#cb24-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-589"><a href="#cb24-589" aria-hidden="true" tabindex="-1"></a>    unique_vals <span class="op">=</span> train_merged[feature].nunique()</span>
<span id="cb24-590"><a href="#cb24-590" aria-hidden="true" tabindex="-1"></a>    is_binary <span class="op">=</span> <span class="bu">set</span>(train_merged[feature].dropna().unique()).issubset({<span class="dv">0</span>, <span class="dv">1</span>})</span>
<span id="cb24-591"><a href="#cb24-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-592"><a href="#cb24-592" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> unique_vals <span class="op">&gt;</span> <span class="dv">10</span> <span class="kw">and</span> <span class="kw">not</span> is_binary <span class="kw">and</span> <span class="kw">not</span> feature.endswith(<span class="st">'_te'</span>):</span>
<span id="cb24-593"><a href="#cb24-593" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> skew <span class="op">&gt;</span> <span class="dv">100</span>:</span>
<span id="cb24-594"><a href="#cb24-594" aria-hidden="true" tabindex="-1"></a>            ultra_skewed.append(feature)</span>
<span id="cb24-595"><a href="#cb24-595" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="dv">2</span><span class="op">&lt;</span> skew <span class="op">&lt;=</span> <span class="dv">100</span>:</span>
<span id="cb24-596"><a href="#cb24-596" aria-hidden="true" tabindex="-1"></a>            moderately_skewed.append(feature)</span>
<span id="cb24-597"><a href="#cb24-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-598"><a href="#cb24-598" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" </span><span class="sc">{</span><span class="bu">len</span>(ultra_skewed)<span class="sc">}</span><span class="ss"> ultra-skewed features to clip at 0.995."</span>)</span>
<span id="cb24-599"><a href="#cb24-599" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" </span><span class="sc">{</span><span class="bu">len</span>(moderately_skewed)<span class="sc">}</span><span class="ss"> moderately-skewed features to clip at 0.999."</span>)</span>
<span id="cb24-600"><a href="#cb24-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-601"><a href="#cb24-601" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Compute quantile clipping bounds ===</span></span>
<span id="cb24-602"><a href="#cb24-602" aria-hidden="true" tabindex="-1"></a>clip_bounds <span class="op">=</span> {}</span>
<span id="cb24-603"><a href="#cb24-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-604"><a href="#cb24-604" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> ultra_skewed:</span>
<span id="cb24-605"><a href="#cb24-605" aria-hidden="true" tabindex="-1"></a>    clip_bounds[col] <span class="op">=</span> (</span>
<span id="cb24-606"><a href="#cb24-606" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.005</span>),</span>
<span id="cb24-607"><a href="#cb24-607" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.995</span>)</span>
<span id="cb24-608"><a href="#cb24-608" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-609"><a href="#cb24-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-610"><a href="#cb24-610" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> moderately_skewed:</span>
<span id="cb24-611"><a href="#cb24-611" aria-hidden="true" tabindex="-1"></a>    clip_bounds[col] <span class="op">=</span> (</span>
<span id="cb24-612"><a href="#cb24-612" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.001</span>),</span>
<span id="cb24-613"><a href="#cb24-613" aria-hidden="true" tabindex="-1"></a>        train_merged[col].quantile(<span class="fl">0.999</span>)</span>
<span id="cb24-614"><a href="#cb24-614" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-615"><a href="#cb24-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-616"><a href="#cb24-616" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Apply clipping to both train and test ===</span></span>
<span id="cb24-617"><a href="#cb24-617" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> df_name, df <span class="kw">in</span> [(<span class="st">'train_merged'</span>, train_merged), (<span class="st">'test_merged'</span>, test_merged)]:</span>
<span id="cb24-618"><a href="#cb24-618" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col, (lower, upper) <span class="kw">in</span> clip_bounds.items():</span>
<span id="cb24-619"><a href="#cb24-619" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> col <span class="kw">in</span> df.columns:</span>
<span id="cb24-620"><a href="#cb24-620" aria-hidden="true" tabindex="-1"></a>            df[col] <span class="op">=</span> df[col].clip(lower, upper)</span>
<span id="cb24-621"><a href="#cb24-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-622"><a href="#cb24-622" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Adaptive clipping applied: 0.995 for ultra-skewed, 0.999 for moderately-skewed features."</span>)</span>
<span id="cb24-623"><a href="#cb24-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-624"><a href="#cb24-624" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-625"><a href="#cb24-625" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.10 Interaction Features for Linear and Nonlinear Models</span></span>
<span id="cb24-626"><a href="#cb24-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-627"><a href="#cb24-627" aria-hidden="true" tabindex="-1"></a>To enrich model expressiveness, we engineered a comprehensive set of interaction features used across both Ridge regression and tree-based models (XGBoost and LightGBM). These included multiplicative and ratio-based terms such as grade quality index, value per age, area x quality, and assess to neigh mean, capturing relationships between physical dimensions, valuation, quality, and neighborhood context. For Ridge regression, these features acted as implicit basis expansions—effectively enabling the linear model to capture non-additive effects by introducing new combinations of input features.</span>
<span id="cb24-628"><a href="#cb24-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-629"><a href="#cb24-629" aria-hidden="true" tabindex="-1"></a>Additionally, we created a specialized set of log-transformed interaction terms—such as log area x grade, log assess x age, and log value diff—used exclusively in the Ridge pipeline. These features helped linearize multiplicative relationships and reduce skew, improving fit under Ridge’s sensitivity to input distribution. Log-based interactions were excluded from tree models, which are inherently robust to skew and insensitive to monotonic transformations like log, as they rely only on the relative ordering of feature values when making splits.</span>
<span id="cb24-632"><a href="#cb24-632" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-633"><a href="#cb24-633" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-634"><a href="#cb24-634" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-635"><a href="#cb24-635" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-636"><a href="#cb24-636" aria-hidden="true" tabindex="-1"></a><span class="co"># === Interaction Features for Ridge Regression ===</span></span>
<span id="cb24-637"><a href="#cb24-637" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_features(df):</span>
<span id="cb24-638"><a href="#cb24-638" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb24-639"><a href="#cb24-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-640"><a href="#cb24-640" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Ratio features ===</span></span>
<span id="cb24-641"><a href="#cb24-641" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'area_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-642"><a href="#cb24-642" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'porch_ratio'</span>] <span class="op">=</span> df[<span class="st">'porch_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-643"><a href="#cb24-643" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'floor_density'</span>] <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-644"><a href="#cb24-644" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_build_density'</span>] <span class="op">=</span> df[<span class="st">'log_building_area_2019'</span>] <span class="op">-</span> df[<span class="st">'log_land_area_2019'</span>]</span>
<span id="cb24-645"><a href="#cb24-645" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_land_to_build_ratio'</span>] <span class="op">=</span> df[<span class="st">'log_land_area_2019'</span>] <span class="op">-</span> df[<span class="st">'log_building_area_2019'</span>]</span>
<span id="cb24-646"><a href="#cb24-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-647"><a href="#cb24-647" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'land_value_2018'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-648"><a href="#cb24-648" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_value_diff'</span>] <span class="op">=</span> df[<span class="st">'log_building_value_2018'</span>] <span class="op">-</span> df[<span class="st">'log_land_value_2018'</span>]</span>
<span id="cb24-649"><a href="#cb24-649" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-650"><a href="#cb24-650" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'price_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-651"><a href="#cb24-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-652"><a href="#cb24-652" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Bathroom &amp; room structure ===</span></span>
<span id="cb24-653"><a href="#cb24-653" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_score'</span>] <span class="op">=</span> df[<span class="st">'full_bath_2019'</span>] <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> df[<span class="st">'half_bath_2019'</span>]</span>
<span id="cb24-654"><a href="#cb24-654" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_density'</span>] <span class="op">=</span> df[<span class="st">'bathroom_score'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-655"><a href="#cb24-655" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedroom_ratio'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-656"><a href="#cb24-656" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_per_floor'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'floors_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-657"><a href="#cb24-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-658"><a href="#cb24-658" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Core interactions ===</span></span>
<span id="cb24-659"><a href="#cb24-659" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedrooms_x_floors'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">*</span> df[<span class="st">'floors_2019'</span>]</span>
<span id="cb24-660"><a href="#cb24-660" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_x_quality'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb24-661"><a href="#cb24-661" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_area_x_grade'</span>] <span class="op">=</span> df[<span class="st">'log_building_area_2019'</span>] <span class="op">*</span> df[<span class="st">'grade_2019'</span>]</span>
<span id="cb24-662"><a href="#cb24-662" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'log_assess_x_age'</span>] <span class="op">=</span> df[<span class="st">'log_assessed_2018'</span>] <span class="op">*</span> df[<span class="st">'building_age'</span>]</span>
<span id="cb24-663"><a href="#cb24-663" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_spread_neigh'</span>] <span class="op">=</span> df[<span class="st">'log_neigh_assess_q3'</span>] <span class="op">-</span> df[<span class="st">'log_neigh_assess_q1'</span>]</span>
<span id="cb24-664"><a href="#cb24-664" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'grade_quality_index'</span>] <span class="op">=</span> df[<span class="st">'grade_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb24-665"><a href="#cb24-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-666"><a href="#cb24-666" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Clean up ===</span></span>
<span id="cb24-667"><a href="#cb24-667" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.replace([np.inf, <span class="op">-</span>np.inf], np.nan)</span>
<span id="cb24-668"><a href="#cb24-668" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.fillna(<span class="dv">0</span>)</span>
<span id="cb24-669"><a href="#cb24-669" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb24-670"><a href="#cb24-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-671"><a href="#cb24-671" aria-hidden="true" tabindex="-1"></a><span class="co"># === Apply to train and test ===</span></span>
<span id="cb24-672"><a href="#cb24-672" aria-hidden="true" tabindex="-1"></a>train_merged <span class="op">=</span> add_features(train_merged)</span>
<span id="cb24-673"><a href="#cb24-673" aria-hidden="true" tabindex="-1"></a>test_merged <span class="op">=</span> add_features(test_merged)</span>
<span id="cb24-674"><a href="#cb24-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-675"><a href="#cb24-675" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-678"><a href="#cb24-678" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-679"><a href="#cb24-679" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-680"><a href="#cb24-680" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-681"><a href="#cb24-681" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-682"><a href="#cb24-682" aria-hidden="true" tabindex="-1"></a><span class="co"># === Interaction Features for Tree models ===</span></span>
<span id="cb24-683"><a href="#cb24-683" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_features(df):</span>
<span id="cb24-684"><a href="#cb24-684" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb24-685"><a href="#cb24-685" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-686"><a href="#cb24-686" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Ratio features ===</span></span>
<span id="cb24-687"><a href="#cb24-687" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'area_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-688"><a href="#cb24-688" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'porch_ratio'</span>] <span class="op">=</span> df[<span class="st">'porch_area_2019'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-689"><a href="#cb24-689" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'floor_density'</span>] <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">/</span> (df[<span class="st">'land_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-690"><a href="#cb24-690" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-691"><a href="#cb24-691" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_ratio'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'land_value_2018'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-692"><a href="#cb24-692" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-693"><a href="#cb24-693" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'price_per_sqft'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_area_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-694"><a href="#cb24-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-695"><a href="#cb24-695" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Bathroom &amp; room structure ===</span></span>
<span id="cb24-696"><a href="#cb24-696" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_score'</span>] <span class="op">=</span> df[<span class="st">'full_bath_2019'</span>] <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> df[<span class="st">'half_bath_2019'</span>]</span>
<span id="cb24-697"><a href="#cb24-697" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bathroom_density'</span>] <span class="op">=</span> df[<span class="st">'bathroom_score'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-698"><a href="#cb24-698" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedroom_ratio'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'total_rooms_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-699"><a href="#cb24-699" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_per_floor'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">/</span> (df[<span class="st">'floors_2019'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-700"><a href="#cb24-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-701"><a href="#cb24-701" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Core interactions ===</span></span>
<span id="cb24-702"><a href="#cb24-702" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'bedrooms_x_floors'</span>] <span class="op">=</span> df[<span class="st">'bedrooms_2019'</span>] <span class="op">*</span> df[<span class="st">'floors_2019'</span>]</span>
<span id="cb24-703"><a href="#cb24-703" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'rooms_x_quality'</span>] <span class="op">=</span> df[<span class="st">'total_rooms_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb24-704"><a href="#cb24-704" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_x_age'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">*</span> df[<span class="st">'building_age'</span>]</span>
<span id="cb24-705"><a href="#cb24-705" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'grade_quality_index'</span>] <span class="op">=</span> df[<span class="st">'grade_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb24-706"><a href="#cb24-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-707"><a href="#cb24-707" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Selected high-signal interactions ===</span></span>
<span id="cb24-708"><a href="#cb24-708" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'area_x_quality'</span>] <span class="op">=</span> df[<span class="st">'building_area_2019'</span>] <span class="op">*</span> df[<span class="st">'quality_2019'</span>]</span>
<span id="cb24-709"><a href="#cb24-709" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'floor_area_x_grade'</span>] <span class="op">=</span> df[<span class="st">'floor_area_total_2019'</span>] <span class="op">*</span> df[<span class="st">'grade_2019'</span>]</span>
<span id="cb24-710"><a href="#cb24-710" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_to_neigh_median'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_median'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-711"><a href="#cb24-711" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'assess_to_neigh_mean'</span>] <span class="op">=</span> df[<span class="st">'assessed_2018'</span>] <span class="op">/</span> (df[<span class="st">'neigh_assess_mean'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-712"><a href="#cb24-712" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">'value_per_age'</span>] <span class="op">=</span> df[<span class="st">'building_value_2018'</span>] <span class="op">/</span> (df[<span class="st">'building_age'</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb24-713"><a href="#cb24-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-714"><a href="#cb24-714" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Clean up ===</span></span>
<span id="cb24-715"><a href="#cb24-715" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.replace([np.inf, <span class="op">-</span>np.inf], np.nan)</span>
<span id="cb24-716"><a href="#cb24-716" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.fillna(<span class="dv">0</span>)</span>
<span id="cb24-717"><a href="#cb24-717" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-718"><a href="#cb24-718" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb24-719"><a href="#cb24-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-720"><a href="#cb24-720" aria-hidden="true" tabindex="-1"></a><span class="co"># === Apply to train and test sets ===</span></span>
<span id="cb24-721"><a href="#cb24-721" aria-hidden="true" tabindex="-1"></a>train_merged <span class="op">=</span> add_features(train_merged)</span>
<span id="cb24-722"><a href="#cb24-722" aria-hidden="true" tabindex="-1"></a>test_merged <span class="op">=</span> add_features(test_merged)</span>
<span id="cb24-723"><a href="#cb24-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-724"><a href="#cb24-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-725"><a href="#cb24-725" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-726"><a href="#cb24-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-727"><a href="#cb24-727" aria-hidden="true" tabindex="-1"></a><span class="fu"># 4. Model Development and Tuning</span></span>
<span id="cb24-728"><a href="#cb24-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-729"><a href="#cb24-729" aria-hidden="true" tabindex="-1"></a><span class="fu">## 4.1 Ridge Regression with Cross-Validation</span></span>
<span id="cb24-730"><a href="#cb24-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-731"><a href="#cb24-731" aria-hidden="true" tabindex="-1"></a>We implemented a Ridge regression model using RidgeCV to automatically select the regularization strength α through nested cross-validation. A 3-fold outer loop was used for estimating out-of-fold (OOF) performance, while each inner fold evaluated a grid of α values ranging from 10⁻³ to 10² on a logarithmic scale. Input features were standardized within a Pipeline using StandardScaler to ensure scale-invariant regression coefficients. The model selected a different optimal α for each fold, reflecting local variance in validation behavior:</span>
<span id="cb24-732"><a href="#cb24-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-733"><a href="#cb24-733" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fold 1 RMSE: 42,050.33—Best α: 2.1544</span>
<span id="cb24-734"><a href="#cb24-734" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fold 2 RMSE: 41,036.52—Best α: 27.8256</span>
<span id="cb24-735"><a href="#cb24-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fold 3 RMSE: 40,619.40—Best α: 0.5995</span>
<span id="cb24-736"><a href="#cb24-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-737"><a href="#cb24-737" aria-hidden="true" tabindex="-1"></a>The final out-of-fold RMSE across all folds was 41,239.79, with an average best α of approximately 10.1932. This indicates that moderate regularization consistently improved generalization across different training splits.</span>
<span id="cb24-738"><a href="#cb24-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-739"><a href="#cb24-739" aria-hidden="true" tabindex="-1"></a>We saved both OOF predictions and test forecasts as NumPy arrays—<span class="in">`ridgecv_oof_preds.npy`</span> and <span class="in">`ridgecv_test_preds.npy`</span>—for later use in model ensembling. These stored outputs served as reliable building blocks for downstream blending and stacking strategies.</span>
<span id="cb24-742"><a href="#cb24-742" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-743"><a href="#cb24-743" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-744"><a href="#cb24-744" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-745"><a href="#cb24-745" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-746"><a href="#cb24-746" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-747"><a href="#cb24-747" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-748"><a href="#cb24-748" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> RidgeCV</span>
<span id="cb24-749"><a href="#cb24-749" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb24-750"><a href="#cb24-750" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb24-751"><a href="#cb24-751" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb24-752"><a href="#cb24-752" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb24-753"><a href="#cb24-753" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb24-754"><a href="#cb24-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-755"><a href="#cb24-755" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Prepare training/test matrices ===</span></span>
<span id="cb24-756"><a href="#cb24-756" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> train_merged.copy()</span>
<span id="cb24-757"><a href="#cb24-757" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_merged.copy()</span>
<span id="cb24-758"><a href="#cb24-758" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.Series(y_train).values <span class="co"># use raw target (not log)</span></span>
<span id="cb24-759"><a href="#cb24-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-760"><a href="#cb24-760" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: RidgeCV pipeline ===</span></span>
<span id="cb24-761"><a href="#cb24-761" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-762"><a href="#cb24-762" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.logspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb24-763"><a href="#cb24-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-764"><a href="#cb24-764" aria-hidden="true" tabindex="-1"></a>ridge_oof <span class="op">=</span> np.zeros(<span class="bu">len</span>(X))</span>
<span id="cb24-765"><a href="#cb24-765" aria-hidden="true" tabindex="-1"></a>ridge_test_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_test))</span>
<span id="cb24-766"><a href="#cb24-766" aria-hidden="true" tabindex="-1"></a>best_alphas <span class="op">=</span> []</span>
<span id="cb24-767"><a href="#cb24-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-768"><a href="#cb24-768" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X)):</span>
<span id="cb24-769"><a href="#cb24-769" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> Fold </span><span class="sc">{</span>fold<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/5"</span>)</span>
<span id="cb24-770"><a href="#cb24-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-771"><a href="#cb24-771" aria-hidden="true" tabindex="-1"></a>    X_train, y_train_fold <span class="op">=</span> X.iloc[train_idx], y[train_idx]</span>
<span id="cb24-772"><a href="#cb24-772" aria-hidden="true" tabindex="-1"></a>    X_val, y_val <span class="op">=</span> X.iloc[val_idx], y[val_idx]</span>
<span id="cb24-773"><a href="#cb24-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-774"><a href="#cb24-774" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> make_pipeline(</span>
<span id="cb24-775"><a href="#cb24-775" aria-hidden="true" tabindex="-1"></a>        StandardScaler(),</span>
<span id="cb24-776"><a href="#cb24-776" aria-hidden="true" tabindex="-1"></a>        RidgeCV(alphas<span class="op">=</span>alphas, cv<span class="op">=</span><span class="dv">3</span>, scoring<span class="op">=</span><span class="st">'neg_root_mean_squared_error'</span>)</span>
<span id="cb24-777"><a href="#cb24-777" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-778"><a href="#cb24-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-779"><a href="#cb24-779" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train_fold)</span>
<span id="cb24-780"><a href="#cb24-780" aria-hidden="true" tabindex="-1"></a>    ridge_oof[val_idx] <span class="op">=</span> model.predict(X_val)</span>
<span id="cb24-781"><a href="#cb24-781" aria-hidden="true" tabindex="-1"></a>    ridge_test_preds <span class="op">+=</span> model.predict(X_test) <span class="op">/</span> kf.get_n_splits()</span>
<span id="cb24-782"><a href="#cb24-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-783"><a href="#cb24-783" aria-hidden="true" tabindex="-1"></a>    best_alpha <span class="op">=</span> model.named_steps[<span class="st">'ridgecv'</span>].alpha_</span>
<span id="cb24-784"><a href="#cb24-784" aria-hidden="true" tabindex="-1"></a>    best_alphas.append(best_alpha)</span>
<span id="cb24-785"><a href="#cb24-785" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb24-786"><a href="#cb24-786" aria-hidden="true" tabindex="-1"></a>    fold_rmse <span class="op">=</span> root_mean_squared_error(y_val, ridge_oof[val_idx])</span>
<span id="cb24-787"><a href="#cb24-787" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Fold </span><span class="sc">{</span>fold<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> RMSE: </span><span class="sc">{</span>fold_rmse<span class="sc">:,.2f}</span><span class="ss"> | Best alpha: </span><span class="sc">{</span>best_alpha<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-788"><a href="#cb24-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-789"><a href="#cb24-789" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 4: Final RMSE ===</span></span>
<span id="cb24-790"><a href="#cb24-790" aria-hidden="true" tabindex="-1"></a>final_rmse <span class="op">=</span> root_mean_squared_error(y, ridge_oof)</span>
<span id="cb24-791"><a href="#cb24-791" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> Final OOF RMSE (RidgeCV): </span><span class="sc">{</span>final_rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb24-792"><a href="#cb24-792" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Average best alpha across folds: </span><span class="sc">{</span>np<span class="sc">.</span>mean(best_alphas)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-793"><a href="#cb24-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-794"><a href="#cb24-794" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 5: Save predictions ===</span></span>
<span id="cb24-795"><a href="#cb24-795" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb24-796"><a href="#cb24-796" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ACCOUNT"</span>: acct_test.values.ravel(),</span>
<span id="cb24-797"><a href="#cb24-797" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TARGET"</span>: ridge_test_preds</span>
<span id="cb24-798"><a href="#cb24-798" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-799"><a href="#cb24-799" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_ridgecv_pipeline.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-800"><a href="#cb24-800" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st"> Saved: submission_ridgecv_pipeline.csv"</span>)</span>
<span id="cb24-801"><a href="#cb24-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-802"><a href="#cb24-802" aria-hidden="true" tabindex="-1"></a><span class="co"># === Optional: Save OOF &amp; test preds for stacking or analysis ===</span></span>
<span id="cb24-803"><a href="#cb24-803" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"ridgecv_oof_preds.npy"</span>, ridge_oof)</span>
<span id="cb24-804"><a href="#cb24-804" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"ridgecv_test_preds.npy"</span>, ridge_test_preds)</span>
<span id="cb24-805"><a href="#cb24-805" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: ridgecv_oof_preds.npy and ridgecv_test_preds.npy"</span>)</span>
<span id="cb24-806"><a href="#cb24-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-807"><a href="#cb24-807" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-808"><a href="#cb24-808" aria-hidden="true" tabindex="-1"></a><span class="fu">## 4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection</span></span>
<span id="cb24-809"><a href="#cb24-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-810"><a href="#cb24-810" aria-hidden="true" tabindex="-1"></a>To capture nonlinear interactions and leverage automatic handling of missing values and categorical splits, we trained two gradient boosting models: LightGBM and XGBoost. Both models followed a structured pipeline consisting of hyperparameter optimization using Optuna, followed by SHAP- and gain-based feature selection, and a final retraining on the selected features.</span>
<span id="cb24-811"><a href="#cb24-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-812"><a href="#cb24-812" aria-hidden="true" tabindex="-1"></a>**Step 1: Hyperparameter Tuning with Optuna.** For each model, we defined an Optuna objective that trained 3-fold cross-validated models using early stopping. We explored hyperparameter ranges tailored to each algorithm, with LightGBM using a native pruning callback and XGBoost leveraging <span class="in">`XGBoostPruningCallback`</span>. During tuning, we stored the best out-of-fold (OOF) predictions across all trials to later use in ensembling.</span>
<span id="cb24-815"><a href="#cb24-815" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-816"><a href="#cb24-816" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-817"><a href="#cb24-817" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-818"><a href="#cb24-818" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-819"><a href="#cb24-819" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-820"><a href="#cb24-820" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-821"><a href="#cb24-821" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb24-822"><a href="#cb24-822" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm <span class="im">as</span> lgb</span>
<span id="cb24-823"><a href="#cb24-823" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb24-824"><a href="#cb24-824" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb24-825"><a href="#cb24-825" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb24-826"><a href="#cb24-826" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.integration <span class="im">import</span> LightGBMPruningCallback</span>
<span id="cb24-827"><a href="#cb24-827" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.pruners <span class="im">import</span> SuccessiveHalvingPruner</span>
<span id="cb24-828"><a href="#cb24-828" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightgbm <span class="im">import</span> log_evaluation, early_stopping</span>
<span id="cb24-829"><a href="#cb24-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-830"><a href="#cb24-830" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 0: Setup Data ===</span></span>
<span id="cb24-831"><a href="#cb24-831" aria-hidden="true" tabindex="-1"></a>X_full <span class="op">=</span> train_merged.copy()</span>
<span id="cb24-832"><a href="#cb24-832" aria-hidden="true" tabindex="-1"></a>y_full <span class="op">=</span> pd.Series(y_train)</span>
<span id="cb24-833"><a href="#cb24-833" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_merged.copy()</span>
<span id="cb24-834"><a href="#cb24-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-835"><a href="#cb24-835" aria-hidden="true" tabindex="-1"></a><span class="co"># Detect categorical columns</span></span>
<span id="cb24-836"><a href="#cb24-836" aria-hidden="true" tabindex="-1"></a>cat_cols <span class="op">=</span> X_full.select_dtypes(include<span class="op">=</span>[<span class="st">'category'</span>, <span class="st">'object'</span>]).columns.tolist()</span>
<span id="cb24-837"><a href="#cb24-837" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> cat_cols:</span>
<span id="cb24-838"><a href="#cb24-838" aria-hidden="true" tabindex="-1"></a>    X_full[col] <span class="op">=</span> X_full[col].astype(<span class="st">"category"</span>)</span>
<span id="cb24-839"><a href="#cb24-839" aria-hidden="true" tabindex="-1"></a>    X_test[col] <span class="op">=</span> X_test[col].astype(<span class="st">"category"</span>)</span>
<span id="cb24-840"><a href="#cb24-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-841"><a href="#cb24-841" aria-hidden="true" tabindex="-1"></a>global_oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb24-842"><a href="#cb24-842" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb24-843"><a href="#cb24-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-844"><a href="#cb24-844" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Define Optuna Objective ===</span></span>
<span id="cb24-845"><a href="#cb24-845" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb24-846"><a href="#cb24-846" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> global_oof_preds, best_score</span>
<span id="cb24-847"><a href="#cb24-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-848"><a href="#cb24-848" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb24-849"><a href="#cb24-849" aria-hidden="true" tabindex="-1"></a>        <span class="st">"objective"</span>: <span class="st">"regression"</span>,</span>
<span id="cb24-850"><a href="#cb24-850" aria-hidden="true" tabindex="-1"></a>        <span class="st">"metric"</span>: <span class="st">"rmse"</span>,</span>
<span id="cb24-851"><a href="#cb24-851" aria-hidden="true" tabindex="-1"></a>        <span class="st">"boosting_type"</span>: <span class="st">"gbdt"</span>,</span>
<span id="cb24-852"><a href="#cb24-852" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: trial.suggest_float(<span class="st">"learning_rate"</span>, <span class="fl">0.025</span>, <span class="fl">0.04</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-853"><a href="#cb24-853" aria-hidden="true" tabindex="-1"></a>        <span class="st">"num_leaves"</span>: trial.suggest_int(<span class="st">"num_leaves"</span>, <span class="dv">160</span>, <span class="dv">220</span>),</span>
<span id="cb24-854"><a href="#cb24-854" aria-hidden="true" tabindex="-1"></a>        <span class="st">"max_depth"</span>: trial.suggest_int(<span class="st">"max_depth"</span>, <span class="dv">7</span>, <span class="dv">11</span>),</span>
<span id="cb24-855"><a href="#cb24-855" aria-hidden="true" tabindex="-1"></a>        <span class="st">"min_child_samples"</span>: trial.suggest_int(<span class="st">"min_child_samples"</span>, <span class="dv">18</span>, <span class="dv">30</span>),</span>
<span id="cb24-856"><a href="#cb24-856" aria-hidden="true" tabindex="-1"></a>        <span class="st">"subsample"</span>: trial.suggest_float(<span class="st">"subsample"</span>, <span class="fl">0.65</span>, <span class="fl">0.88</span>),</span>
<span id="cb24-857"><a href="#cb24-857" aria-hidden="true" tabindex="-1"></a>        <span class="st">"colsample_bytree"</span>: trial.suggest_float(<span class="st">"colsample_bytree"</span>, <span class="fl">0.6</span>, <span class="fl">0.75</span>),</span>
<span id="cb24-858"><a href="#cb24-858" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_alpha"</span>: trial.suggest_float(<span class="st">"reg_alpha"</span>, <span class="fl">1.0</span>, <span class="fl">5.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-859"><a href="#cb24-859" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_lambda"</span>: trial.suggest_float(<span class="st">"reg_lambda"</span>, <span class="fl">1.0</span>, <span class="fl">4.0</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-860"><a href="#cb24-860" aria-hidden="true" tabindex="-1"></a>        <span class="st">"min_split_gain"</span>: trial.suggest_float(<span class="st">"min_split_gain"</span>, <span class="fl">0.15</span>, <span class="fl">0.25</span>),</span>
<span id="cb24-861"><a href="#cb24-861" aria-hidden="true" tabindex="-1"></a>        <span class="st">"verbose"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb24-862"><a href="#cb24-862" aria-hidden="true" tabindex="-1"></a>        <span class="st">"n_jobs"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb24-863"><a href="#cb24-863" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb24-864"><a href="#cb24-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-865"><a href="#cb24-865" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-866"><a href="#cb24-866" aria-hidden="true" tabindex="-1"></a>    val_rmse <span class="op">=</span> []</span>
<span id="cb24-867"><a href="#cb24-867" aria-hidden="true" tabindex="-1"></a>    oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb24-868"><a href="#cb24-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-869"><a href="#cb24-869" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb24-870"><a href="#cb24-870" aria-hidden="true" tabindex="-1"></a>        X_train, X_val <span class="op">=</span> X_full.iloc[train_idx], X_full.iloc[val_idx]</span>
<span id="cb24-871"><a href="#cb24-871" aria-hidden="true" tabindex="-1"></a>        y_train_fold, y_val <span class="op">=</span> y_full.iloc[train_idx], y_full.iloc[val_idx]</span>
<span id="cb24-872"><a href="#cb24-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-873"><a href="#cb24-873" aria-hidden="true" tabindex="-1"></a>        dtrain <span class="op">=</span> lgb.Dataset(X_train, label<span class="op">=</span>y_train_fold, categorical_feature<span class="op">=</span>cat_cols)</span>
<span id="cb24-874"><a href="#cb24-874" aria-hidden="true" tabindex="-1"></a>        dvalid <span class="op">=</span> lgb.Dataset(X_val, label<span class="op">=</span>y_val, categorical_feature<span class="op">=</span>cat_cols)</span>
<span id="cb24-875"><a href="#cb24-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-876"><a href="#cb24-876" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> lgb.train(</span>
<span id="cb24-877"><a href="#cb24-877" aria-hidden="true" tabindex="-1"></a>            params,</span>
<span id="cb24-878"><a href="#cb24-878" aria-hidden="true" tabindex="-1"></a>            dtrain,</span>
<span id="cb24-879"><a href="#cb24-879" aria-hidden="true" tabindex="-1"></a>            valid_sets<span class="op">=</span>[dvalid],</span>
<span id="cb24-880"><a href="#cb24-880" aria-hidden="true" tabindex="-1"></a>            num_boost_round<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb24-881"><a href="#cb24-881" aria-hidden="true" tabindex="-1"></a>            callbacks<span class="op">=</span>[</span>
<span id="cb24-882"><a href="#cb24-882" aria-hidden="true" tabindex="-1"></a>                early_stopping(stopping_rounds<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb24-883"><a href="#cb24-883" aria-hidden="true" tabindex="-1"></a>                log_evaluation(period<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb24-884"><a href="#cb24-884" aria-hidden="true" tabindex="-1"></a>                LightGBMPruningCallback(trial, <span class="st">"rmse"</span>)</span>
<span id="cb24-885"><a href="#cb24-885" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb24-886"><a href="#cb24-886" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-887"><a href="#cb24-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-888"><a href="#cb24-888" aria-hidden="true" tabindex="-1"></a>        val_pred <span class="op">=</span> model.predict(X_val, num_iteration<span class="op">=</span>model.best_iteration)</span>
<span id="cb24-889"><a href="#cb24-889" aria-hidden="true" tabindex="-1"></a>        oof_preds[val_idx] <span class="op">=</span> val_pred</span>
<span id="cb24-890"><a href="#cb24-890" aria-hidden="true" tabindex="-1"></a>        val_rmse.append(root_mean_squared_error(y_val, val_pred))</span>
<span id="cb24-891"><a href="#cb24-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-892"><a href="#cb24-892" aria-hidden="true" tabindex="-1"></a>    mean_rmse <span class="op">=</span> np.mean(val_rmse)</span>
<span id="cb24-893"><a href="#cb24-893" aria-hidden="true" tabindex="-1"></a>    trial.set_user_attr(<span class="st">"cv_rmse"</span>, mean_rmse)</span>
<span id="cb24-894"><a href="#cb24-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-895"><a href="#cb24-895" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mean_rmse <span class="op">&lt;</span> best_score:</span>
<span id="cb24-896"><a href="#cb24-896" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> mean_rmse</span>
<span id="cb24-897"><a href="#cb24-897" aria-hidden="true" tabindex="-1"></a>        global_oof_preds[:] <span class="op">=</span> oof_preds</span>
<span id="cb24-898"><a href="#cb24-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-899"><a href="#cb24-899" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> | CV RMSE: </span><span class="sc">{</span>mean_rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb24-900"><a href="#cb24-900" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_rmse</span>
<span id="cb24-901"><a href="#cb24-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-902"><a href="#cb24-902" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Run Optuna ===</span></span>
<span id="cb24-903"><a href="#cb24-903" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(</span>
<span id="cb24-904"><a href="#cb24-904" aria-hidden="true" tabindex="-1"></a>    direction<span class="op">=</span><span class="st">'minimize'</span>,</span>
<span id="cb24-905"><a href="#cb24-905" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">'lgbm_study_final_with_shap'</span>,</span>
<span id="cb24-906"><a href="#cb24-906" aria-hidden="true" tabindex="-1"></a>    storage<span class="op">=</span><span class="st">'sqlite:///lgbm_study_final_with_shap.db'</span>,</span>
<span id="cb24-907"><a href="#cb24-907" aria-hidden="true" tabindex="-1"></a>    load_if_exists<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-908"><a href="#cb24-908" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>SuccessiveHalvingPruner(min_resource<span class="op">=</span><span class="dv">100</span>, reduction_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb24-909"><a href="#cb24-909" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-910"><a href="#cb24-910" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">25</span>, show_progress_bar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-911"><a href="#cb24-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-912"><a href="#cb24-912" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best RMSE:"</span>, study.best_value)</span>
<span id="cb24-913"><a href="#cb24-913" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best Parameters:"</span>, study.best_params)</span>
<span id="cb24-914"><a href="#cb24-914" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"oof_preds_lgbm.npy"</span>, global_oof_preds)</span>
<span id="cb24-915"><a href="#cb24-915" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: oof_preds_lgbm.npy"</span>)</span>
<span id="cb24-916"><a href="#cb24-916" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-919"><a href="#cb24-919" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-920"><a href="#cb24-920" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-921"><a href="#cb24-921" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-922"><a href="#cb24-922" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-923"><a href="#cb24-923" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-924"><a href="#cb24-924" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-925"><a href="#cb24-925" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb24-926"><a href="#cb24-926" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor</span>
<span id="cb24-927"><a href="#cb24-927" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb24-928"><a href="#cb24-928" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb24-929"><a href="#cb24-929" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optuna.integration <span class="im">import</span> XGBoostPruningCallback</span>
<span id="cb24-930"><a href="#cb24-930" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> shap <span class="im">import</span> TreeExplainer</span>
<span id="cb24-931"><a href="#cb24-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-932"><a href="#cb24-932" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 0: Prepare Data ===</span></span>
<span id="cb24-933"><a href="#cb24-933" aria-hidden="true" tabindex="-1"></a>X_full <span class="op">=</span> train_merged.copy()</span>
<span id="cb24-934"><a href="#cb24-934" aria-hidden="true" tabindex="-1"></a>y_full <span class="op">=</span> pd.Series(y_train)</span>
<span id="cb24-935"><a href="#cb24-935" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_merged.copy()</span>
<span id="cb24-936"><a href="#cb24-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-937"><a href="#cb24-937" aria-hidden="true" tabindex="-1"></a>bin_cols <span class="op">=</span> [</span>
<span id="cb24-938"><a href="#cb24-938" aria-hidden="true" tabindex="-1"></a>    <span class="st">'building_value_growth_bin'</span>,</span>
<span id="cb24-939"><a href="#cb24-939" aria-hidden="true" tabindex="-1"></a>    <span class="st">'assessed_growth_bin'</span>,</span>
<span id="cb24-940"><a href="#cb24-940" aria-hidden="true" tabindex="-1"></a>    <span class="st">'land_value_growth_bin'</span>,<span class="st">'year_built_bin'</span></span>
<span id="cb24-941"><a href="#cb24-941" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb24-942"><a href="#cb24-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-943"><a href="#cb24-943" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> bin_cols:</span>
<span id="cb24-944"><a href="#cb24-944" aria-hidden="true" tabindex="-1"></a>    X_full[col] <span class="op">=</span> X_full[col].cat.codes</span>
<span id="cb24-945"><a href="#cb24-945" aria-hidden="true" tabindex="-1"></a>    X_test[col] <span class="op">=</span> X_test[col].cat.codes</span>
<span id="cb24-946"><a href="#cb24-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-947"><a href="#cb24-947" aria-hidden="true" tabindex="-1"></a>categorical_cols <span class="op">=</span> X_full.select_dtypes(include<span class="op">=</span><span class="st">'object'</span>).columns.tolist()</span>
<span id="cb24-948"><a href="#cb24-948" aria-hidden="true" tabindex="-1"></a>X_full[categorical_cols] <span class="op">=</span> X_full[categorical_cols].astype(<span class="st">'category'</span>)</span>
<span id="cb24-949"><a href="#cb24-949" aria-hidden="true" tabindex="-1"></a>X_test[categorical_cols] <span class="op">=</span> X_test[categorical_cols].astype(<span class="st">'category'</span>)</span>
<span id="cb24-950"><a href="#cb24-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-951"><a href="#cb24-951" aria-hidden="true" tabindex="-1"></a><span class="co"># === Global OOF Tracker ===</span></span>
<span id="cb24-952"><a href="#cb24-952" aria-hidden="true" tabindex="-1"></a>global_oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb24-953"><a href="#cb24-953" aria-hidden="true" tabindex="-1"></a>best_score <span class="op">=</span> <span class="bu">float</span>(<span class="st">"inf"</span>)</span>
<span id="cb24-954"><a href="#cb24-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-955"><a href="#cb24-955" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 1: Optuna Objective Function (No SHAP during tuning) ===</span></span>
<span id="cb24-956"><a href="#cb24-956" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb24-957"><a href="#cb24-957" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> global_oof_preds, best_score</span>
<span id="cb24-958"><a href="#cb24-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-959"><a href="#cb24-959" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> {</span>
<span id="cb24-960"><a href="#cb24-960" aria-hidden="true" tabindex="-1"></a>        <span class="st">"objective"</span>: <span class="st">"reg:squarederror"</span>,</span>
<span id="cb24-961"><a href="#cb24-961" aria-hidden="true" tabindex="-1"></a>        <span class="st">"eval_metric"</span>: <span class="st">"rmse"</span>,</span>
<span id="cb24-962"><a href="#cb24-962" aria-hidden="true" tabindex="-1"></a>        <span class="st">"tree_method"</span>: <span class="st">"hist"</span>,</span>
<span id="cb24-963"><a href="#cb24-963" aria-hidden="true" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: trial.suggest_float(<span class="st">"learning_rate"</span>, <span class="fl">0.047</span>, <span class="fl">0.05</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-964"><a href="#cb24-964" aria-hidden="true" tabindex="-1"></a>        <span class="st">"max_depth"</span>: <span class="dv">6</span>,</span>
<span id="cb24-965"><a href="#cb24-965" aria-hidden="true" tabindex="-1"></a>        <span class="st">"min_child_weight"</span>: trial.suggest_int(<span class="st">"min_child_weight"</span>, <span class="dv">11</span>, <span class="dv">12</span>),</span>
<span id="cb24-966"><a href="#cb24-966" aria-hidden="true" tabindex="-1"></a>        <span class="st">"subsample"</span>: trial.suggest_float(<span class="st">"subsample"</span>, <span class="fl">0.87</span>, <span class="fl">0.89</span>),</span>
<span id="cb24-967"><a href="#cb24-967" aria-hidden="true" tabindex="-1"></a>        <span class="st">"colsample_bytree"</span>: trial.suggest_float(<span class="st">"colsample_bytree"</span>, <span class="fl">0.7</span>, <span class="fl">0.74</span>),</span>
<span id="cb24-968"><a href="#cb24-968" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_alpha"</span>: trial.suggest_float(<span class="st">"reg_alpha"</span>, <span class="fl">0.30</span>, <span class="fl">0.56</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-969"><a href="#cb24-969" aria-hidden="true" tabindex="-1"></a>        <span class="st">"reg_lambda"</span>: trial.suggest_float(<span class="st">"reg_lambda"</span>, <span class="fl">0.05</span>, <span class="fl">0.11</span>, log<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-970"><a href="#cb24-970" aria-hidden="true" tabindex="-1"></a>        <span class="st">"gamma"</span>: trial.suggest_float(<span class="st">"gamma"</span>, <span class="fl">1.1</span>, <span class="fl">4.3</span>),</span>
<span id="cb24-971"><a href="#cb24-971" aria-hidden="true" tabindex="-1"></a>        <span class="st">"n_estimators"</span>: <span class="dv">1000</span>,</span>
<span id="cb24-972"><a href="#cb24-972" aria-hidden="true" tabindex="-1"></a>        <span class="st">"n_jobs"</span>: <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb24-973"><a href="#cb24-973" aria-hidden="true" tabindex="-1"></a>        <span class="st">"enable_categorical"</span>: <span class="va">True</span>,</span>
<span id="cb24-974"><a href="#cb24-974" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb24-975"><a href="#cb24-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-976"><a href="#cb24-976" aria-hidden="true" tabindex="-1"></a>    kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-977"><a href="#cb24-977" aria-hidden="true" tabindex="-1"></a>    oof_preds <span class="op">=</span> np.zeros(<span class="bu">len</span>(X_full))</span>
<span id="cb24-978"><a href="#cb24-978" aria-hidden="true" tabindex="-1"></a>    fold_rmse <span class="op">=</span> []</span>
<span id="cb24-979"><a href="#cb24-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-980"><a href="#cb24-980" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb24-981"><a href="#cb24-981" aria-hidden="true" tabindex="-1"></a>        X_train, y_train_fold <span class="op">=</span> X_full.iloc[train_idx], y_full.iloc[train_idx]</span>
<span id="cb24-982"><a href="#cb24-982" aria-hidden="true" tabindex="-1"></a>        X_val, y_val <span class="op">=</span> X_full.iloc[val_idx], y_full.iloc[val_idx]</span>
<span id="cb24-983"><a href="#cb24-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-984"><a href="#cb24-984" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> XGBRegressor(</span>
<span id="cb24-985"><a href="#cb24-985" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>params,</span>
<span id="cb24-986"><a href="#cb24-986" aria-hidden="true" tabindex="-1"></a>            early_stopping_rounds<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb24-987"><a href="#cb24-987" aria-hidden="true" tabindex="-1"></a>            callbacks<span class="op">=</span>[XGBoostPruningCallback(trial, <span class="st">"validation_0-rmse"</span>),</span>
<span id="cb24-988"><a href="#cb24-988" aria-hidden="true" tabindex="-1"></a>                       ]</span>
<span id="cb24-989"><a href="#cb24-989" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb24-990"><a href="#cb24-990" aria-hidden="true" tabindex="-1"></a>        model.fit(X_train, y_train_fold, eval_set<span class="op">=</span>[(X_val, y_val)], verbose<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb24-991"><a href="#cb24-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-992"><a href="#cb24-992" aria-hidden="true" tabindex="-1"></a>        val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb24-993"><a href="#cb24-993" aria-hidden="true" tabindex="-1"></a>        oof_preds[val_idx] <span class="op">=</span> val_pred</span>
<span id="cb24-994"><a href="#cb24-994" aria-hidden="true" tabindex="-1"></a>        fold_rmse.append(root_mean_squared_error(y_val, val_pred))</span>
<span id="cb24-995"><a href="#cb24-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-996"><a href="#cb24-996" aria-hidden="true" tabindex="-1"></a>    mean_rmse <span class="op">=</span> np.mean(fold_rmse)</span>
<span id="cb24-997"><a href="#cb24-997" aria-hidden="true" tabindex="-1"></a>    trial.set_user_attr(<span class="st">"cv_rmse"</span>, mean_rmse)</span>
<span id="cb24-998"><a href="#cb24-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-999"><a href="#cb24-999" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mean_rmse <span class="op">&lt;</span> best_score:</span>
<span id="cb24-1000"><a href="#cb24-1000" aria-hidden="true" tabindex="-1"></a>        best_score <span class="op">=</span> mean_rmse</span>
<span id="cb24-1001"><a href="#cb24-1001" aria-hidden="true" tabindex="-1"></a>        global_oof_preds[:] <span class="op">=</span> oof_preds</span>
<span id="cb24-1002"><a href="#cb24-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1003"><a href="#cb24-1003" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> | CV RMSE: </span><span class="sc">{</span>mean_rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb24-1004"><a href="#cb24-1004" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_rmse</span>
<span id="cb24-1005"><a href="#cb24-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1006"><a href="#cb24-1006" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 2: Run Optuna ===</span></span>
<span id="cb24-1007"><a href="#cb24-1007" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(</span>
<span id="cb24-1008"><a href="#cb24-1008" aria-hidden="true" tabindex="-1"></a>    direction<span class="op">=</span><span class="st">'minimize'</span>,</span>
<span id="cb24-1009"><a href="#cb24-1009" aria-hidden="true" tabindex="-1"></a>    study_name<span class="op">=</span><span class="st">'xgbreg_optuna_final_no_shap'</span>,</span>
<span id="cb24-1010"><a href="#cb24-1010" aria-hidden="true" tabindex="-1"></a>    pruner<span class="op">=</span>optuna.pruners.SuccessiveHalvingPruner(min_resource<span class="op">=</span><span class="dv">100</span>, reduction_factor<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb24-1011"><a href="#cb24-1011" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-1012"><a href="#cb24-1012" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">25</span>, show_progress_bar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-1013"><a href="#cb24-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1014"><a href="#cb24-1014" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best RMSE:"</span>, study.best_value)</span>
<span id="cb24-1015"><a href="#cb24-1015" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Best Parameters:"</span>, study.best_params)</span>
<span id="cb24-1016"><a href="#cb24-1016" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"oof_preds_xgbreg.npy"</span>, global_oof_preds)</span>
<span id="cb24-1017"><a href="#cb24-1017" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: oof_preds_xgbreg.npy"</span>)</span>
<span id="cb24-1018"><a href="#cb24-1018" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1019"><a href="#cb24-1019" aria-hidden="true" tabindex="-1"></a>**Step 2: SHAP and Gain-Based Feature Selection.** After tuning, we trained new LightGBM and XGBoost models using the best parameters on each fold of the training data. For each fold, we computed SHAP importance values and LightGBM/XGBoost gain importances. We retained features that collectively accounted for 95% of total importance in either SHAP or gain, and constructed a union of these high-signal features across all folds. This union was used to define the final reduced feature space for retraining.</span>
<span id="cb24-1022"><a href="#cb24-1022" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-1023"><a href="#cb24-1023" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-1024"><a href="#cb24-1024" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-1025"><a href="#cb24-1025" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-1026"><a href="#cb24-1026" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: SHAP + GAIN Feature Selection for LGBM ===</span></span>
<span id="cb24-1027"><a href="#cb24-1027" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-1028"><a href="#cb24-1028" aria-hidden="true" tabindex="-1"></a>selected_feature_sets <span class="op">=</span> []</span>
<span id="cb24-1029"><a href="#cb24-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1030"><a href="#cb24-1030" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb24-1031"><a href="#cb24-1031" aria-hidden="true" tabindex="-1"></a>    X_train_raw, y_train_fold <span class="op">=</span> X_full.iloc[train_idx], y_full.iloc[train_idx]</span>
<span id="cb24-1032"><a href="#cb24-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1033"><a href="#cb24-1033" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> lgb.Dataset(X_train_raw, label<span class="op">=</span>y_train_fold, categorical_feature<span class="op">=</span>cat_cols)</span>
<span id="cb24-1034"><a href="#cb24-1034" aria-hidden="true" tabindex="-1"></a>    model_temp <span class="op">=</span> lgb.train(</span>
<span id="cb24-1035"><a href="#cb24-1035" aria-hidden="true" tabindex="-1"></a>        study.best_params,</span>
<span id="cb24-1036"><a href="#cb24-1036" aria-hidden="true" tabindex="-1"></a>        train_dataset,</span>
<span id="cb24-1037"><a href="#cb24-1037" aria-hidden="true" tabindex="-1"></a>        num_boost_round<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb24-1038"><a href="#cb24-1038" aria-hidden="true" tabindex="-1"></a>        valid_sets<span class="op">=</span>[train_dataset],</span>
<span id="cb24-1039"><a href="#cb24-1039" aria-hidden="true" tabindex="-1"></a>        callbacks<span class="op">=</span>[log_evaluation(period<span class="op">=</span><span class="dv">100</span>)] </span>
<span id="cb24-1040"><a href="#cb24-1040" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-1041"><a href="#cb24-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1042"><a href="#cb24-1042" aria-hidden="true" tabindex="-1"></a>    <span class="co"># SHAP importance</span></span>
<span id="cb24-1043"><a href="#cb24-1043" aria-hidden="true" tabindex="-1"></a>    explainer <span class="op">=</span> shap.TreeExplainer(model_temp)</span>
<span id="cb24-1044"><a href="#cb24-1044" aria-hidden="true" tabindex="-1"></a>    shap_values <span class="op">=</span> explainer.shap_values(X_train_raw)</span>
<span id="cb24-1045"><a href="#cb24-1045" aria-hidden="true" tabindex="-1"></a>    shap_df <span class="op">=</span> pd.DataFrame(np.<span class="bu">abs</span>(shap_values), columns<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb24-1046"><a href="#cb24-1046" aria-hidden="true" tabindex="-1"></a>    shap_importance <span class="op">=</span> shap_df.mean().sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1047"><a href="#cb24-1047" aria-hidden="true" tabindex="-1"></a>    shap_cumsum <span class="op">=</span> shap_importance.cumsum() <span class="op">/</span> shap_importance.<span class="bu">sum</span>()</span>
<span id="cb24-1048"><a href="#cb24-1048" aria-hidden="true" tabindex="-1"></a>    top_shap <span class="op">=</span> shap_cumsum[shap_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb24-1049"><a href="#cb24-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1050"><a href="#cb24-1050" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gain importance</span></span>
<span id="cb24-1051"><a href="#cb24-1051" aria-hidden="true" tabindex="-1"></a>    gain_importance <span class="op">=</span> pd.Series(model_temp.feature_importance(importance_type<span class="op">=</span><span class="st">'gain'</span>), index<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb24-1052"><a href="#cb24-1052" aria-hidden="true" tabindex="-1"></a>    gain_sorted <span class="op">=</span> gain_importance.sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1053"><a href="#cb24-1053" aria-hidden="true" tabindex="-1"></a>    gain_cumsum <span class="op">=</span> gain_sorted.cumsum() <span class="op">/</span> gain_sorted.<span class="bu">sum</span>()</span>
<span id="cb24-1054"><a href="#cb24-1054" aria-hidden="true" tabindex="-1"></a>    top_gain <span class="op">=</span> gain_cumsum[gain_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb24-1055"><a href="#cb24-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1056"><a href="#cb24-1056" aria-hidden="true" tabindex="-1"></a>    selected_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(top_shap).union(<span class="bu">set</span>(top_gain)))</span>
<span id="cb24-1057"><a href="#cb24-1057" aria-hidden="true" tabindex="-1"></a>    selected_feature_sets.append(selected_features)</span>
<span id="cb24-1058"><a href="#cb24-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1059"><a href="#cb24-1059" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 4: Final Feature Union ===</span></span>
<span id="cb24-1060"><a href="#cb24-1060" aria-hidden="true" tabindex="-1"></a>final_union_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>().union(<span class="op">*</span>selected_feature_sets))</span>
<span id="cb24-1061"><a href="#cb24-1061" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Final Union Feature Count:"</span>, <span class="bu">len</span>(final_union_features))</span>
<span id="cb24-1062"><a href="#cb24-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1063"><a href="#cb24-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter only those categorical columns that are in final features</span></span>
<span id="cb24-1064"><a href="#cb24-1064" aria-hidden="true" tabindex="-1"></a>filtered_cat_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> cat_cols <span class="cf">if</span> col <span class="kw">in</span> final_union_features]</span>
<span id="cb24-1065"><a href="#cb24-1065" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1068"><a href="#cb24-1068" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-1069"><a href="#cb24-1069" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-1070"><a href="#cb24-1070" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-1071"><a href="#cb24-1071" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-1072"><a href="#cb24-1072" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 3: Post-Optuna SHAP + Gain Feature Selection for XGBoost ===</span></span>
<span id="cb24-1073"><a href="#cb24-1073" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-1074"><a href="#cb24-1074" aria-hidden="true" tabindex="-1"></a>selected_feature_sets <span class="op">=</span> []</span>
<span id="cb24-1075"><a href="#cb24-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1076"><a href="#cb24-1076" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold, (train_idx, val_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X_full)):</span>
<span id="cb24-1077"><a href="#cb24-1077" aria-hidden="true" tabindex="-1"></a>    X_train_raw, y_train_fold <span class="op">=</span> X_full.iloc[train_idx], y_full.iloc[train_idx]</span>
<span id="cb24-1078"><a href="#cb24-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1079"><a href="#cb24-1079" aria-hidden="true" tabindex="-1"></a>    model_temp <span class="op">=</span> XGBRegressor(<span class="op">**</span>study.best_params, n_estimators<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb24-1080"><a href="#cb24-1080" aria-hidden="true" tabindex="-1"></a>    model_temp.fit(X_train_raw, y_train_fold)</span>
<span id="cb24-1081"><a href="#cb24-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1082"><a href="#cb24-1082" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === SHAP Importance ===</span></span>
<span id="cb24-1083"><a href="#cb24-1083" aria-hidden="true" tabindex="-1"></a>    explainer <span class="op">=</span> TreeExplainer(model_temp)</span>
<span id="cb24-1084"><a href="#cb24-1084" aria-hidden="true" tabindex="-1"></a>    shap_values <span class="op">=</span> explainer.shap_values(X_train_raw)</span>
<span id="cb24-1085"><a href="#cb24-1085" aria-hidden="true" tabindex="-1"></a>    shap_df <span class="op">=</span> pd.DataFrame(np.<span class="bu">abs</span>(shap_values), columns<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb24-1086"><a href="#cb24-1086" aria-hidden="true" tabindex="-1"></a>    shap_importance <span class="op">=</span> shap_df.mean().sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1087"><a href="#cb24-1087" aria-hidden="true" tabindex="-1"></a>    shap_cumsum <span class="op">=</span> shap_importance.cumsum() <span class="op">/</span> shap_importance.<span class="bu">sum</span>()</span>
<span id="cb24-1088"><a href="#cb24-1088" aria-hidden="true" tabindex="-1"></a>    top_shap <span class="op">=</span> shap_cumsum[shap_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb24-1089"><a href="#cb24-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1090"><a href="#cb24-1090" aria-hidden="true" tabindex="-1"></a>    <span class="co"># === Gain Importance ===</span></span>
<span id="cb24-1091"><a href="#cb24-1091" aria-hidden="true" tabindex="-1"></a>    gain_importance <span class="op">=</span> pd.Series(model_temp.feature_importances_, index<span class="op">=</span>X_train_raw.columns)</span>
<span id="cb24-1092"><a href="#cb24-1092" aria-hidden="true" tabindex="-1"></a>    gain_sorted <span class="op">=</span> gain_importance.sort_values(ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1093"><a href="#cb24-1093" aria-hidden="true" tabindex="-1"></a>    gain_cumsum <span class="op">=</span> gain_sorted.cumsum() <span class="op">/</span> gain_sorted.<span class="bu">sum</span>()</span>
<span id="cb24-1094"><a href="#cb24-1094" aria-hidden="true" tabindex="-1"></a>    top_gain <span class="op">=</span> gain_cumsum[gain_cumsum <span class="op">&lt;=</span> <span class="fl">0.95</span>].index.tolist()</span>
<span id="cb24-1095"><a href="#cb24-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1096"><a href="#cb24-1096" aria-hidden="true" tabindex="-1"></a>    selected_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(top_shap).union(<span class="bu">set</span>(top_gain)))</span>
<span id="cb24-1097"><a href="#cb24-1097" aria-hidden="true" tabindex="-1"></a>    selected_feature_sets.append(selected_features)</span>
<span id="cb24-1098"><a href="#cb24-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1099"><a href="#cb24-1099" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 4: Final Feature Union ===</span></span>
<span id="cb24-1100"><a href="#cb24-1100" aria-hidden="true" tabindex="-1"></a>final_union_features <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>().union(<span class="op">*</span>selected_feature_sets))</span>
<span id="cb24-1101"><a href="#cb24-1101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Final Union Feature Count:"</span>, <span class="bu">len</span>(final_union_features))</span>
<span id="cb24-1102"><a href="#cb24-1102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1103"><a href="#cb24-1103" aria-hidden="true" tabindex="-1"></a>**Step 3: Final Model Training and Inference.** Each final model was retrained on the full training set using only the selected features, with early stopping enabled to prevent overfitting. Predictions on the test set were generated using the best iteration count. All model outputs—including OOF predictions and test forecasts—were saved for submission and later use in ensemble blending.</span>
<span id="cb24-1104"><a href="#cb24-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1107"><a href="#cb24-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-1108"><a href="#cb24-1108" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-1109"><a href="#cb24-1109" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-1110"><a href="#cb24-1110" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-1111"><a href="#cb24-1111" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 5: Final Model on Selected Features for LGBM ===</span></span>
<span id="cb24-1112"><a href="#cb24-1112" aria-hidden="true" tabindex="-1"></a>X_full_selected <span class="op">=</span> X_full[final_union_features]</span>
<span id="cb24-1113"><a href="#cb24-1113" aria-hidden="true" tabindex="-1"></a>X_test_selected <span class="op">=</span> X_test[final_union_features]</span>
<span id="cb24-1114"><a href="#cb24-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1115"><a href="#cb24-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1116"><a href="#cb24-1116" aria-hidden="true" tabindex="-1"></a>final_dataset <span class="op">=</span> lgb.Dataset(X_full_selected, label<span class="op">=</span>y_full, categorical_feature<span class="op">=</span>filtered_cat_cols)</span>
<span id="cb24-1117"><a href="#cb24-1117" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> lgb.train(</span>
<span id="cb24-1118"><a href="#cb24-1118" aria-hidden="true" tabindex="-1"></a>    study.best_params,</span>
<span id="cb24-1119"><a href="#cb24-1119" aria-hidden="true" tabindex="-1"></a>    final_dataset,</span>
<span id="cb24-1120"><a href="#cb24-1120" aria-hidden="true" tabindex="-1"></a>    num_boost_round<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb24-1121"><a href="#cb24-1121" aria-hidden="true" tabindex="-1"></a>    valid_sets<span class="op">=</span>[final_dataset],</span>
<span id="cb24-1122"><a href="#cb24-1122" aria-hidden="true" tabindex="-1"></a>    valid_names<span class="op">=</span>[<span class="st">"train"</span>],</span>
<span id="cb24-1123"><a href="#cb24-1123" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[log_evaluation(period<span class="op">=</span><span class="dv">100</span>)]</span>
<span id="cb24-1124"><a href="#cb24-1124" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-1125"><a href="#cb24-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1126"><a href="#cb24-1126" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 6: Predict on Test Set ===</span></span>
<span id="cb24-1127"><a href="#cb24-1127" aria-hidden="true" tabindex="-1"></a>test_preds <span class="op">=</span> final_model.predict(X_test_selected, num_iteration<span class="op">=</span>final_model.best_iteration)</span>
<span id="cb24-1128"><a href="#cb24-1128" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_lgbm_shap.npy"</span>, test_preds)</span>
<span id="cb24-1129"><a href="#cb24-1129" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: test_preds_lgbm_shap.npy"</span>)</span>
<span id="cb24-1130"><a href="#cb24-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1131"><a href="#cb24-1131" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 7: Save Submission ===</span></span>
<span id="cb24-1132"><a href="#cb24-1132" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb24-1133"><a href="#cb24-1133" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ACCOUNT'</span>: acct_test.values.ravel(),  <span class="co"># Replace with your ID col</span></span>
<span id="cb24-1134"><a href="#cb24-1134" aria-hidden="true" tabindex="-1"></a>    <span class="st">'TARGET'</span>: test_preds</span>
<span id="cb24-1135"><a href="#cb24-1135" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-1136"><a href="#cb24-1136" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_lgbm_shap.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1137"><a href="#cb24-1137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Submission saved: submission_lgbm_shap.csv"</span>)</span>
<span id="cb24-1138"><a href="#cb24-1138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1141"><a href="#cb24-1141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-1142"><a href="#cb24-1142" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-1143"><a href="#cb24-1143" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-1144"><a href="#cb24-1144" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-1145"><a href="#cb24-1145" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 5: Final Model on Selected Features for XGBoost ===</span></span>
<span id="cb24-1146"><a href="#cb24-1146" aria-hidden="true" tabindex="-1"></a>X_full_selected <span class="op">=</span> X_full[final_union_features]</span>
<span id="cb24-1147"><a href="#cb24-1147" aria-hidden="true" tabindex="-1"></a>X_test_selected <span class="op">=</span> X_test[final_union_features]</span>
<span id="cb24-1148"><a href="#cb24-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1149"><a href="#cb24-1149" aria-hidden="true" tabindex="-1"></a>final_model <span class="op">=</span> XGBRegressor(<span class="op">**</span>study.best_params)</span>
<span id="cb24-1150"><a href="#cb24-1150" aria-hidden="true" tabindex="-1"></a>final_model.set_params(n_estimators<span class="op">=</span><span class="dv">1000</span>, verbosity<span class="op">=</span><span class="dv">1</span>, early_stopping_rounds<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb24-1151"><a href="#cb24-1151" aria-hidden="true" tabindex="-1"></a>final_model.fit(X_full_selected, y_full, eval_set<span class="op">=</span>[(X_full_selected, y_full)], verbose<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb24-1152"><a href="#cb24-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1153"><a href="#cb24-1153" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 6: Predict on Test Set ===</span></span>
<span id="cb24-1154"><a href="#cb24-1154" aria-hidden="true" tabindex="-1"></a>test_preds <span class="op">=</span> final_model.predict(X_test_selected)</span>
<span id="cb24-1155"><a href="#cb24-1155" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_xgbreg.npy"</span>, test_preds)</span>
<span id="cb24-1156"><a href="#cb24-1156" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Saved: test_preds_xgbreg.npy"</span>)</span>
<span id="cb24-1157"><a href="#cb24-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1158"><a href="#cb24-1158" aria-hidden="true" tabindex="-1"></a><span class="co"># === STEP 7: Create Submission File ===</span></span>
<span id="cb24-1159"><a href="#cb24-1159" aria-hidden="true" tabindex="-1"></a>account_ids <span class="op">=</span> acct_test.values.ravel()  <span class="co"># Replace with actual ID column</span></span>
<span id="cb24-1160"><a href="#cb24-1160" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb24-1161"><a href="#cb24-1161" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ACCOUNT'</span>: account_ids,</span>
<span id="cb24-1162"><a href="#cb24-1162" aria-hidden="true" tabindex="-1"></a>    <span class="st">'TARGET'</span>: test_preds</span>
<span id="cb24-1163"><a href="#cb24-1163" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-1164"><a href="#cb24-1164" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_xgbreg.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1165"><a href="#cb24-1165" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" Submission saved: submission_xgbreg.csv"</span>)</span>
<span id="cb24-1166"><a href="#cb24-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1167"><a href="#cb24-1167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1168"><a href="#cb24-1168" aria-hidden="true" tabindex="-1"></a>This hybrid approach—combining Optuna-based tuning with SHAP-driven interpretability—allowed us to retain only high-impact features, thereby improving generalization and reducing overfitting without sacrificing performance. The best out-of-fold RMSE achieved was 40,925.29 with XGBoost and 41,641.42 with LightGBM, confirming the robustness of both pipelines.</span>
<span id="cb24-1169"><a href="#cb24-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1170"><a href="#cb24-1170" aria-hidden="true" tabindex="-1"></a><span class="fu"># 5. Ensembling Strategy</span></span>
<span id="cb24-1171"><a href="#cb24-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1172"><a href="#cb24-1172" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5.1 Weighted Model Blending with Optuna</span></span>
<span id="cb24-1173"><a href="#cb24-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1174"><a href="#cb24-1174" aria-hidden="true" tabindex="-1"></a>To consolidate the strengths of our top-performing base models—XGBoost, RidgeCV, and LightGBM—we employed a weighted blending strategy optimized using Optuna. This approach directly searched for the optimal linear combination of model predictions that minimized RMSE on a holdout set.</span>
<span id="cb24-1175"><a href="#cb24-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1176"><a href="#cb24-1176" aria-hidden="true" tabindex="-1"></a>We first constructed a meta-training set consisting of out-of-fold (OOF) predictions from each base model. A corresponding test matrix was constructed from each model’s final test predictions. The blending weights were constrained to be non-negative and normalized to sum to one.</span>
<span id="cb24-1177"><a href="#cb24-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1178"><a href="#cb24-1178" aria-hidden="true" tabindex="-1"></a>An Optuna study was run for 100 trials, where each trial proposed a new set of blending weights and evaluated their performance via RMSE on the holdout split. The final optimized weights were:</span>
<span id="cb24-1179"><a href="#cb24-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1180"><a href="#cb24-1180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>XGBoost: w₀ = 25.98%</span>
<span id="cb24-1181"><a href="#cb24-1181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RidgeCV: w₁ = 33.53%</span>
<span id="cb24-1182"><a href="#cb24-1182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LightGBM: w₂ = 40.49%</span>
<span id="cb24-1183"><a href="#cb24-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1184"><a href="#cb24-1184" aria-hidden="true" tabindex="-1"></a>These weights were then used to produce a final blended prediction for the test set. The resulting predictions achieved an RMSE of 36,239.91 on the holdout set—outperforming all individual base models and demonstrating the value of combining linear and tree-based perspectives.</span>
<span id="cb24-1187"><a href="#cb24-1187" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-1188"><a href="#cb24-1188" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-1189"><a href="#cb24-1189" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-1190"><a href="#cb24-1190" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-1191"><a href="#cb24-1191" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-1192"><a href="#cb24-1192" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-1193"><a href="#cb24-1193" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optuna</span>
<span id="cb24-1194"><a href="#cb24-1194" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb24-1195"><a href="#cb24-1195" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb24-1196"><a href="#cb24-1196" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb24-1197"><a href="#cb24-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1198"><a href="#cb24-1198" aria-hidden="true" tabindex="-1"></a><span class="co"># === Setup Logging ===</span></span>
<span id="cb24-1199"><a href="#cb24-1199" aria-hidden="true" tabindex="-1"></a>logging.basicConfig(level<span class="op">=</span>logging.INFO, <span class="bu">format</span><span class="op">=</span><span class="st">"</span><span class="sc">%(asctime)s</span><span class="st"> [</span><span class="sc">%(levelname)s</span><span class="st">] </span><span class="sc">%(message)s</span><span class="st">"</span>)</span>
<span id="cb24-1200"><a href="#cb24-1200" aria-hidden="true" tabindex="-1"></a>logger <span class="op">=</span> logging.getLogger(<span class="st">"OptunaBlender"</span>)</span>
<span id="cb24-1201"><a href="#cb24-1201" aria-hidden="true" tabindex="-1"></a>optuna.logging.set_verbosity(optuna.logging.INFO)</span>
<span id="cb24-1202"><a href="#cb24-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1203"><a href="#cb24-1203" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load base model predictions ===</span></span>
<span id="cb24-1204"><a href="#cb24-1204" aria-hidden="true" tabindex="-1"></a>oof_xgb <span class="op">=</span> np.load(<span class="st">"oof_preds_xgbreg.npy"</span>)</span>
<span id="cb24-1205"><a href="#cb24-1205" aria-hidden="true" tabindex="-1"></a>ridge_oof <span class="op">=</span> np.load(<span class="st">"ridgecv_oof_preds.npy"</span>)</span>
<span id="cb24-1206"><a href="#cb24-1206" aria-hidden="true" tabindex="-1"></a>oof_lgb<span class="op">=</span>np.load(<span class="st">"oof_preds_lgbm.npy"</span>)</span>
<span id="cb24-1207"><a href="#cb24-1207" aria-hidden="true" tabindex="-1"></a>test_xgb <span class="op">=</span> np.load(<span class="st">"test_preds_xgbreg.npy"</span>)</span>
<span id="cb24-1208"><a href="#cb24-1208" aria-hidden="true" tabindex="-1"></a>ridge_test_preds <span class="op">=</span> np.load(<span class="st">"ridgecv_test_preds.npy"</span>)</span>
<span id="cb24-1209"><a href="#cb24-1209" aria-hidden="true" tabindex="-1"></a>test_lgb<span class="op">=</span>np.load(<span class="st">"test_preds_lgbm_shap.npy"</span>)</span>
<span id="cb24-1210"><a href="#cb24-1210" aria-hidden="true" tabindex="-1"></a><span class="co"># === Targets and prediction stack ===</span></span>
<span id="cb24-1211"><a href="#cb24-1211" aria-hidden="true" tabindex="-1"></a>y_meta <span class="op">=</span> train[<span class="st">'TARGET'</span>].values</span>
<span id="cb24-1212"><a href="#cb24-1212" aria-hidden="true" tabindex="-1"></a>X_base <span class="op">=</span> np.vstack([oof_xgb, ridge_oof,oof_lgb]).T</span>
<span id="cb24-1213"><a href="#cb24-1213" aria-hidden="true" tabindex="-1"></a>X_test_base <span class="op">=</span> np.vstack([test_xgb, ridge_test_preds,test_lgb]).T</span>
<span id="cb24-1214"><a href="#cb24-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1215"><a href="#cb24-1215" aria-hidden="true" tabindex="-1"></a><span class="co"># === Holdout split ===</span></span>
<span id="cb24-1216"><a href="#cb24-1216" aria-hidden="true" tabindex="-1"></a>X_train, X_holdout, y_train, y_holdout <span class="op">=</span> train_test_split(X_base, y_meta, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-1217"><a href="#cb24-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1218"><a href="#cb24-1218" aria-hidden="true" tabindex="-1"></a><span class="co"># === Objective Function ===</span></span>
<span id="cb24-1219"><a href="#cb24-1219" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(trial):</span>
<span id="cb24-1220"><a href="#cb24-1220" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> [trial.suggest_float(<span class="ss">f"w</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, <span class="dv">0</span>, <span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X_train.shape[<span class="dv">1</span>])]</span>
<span id="cb24-1221"><a href="#cb24-1221" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> np.array(weights)</span>
<span id="cb24-1222"><a href="#cb24-1222" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">/=</span> weights.<span class="bu">sum</span>()  <span class="co"># normalize</span></span>
<span id="cb24-1223"><a href="#cb24-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1224"><a href="#cb24-1224" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> X_holdout <span class="op">@</span> weights</span>
<span id="cb24-1225"><a href="#cb24-1225" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">=</span> root_mean_squared_error(y_holdout, preds)</span>
<span id="cb24-1226"><a href="#cb24-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1227"><a href="#cb24-1227" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="ss">f"Trial </span><span class="sc">{</span>trial<span class="sc">.</span>number<span class="sc">}</span><span class="ss"> | Weights: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(weights, <span class="dv">3</span>)<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> | RMSE: </span><span class="sc">{</span>rmse<span class="sc">:,.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-1228"><a href="#cb24-1228" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rmse</span>
<span id="cb24-1229"><a href="#cb24-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1230"><a href="#cb24-1230" aria-hidden="true" tabindex="-1"></a><span class="co"># === Run Study ===</span></span>
<span id="cb24-1231"><a href="#cb24-1231" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="st">" Starting Optuna optimization for weighted blending..."</span>)</span>
<span id="cb24-1232"><a href="#cb24-1232" aria-hidden="true" tabindex="-1"></a>study <span class="op">=</span> optuna.create_study(direction<span class="op">=</span><span class="st">"minimize"</span>)</span>
<span id="cb24-1233"><a href="#cb24-1233" aria-hidden="true" tabindex="-1"></a>study.optimize(objective, n_trials<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb24-1234"><a href="#cb24-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1235"><a href="#cb24-1235" aria-hidden="true" tabindex="-1"></a><span class="co"># === Best weights ===</span></span>
<span id="cb24-1236"><a href="#cb24-1236" aria-hidden="true" tabindex="-1"></a>best_weights <span class="op">=</span> np.array([study.best_trial.params[<span class="ss">f"w</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X_base.shape[<span class="dv">1</span>])])</span>
<span id="cb24-1237"><a href="#cb24-1237" aria-hidden="true" tabindex="-1"></a>best_weights <span class="op">/=</span> best_weights.<span class="bu">sum</span>()</span>
<span id="cb24-1238"><a href="#cb24-1238" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="ss">f" Best weights: </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">round</span>(best_weights, <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-1239"><a href="#cb24-1239" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="ss">f" Best RMSE: </span><span class="sc">{</span>study<span class="sc">.</span>best_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb24-1240"><a href="#cb24-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1241"><a href="#cb24-1241" aria-hidden="true" tabindex="-1"></a><span class="co"># === Final test prediction ===</span></span>
<span id="cb24-1242"><a href="#cb24-1242" aria-hidden="true" tabindex="-1"></a>meta_preds <span class="op">=</span> X_test_base <span class="op">@</span> best_weights</span>
<span id="cb24-1243"><a href="#cb24-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1244"><a href="#cb24-1244" aria-hidden="true" tabindex="-1"></a><span class="co"># === Save predictions ===</span></span>
<span id="cb24-1245"><a href="#cb24-1245" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_optuna_blended.npy"</span>, meta_preds)</span>
<span id="cb24-1246"><a href="#cb24-1246" aria-hidden="true" tabindex="-1"></a>account_ids <span class="op">=</span> acct_test.values.ravel()</span>
<span id="cb24-1247"><a href="#cb24-1247" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb24-1248"><a href="#cb24-1248" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ACCOUNT"</span>: account_ids,</span>
<span id="cb24-1249"><a href="#cb24-1249" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TARGET"</span>: meta_preds</span>
<span id="cb24-1250"><a href="#cb24-1250" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-1251"><a href="#cb24-1251" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_optuna_blended.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1252"><a href="#cb24-1252" aria-hidden="true" tabindex="-1"></a>logger.info(<span class="st">" Saved: test_preds_optuna_blended.npy and submission_optuna_blended.csv"</span>)</span>
<span id="cb24-1253"><a href="#cb24-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1254"><a href="#cb24-1254" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1255"><a href="#cb24-1255" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5.2 Stacked Ensembling with ElasticNetCV</span></span>
<span id="cb24-1256"><a href="#cb24-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1257"><a href="#cb24-1257" aria-hidden="true" tabindex="-1"></a>To complement our Optuna-based weighted average ensemble, we implemented a stacked generalization approach using ElasticNetCV as a meta-learner. This method treats out-of-fold (OOF) predictions from the base models—XGBoost, RidgeCV, and LightGBM—as features in a second-level regression model. By learning how to optimally combine base predictions, the meta-model can capture nonlinear inter-model relationships while applying regularization to prevent overfitting.</span>
<span id="cb24-1258"><a href="#cb24-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1259"><a href="#cb24-1259" aria-hidden="true" tabindex="-1"></a>**Meta-Model Training.** We concatenated the OOF predictions into a 3-column meta-feature matrix and used it to fit an ElasticNetCV model wrapped in a StandardScaler pipeline. The meta-model searched over a grid of <span class="in">`l1_ratio`</span> and <span class="in">`alpha`</span> values, using 3-fold cross-validation to identify the optimal regularization configuration.</span>
<span id="cb24-1260"><a href="#cb24-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1261"><a href="#cb24-1261" aria-hidden="true" tabindex="-1"></a>**Holdout Evaluation.** For evaluation, we trained the meta-learner on an 80% split and evaluated on a 20% holdout. The resulting RMSE was 36,344.64, closely aligned with the Optuna-weighted blend. The selected <span class="in">`alpha`</span> was 0.01, indicating strong regularization and robust coefficient shrinkage.</span>
<span id="cb24-1262"><a href="#cb24-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1263"><a href="#cb24-1263" aria-hidden="true" tabindex="-1"></a>**Final Test Predictions.** The final model was retrained on the full meta-feature set and used to predict the test set. This stacked approach provided a robust and regularized alternative to linear averaging, automatically downweighting weaker models while maintaining interpretability and reproducibility.</span>
<span id="cb24-1264"><a href="#cb24-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1265"><a href="#cb24-1265" aria-hidden="true" tabindex="-1"></a>On the competition leaderboard, the ElasticNetCV ensemble—combining Ridge, XGBoost, and LightGBM predictions—secured the top rank with a private leaderboard RMSE of 36,021, just 2 points above the public leaderboard score of 36,019. Interestingly, while an Optuna-weighted linear blend achieved a lower public RMSE, it ranked below the ElasticNet ensemble on the final evaluation, underscoring the latter’s generalization strength.</span>
<span id="cb24-1268"><a href="#cb24-1268" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb24-1269"><a href="#cb24-1269" aria-hidden="true" tabindex="-1"></a>echo: true</span>
<span id="cb24-1270"><a href="#cb24-1270" aria-hidden="true" tabindex="-1"></a>output: false</span>
<span id="cb24-1271"><a href="#cb24-1271" aria-hidden="true" tabindex="-1"></a>collapse: true</span>
<span id="cb24-1272"><a href="#cb24-1272" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-1273"><a href="#cb24-1273" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> ElasticNetCV</span>
<span id="cb24-1274"><a href="#cb24-1274" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb24-1275"><a href="#cb24-1275" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb24-1276"><a href="#cb24-1276" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb24-1277"><a href="#cb24-1277" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> root_mean_squared_error</span>
<span id="cb24-1278"><a href="#cb24-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1279"><a href="#cb24-1279" aria-hidden="true" tabindex="-1"></a><span class="co"># === Load OOF + Test Predictions ===</span></span>
<span id="cb24-1280"><a href="#cb24-1280" aria-hidden="true" tabindex="-1"></a>oof_xgb <span class="op">=</span> np.load(<span class="st">"oof_preds_xgbreg.npy"</span>)</span>
<span id="cb24-1281"><a href="#cb24-1281" aria-hidden="true" tabindex="-1"></a>test_xgb <span class="op">=</span> np.load(<span class="st">"test_preds_xgbreg.npy"</span>)</span>
<span id="cb24-1282"><a href="#cb24-1282" aria-hidden="true" tabindex="-1"></a>ridge_oof<span class="op">=</span>np.load(<span class="st">"ridgecv_oof_preds.npy"</span>)</span>
<span id="cb24-1283"><a href="#cb24-1283" aria-hidden="true" tabindex="-1"></a>ridge_test_preds<span class="op">=</span>np.load(<span class="st">"ridgecv_test_preds.npy"</span>)</span>
<span id="cb24-1284"><a href="#cb24-1284" aria-hidden="true" tabindex="-1"></a>oof_lgb<span class="op">=</span>np.load(<span class="st">"oof_preds_lgbm.npy"</span>)</span>
<span id="cb24-1285"><a href="#cb24-1285" aria-hidden="true" tabindex="-1"></a>test_lgb<span class="op">=</span>np.load(<span class="st">"test_preds_lgbm_shap.npy"</span>)</span>
<span id="cb24-1286"><a href="#cb24-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1287"><a href="#cb24-1287" aria-hidden="true" tabindex="-1"></a><span class="co"># === 3. Combine full meta-input feature set ===</span></span>
<span id="cb24-1288"><a href="#cb24-1288" aria-hidden="true" tabindex="-1"></a>X_meta <span class="op">=</span> np.hstack([</span>
<span id="cb24-1289"><a href="#cb24-1289" aria-hidden="true" tabindex="-1"></a>    oof_xgb.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb24-1290"><a href="#cb24-1290" aria-hidden="true" tabindex="-1"></a>    ridge_oof.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb24-1291"><a href="#cb24-1291" aria-hidden="true" tabindex="-1"></a>    oof_lgb.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb24-1292"><a href="#cb24-1292" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb24-1293"><a href="#cb24-1293" aria-hidden="true" tabindex="-1"></a>y_meta <span class="op">=</span> train[<span class="st">'TARGET'</span>].values</span>
<span id="cb24-1294"><a href="#cb24-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1295"><a href="#cb24-1295" aria-hidden="true" tabindex="-1"></a>X_meta_test <span class="op">=</span> np.hstack([</span>
<span id="cb24-1296"><a href="#cb24-1296" aria-hidden="true" tabindex="-1"></a>    test_xgb.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb24-1297"><a href="#cb24-1297" aria-hidden="true" tabindex="-1"></a>    ridge_test_preds.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb24-1298"><a href="#cb24-1298" aria-hidden="true" tabindex="-1"></a>    test_lgb.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb24-1299"><a href="#cb24-1299" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb24-1300"><a href="#cb24-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1301"><a href="#cb24-1301" aria-hidden="true" tabindex="-1"></a><span class="co"># === 4. Train ElasticNetCV meta-learner ===</span></span>
<span id="cb24-1302"><a href="#cb24-1302" aria-hidden="true" tabindex="-1"></a>meta_model <span class="op">=</span> make_pipeline(</span>
<span id="cb24-1303"><a href="#cb24-1303" aria-hidden="true" tabindex="-1"></a>    StandardScaler(),</span>
<span id="cb24-1304"><a href="#cb24-1304" aria-hidden="true" tabindex="-1"></a>    ElasticNetCV(</span>
<span id="cb24-1305"><a href="#cb24-1305" aria-hidden="true" tabindex="-1"></a>        l1_ratio<span class="op">=</span>[<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="dv">1</span>],</span>
<span id="cb24-1306"><a href="#cb24-1306" aria-hidden="true" tabindex="-1"></a>        alphas<span class="op">=</span>np.logspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">100</span>),</span>
<span id="cb24-1307"><a href="#cb24-1307" aria-hidden="true" tabindex="-1"></a>        cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb24-1308"><a href="#cb24-1308" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span><span class="dv">5000</span>,</span>
<span id="cb24-1309"><a href="#cb24-1309" aria-hidden="true" tabindex="-1"></a>        n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb24-1310"><a href="#cb24-1310" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-1311"><a href="#cb24-1311" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-1312"><a href="#cb24-1312" aria-hidden="true" tabindex="-1"></a>meta_model.fit(X_meta, y_meta)</span>
<span id="cb24-1313"><a href="#cb24-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1314"><a href="#cb24-1314" aria-hidden="true" tabindex="-1"></a><span class="co"># === 5. Predict and evaluate (optional holdout split) ===</span></span>
<span id="cb24-1315"><a href="#cb24-1315" aria-hidden="true" tabindex="-1"></a><span class="co"># You can skip this section if you're blending on full train</span></span>
<span id="cb24-1316"><a href="#cb24-1316" aria-hidden="true" tabindex="-1"></a>X_train, X_holdout, y_train, y_holdout <span class="op">=</span> train_test_split(X_meta, y_meta, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb24-1317"><a href="#cb24-1317" aria-hidden="true" tabindex="-1"></a>meta_model.fit(X_train, y_train)</span>
<span id="cb24-1318"><a href="#cb24-1318" aria-hidden="true" tabindex="-1"></a>holdout_preds <span class="op">=</span> meta_model.predict(X_holdout)</span>
<span id="cb24-1319"><a href="#cb24-1319" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> root_mean_squared_error(y_holdout, holdout_preds)</span>
<span id="cb24-1320"><a href="#cb24-1320" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ElasticNetCV Blended Meta Holdout RMSE: </span><span class="sc">{</span>rmse<span class="sc">:,.2f}</span><span class="ss">"</span>)</span>
<span id="cb24-1321"><a href="#cb24-1321" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> meta_model.named_steps[<span class="st">'elasticnetcv'</span>].alpha_</span>
<span id="cb24-1322"><a href="#cb24-1322" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Best alpha selected: </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-1323"><a href="#cb24-1323" aria-hidden="true" tabindex="-1"></a><span class="co"># === 6. Final predictions for test set ===</span></span>
<span id="cb24-1324"><a href="#cb24-1324" aria-hidden="true" tabindex="-1"></a>meta_preds <span class="op">=</span> meta_model.predict(X_meta_test)</span>
<span id="cb24-1325"><a href="#cb24-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1326"><a href="#cb24-1326" aria-hidden="true" tabindex="-1"></a><span class="co"># === 7. Save blended test predictions ===</span></span>
<span id="cb24-1327"><a href="#cb24-1327" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">"test_preds_elasticnet_blended.npy"</span>, meta_preds)</span>
<span id="cb24-1328"><a href="#cb24-1328" aria-hidden="true" tabindex="-1"></a>account_ids <span class="op">=</span> acct_test.values.ravel() </span>
<span id="cb24-1329"><a href="#cb24-1329" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.DataFrame({</span>
<span id="cb24-1330"><a href="#cb24-1330" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ACCOUNT"</span>: account_ids,  <span class="co"># Replace with your actual ID column</span></span>
<span id="cb24-1331"><a href="#cb24-1331" aria-hidden="true" tabindex="-1"></a>    <span class="st">"TARGET"</span>: meta_preds</span>
<span id="cb24-1332"><a href="#cb24-1332" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-1333"><a href="#cb24-1333" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">"submission_elasticnet_blended.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-1334"><a href="#cb24-1334" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">" ElasticNetCV blended stacking submission saved."</span>)</span>
<span id="cb24-1335"><a href="#cb24-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1336"><a href="#cb24-1336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb24-1337"><a href="#cb24-1337" aria-hidden="true" tabindex="-1"></a><span class="fu"># 6. Residual Error Analysis</span></span>
<span id="cb24-1338"><a href="#cb24-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1339"><a href="#cb24-1339" aria-hidden="true" tabindex="-1"></a>To evaluate model robustness, we conducted residual analysis across key dimensions. Three consistent patterns emerged:</span>
<span id="cb24-1340"><a href="#cb24-1340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1341"><a href="#cb24-1341" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Underprediction for High-Value Properties:** Residuals increased with actual property value, particularly above $5M. This indicates systematic underestimation of high-end properties, likely due to their rarity and unique characteristics.</span>
<span id="cb24-1342"><a href="#cb24-1342" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Volatility in Protested Properties:** Properties with multiple protests between 2015–2018 exhibited larger residual variance, despite median residuals near zero. This suggests that frequently disputed properties are harder to predict, potentially due to unrecorded structural changes or historical valuation disagreements not captured in the dataset.</span>
<span id="cb24-1343"><a href="#cb24-1343" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Poor Generalization in Sparse Neighborhoods:** Residuals were more variable and extreme in neighborhoods with low frequency in the training data. This suggests weaker model generalization in underrepresented regions. Frequency-aware encodings or collapsing sparse neighborhoods into broader groups may enhance stability and reduce prediction noise in these areas.</span>
<span id="cb24-1344"><a href="#cb24-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1345"><a href="#cb24-1345" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb24-1346"><a href="#cb24-1346" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb24-1347"><a href="#cb24-1347" aria-hidden="true" tabindex="-1"></a><span class="al">![Neighborhood Frequency](images/residuals_vs_neighborhood_freq.png)</span>{fig-cap="Residuals vs. Neighborhood Frequency" width=100%}</span>
<span id="cb24-1348"><a href="#cb24-1348" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1349"><a href="#cb24-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1350"><a href="#cb24-1350" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb24-1351"><a href="#cb24-1351" aria-hidden="true" tabindex="-1"></a><span class="al">![Protest Count](images/residuals_by_protest_count.png)</span>{fig-cap="Residuals by Protest Count (2015–2018)" width=100%}</span>
<span id="cb24-1352"><a href="#cb24-1352" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1353"><a href="#cb24-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1354"><a href="#cb24-1354" aria-hidden="true" tabindex="-1"></a>::: {.column width="33%"}</span>
<span id="cb24-1355"><a href="#cb24-1355" aria-hidden="true" tabindex="-1"></a><span class="al">![Actual Value](images/residuals_vs_actual_value.png)</span>{fig-cap="Residuals vs. Actual Value" width=100%}</span>
<span id="cb24-1356"><a href="#cb24-1356" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1357"><a href="#cb24-1357" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1358"><a href="#cb24-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1359"><a href="#cb24-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1360"><a href="#cb24-1360" aria-hidden="true" tabindex="-1"></a><span class="fu"># 7. SHAP-Based Interpretability and Insights</span></span>
<span id="cb24-1361"><a href="#cb24-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1362"><a href="#cb24-1362" aria-hidden="true" tabindex="-1"></a><span class="fu">## Top Predictive Features</span></span>
<span id="cb24-1363"><a href="#cb24-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1364"><a href="#cb24-1364" aria-hidden="true" tabindex="-1"></a>SHAP analysis revealed that features like assessed 2018, building value 2018, and land value 2018 were primary drivers of the model’s predictions. Structural attributes such as building area 2019, floor area x grade, and grade 2019 also carried strong explanatory power. These results confirmed the value of engineering ratio and interaction terms that encode economic density, build quality, and age-adjusted valuation. Neighborhood-level variables, especially neigh assess std and frequency encodings, further demonstrated the importance of local context in real estate assessment.</span>
<span id="cb24-1365"><a href="#cb24-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1366"><a href="#cb24-1366" aria-hidden="true" tabindex="-1"></a><span class="fu">## Low-Impact Features</span></span>
<span id="cb24-1367"><a href="#cb24-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1368"><a href="#cb24-1368" aria-hidden="true" tabindex="-1"></a>Although SHAP-ranked bottom 30 features showed limited average contribution, they were retained via the 95% SHAP + Gain union due to their potential complementary value. Examples include early-year indicators like quality description 2015, fireplaces 2016, and spatial ratios like porch ratio. Their inclusion likely enhanced generalization by supporting edge cases, and their low impact helped confirm that more aggressive feature pruning would have offered little gain.</span>
<span id="cb24-1369"><a href="#cb24-1369" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb24-1370"><a href="#cb24-1370" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb24-1371"><a href="#cb24-1371" aria-hidden="true" tabindex="-1"></a><span class="al">![SHAP Bottom 30](images/shap_summary_bottom30_union.png)</span>{fig-cap="Bottom 30 SHAP Features" width=100%}</span>
<span id="cb24-1372"><a href="#cb24-1372" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1373"><a href="#cb24-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1374"><a href="#cb24-1374" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb24-1375"><a href="#cb24-1375" aria-hidden="true" tabindex="-1"></a><span class="al">![SHAP Top 30](images/shap_summary_top30_union.png)</span>{fig-cap="Top 30 SHAP Features" width=100%}</span>
<span id="cb24-1376"><a href="#cb24-1376" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1377"><a href="#cb24-1377" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb24-1378"><a href="#cb24-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1379"><a href="#cb24-1379" aria-hidden="true" tabindex="-1"></a><span class="fu"># 8. Reflections and Lessons Learned</span></span>
<span id="cb24-1380"><a href="#cb24-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1381"><a href="#cb24-1381" aria-hidden="true" tabindex="-1"></a>Our modeling pipeline blended SHAP-driven interpretability with ensemble-based prediction to achieve both transparency and predictive strength. Key lessons emerged across three dimensions:</span>
<span id="cb24-1382"><a href="#cb24-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1383"><a href="#cb24-1383" aria-hidden="true" tabindex="-1"></a><span class="fu">## What Worked Well</span></span>
<span id="cb24-1384"><a href="#cb24-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1385"><a href="#cb24-1385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The SHAP + Gain union was highly effective in denoising the feature space and avoiding overfitting, retaining only high-impact predictors across folds.</span>
<span id="cb24-1386"><a href="#cb24-1386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The ElasticNet ensemble combined linear and non-linear model strengths to capture nuanced patterns in both well-represented and sparse regions.</span>
<span id="cb24-1387"><a href="#cb24-1387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optuna tuning reduced manual trial-and-error and consistently improved generalization across Ridge, LGBM, and XGBoost pipelines.</span>
<span id="cb24-1388"><a href="#cb24-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1389"><a href="#cb24-1389" aria-hidden="true" tabindex="-1"></a><span class="fu">## What Could Improve</span></span>
<span id="cb24-1390"><a href="#cb24-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1391"><a href="#cb24-1391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We did not model temporal dependencies across yearly features—treating them as time-series sequences or using transformer architectures may better account for evolving valuation dynamics over time.</span>
<span id="cb24-1392"><a href="#cb24-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Geographical variation was only partially captured using frequency encoding. Future work could explore residual stacking or blended stacking techniques with subgroup-aware features (e.g., neighborhood frequency, protest history) to correct localized error patterns more explicitly.</span>
<span id="cb24-1393"><a href="#cb24-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-1394"><a href="#cb24-1394" aria-hidden="true" tabindex="-1"></a>Overall, the final ElasticNet ensemble—integrating Ridge, SHAP-informed LGBM, and Optuna-tuned XGBoost—delivered a strong performance on the private leaderboard. Future extensions may benefit from incorporating time-aware modeling, subgroup-specific residual correction, or spatially informed representations to further improve accuracy and fairness in municipal assessment systems.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>