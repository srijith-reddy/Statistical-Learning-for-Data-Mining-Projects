[
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "STA 9890 Project: Property Valuation",
    "section": "",
    "text": "Property assessment values directly influence individual tax obligations, urban development decisions, and housing affordability analyses. However, public datasets are often noisy and incomplete‚Äîfeaturing missing renovation records, outdated area measurements, or abrupt shifts in land valuation. Therefore, a high-performing ML model must not only minimize prediction error but also provide interpretable insights that help stakeholders understand, trust, and audit the predictions‚Äîespecially when such predictions may inform public policy or fiscal planning."
  },
  {
    "objectID": "project3.html#full-report",
    "href": "project3.html#full-report",
    "title": "STA 9890 Project: Property Valuation",
    "section": "",
    "text": "Download the full PDF report"
  },
  {
    "objectID": "project3.html#summary",
    "href": "project3.html#summary",
    "title": "STA 9890 Project: Property Valuation",
    "section": "üîç Summary",
    "text": "üîç Summary\n\nTask: Predict 2019 assessed property value using structured property records from 2015‚Äì2019\nModels: RidgeCV, XGBoost, LightGBM\nFeature Selection: SHAP + Gain union\nEnsembling: Optuna weights + ElasticNet stacked\nTop RMSE: 36,021 on private leaderboard (ElasticNet ensemble)"
  },
  {
    "objectID": "project3.html#full-modeling-pipeline",
    "href": "project3.html#full-modeling-pipeline",
    "title": "STA 9890 Project: Property Valuation",
    "section": "üöÄ Full Modeling Pipeline",
    "text": "üöÄ Full Modeling Pipeline"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "My Portfolio\nExplore my projects in statistical learning and data mining.\n\nProject 1\nProject 2\nProject 3"
  },
  {
    "objectID": "project3.html#why-prediction-accuracy-and-interpretability-matter",
    "href": "project3.html#why-prediction-accuracy-and-interpretability-matter",
    "title": "STA 9890 Project: Property Valuation",
    "section": "",
    "text": "Property assessment values directly influence individual tax obligations, urban development decisions, and housing affordability analyses. However, public datasets are often noisy and incomplete‚Äîfeaturing missing renovation records, outdated area measurements, or abrupt shifts in land valuation. Therefore, a high-performing ML model must not only minimize prediction error but also provide interpretable insights that help stakeholders understand, trust, and audit the predictions‚Äîespecially when such predictions may inform public policy or fiscal planning."
  },
  {
    "objectID": "project3.html#logic-driven-missing-value-handling-and-imputation",
    "href": "project3.html#logic-driven-missing-value-handling-and-imputation",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.1 Logic-Driven Missing Value Handling and Imputation",
    "text": "3.1 Logic-Driven Missing Value Handling and Imputation\nBackfilling Year-Based Columns Across Feature Groups: To handle missing values in temporally structured features (e.g., building area, quality, full bath), we designed a consistent backfilling approach that uses older data to impute more recent years. Specifically, columns were ordered from newest to oldest (i.e., 2019, 2018, ‚Ä¶, 2015), and we applied bfill(axis=1) across these columns. This setup causes older values (e.g., from 2015 or 2016) to be used to fill in newer year columns (e.g., 2018 or 2019), effectively implementing a forward fill in temporal logic. This approach assumes that older data reflects the property‚Äôs original state more accurately and helps prevent later-year anomalies or missing values from distorting long-term trends.\nThis logic was applied across multiple feature groups: - Residential count features: floors, full bath, half bath, bedrooms, total rooms - Area-based features: building area, land area - Valuation features: building value, land value, assessed - Categorical building attributes: foundation type, grade, building condition, quality, quality description, physical condition, exterior walls, has cooling, has heat\n\n  echo: true\n  output: false\n  collapse: true\ndef backfill_categorical_year_features(df, features, years):\n    for feature in features:\n        year_cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n        if len(year_cols) &gt;= 2:\n            df[year_cols] = df[year_cols].bfill(axis=1)\n            print(f\" Backfilled: {feature} across {year_cols}\")\n        else:\n            print(f\" Skipped: {feature} ‚Äî not enough year-based columns.\")\n    return df\n\n# Backfill from most recent year to oldest\nyears = ['2019', '2018', '2017', '2016', '2015']\nfeatures = ['building_condition', 'foundation_type', 'grade', 'has_cooling', \n            'has_heat', 'physical_condition', 'exterior_walls']\n\n# Apply to train and test\ntrain_merged = backfill_categorical_year_features(train_merged, features, years)\ntest_merged = backfill_categorical_year_features(test_merged, features, years)\n\nZero-Aware Property Filtering: In cases where floor_area_total_2019 = 0, we treated the property as non-residential or commercial and applied domain-specific logic to avoid inappropriate imputations or distortions in the modeling process: - All related residential building features‚Äîsuch as full bath, total rooms, garage area, and porch area‚Äîwere set to zero. - We also zeroed out all building area variables and the corresponding building value variables to reflect the absence of a residential structure.\nThese records were retained in the dataset rather than dropped, as they likely represent a distinct class of properties where valuation is driven primarily by land characteristics. Explicitly identifying and treating these properties allowed the model to better separate residential and non-residential valuation patterns.\n\necho: true\noutput: false\ncollapse: true\n# === STEP 0: Define base feature names ===\nnumeric_bases = [\n    'garage_area', 'porch_area', 'floors', 'half_bath', 'full_bath',\n    'total_rooms', 'bedrooms', 'fireplaces', 'building_area', 'building_value'\n]\n\ncategorical_fill_map = {\n    'quality': 'None',\n    'quality_description': 'None',\n    'building_condition': 'None',\n    'foundation_type': 'None',\n    'grade': 'None',\n    'has_cooling': False,\n    'has_heat': False,\n    'physical_condition': 'None',\n    'exterior_walls': 'None',\n    'protested': False\n}\n\n# Generate full list of columns (2015‚Äì2019 only, no final columns)\nnumeric_cols_to_zero = [\n    f'{base}_{year}' for base in numeric_bases for year in range(2015, 2020)\n] + ['building_value_growth']\n\ncategorical_cols_to_fill = {\n    f'{base}_{year}': val\n    for base, val in categorical_fill_map.items()\n    for year in range(2015, 2020)\n}\n\n# === STEP 1: Apply imputation if floor_area_total_2019 == 0 ===\nfor df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n    if 'floor_area_total_2019' in df.columns:\n        zero_floor_mask = df['floor_area_total_2019'] == 0\n\n        # Fill numeric columns with 0\n        for col in numeric_cols_to_zero:\n            if col in df.columns:\n                df.loc[zero_floor_mask, col] = df.loc[zero_floor_mask, col].fillna(0)\n\n        # Fill categorical/boolean columns\n        for col, fill_val in categorical_cols_to_fill.items():\n            if col in df.columns:\n                df.loc[zero_floor_mask, col] = df.loc[zero_floor_mask, col].fillna(fill_val)\n\n        print(f\" Filled structure-dependent missing values in {df_name} for {zero_floor_mask.sum()} rows\")\n    else:\n        print(f\" 'floor_area_total_2019' not found in {df_name}\")\n\nFeature Drop Based on Sparse Signals: For features that appeared largely irrelevant or unused across the dataset, we calculated the percentage of zero values in columns such as mobile home area, deck area, and porch area. If a column contained over 90% zeros, it was considered non-informative and dropped from the modeling pipeline to reduce dimensionality and noise.\n\necho: true\noutput: false\ncollapse: true\ncols_to_drop = [col for col in train_merged.columns if col.startswith(\"mobile_home_area\")]\n\n# Drop from both sets\ntrain_merged.drop(columns=cols_to_drop, inplace=True)\ntest_merged.drop(columns=cols_to_drop, inplace=True)\n\nprint(f\" Dropped columns from train/test: {cols_to_drop}\")\n\nMulti-Level Median and Mode Imputation: After applying logic-based pruning, we used a three-level median imputation strategy for continuous features (e.g., assessed value 2017, building area 2017) based on the following hierarchy: - Level 1: neighborhood-level median - Level 2: region-level median - Level 3: global median (fallback)\n\necho: true\noutput: false\ncollapse: true\n# List of all assessed columns to impute\nassessed_cols = ['assessed_2015', 'assessed_2016', 'assessed_2017', 'assessed_2018']\n\nfor col in assessed_cols:\n    if col not in train_merged.columns:\n        continue\n\n    # Step 1: Compute medians from training data only\n    neigh_medians = train_merged.groupby('neighborhood')[col].median()\n    region_medians = train_merged.groupby('region')[col].median()\n    global_median = train_merged[col].median()\n\n    # Step 2: Train set imputation\n    train_merged[col] = train_merged.apply(\n        lambda row: neigh_medians[row['neighborhood']]\n        if pd.isna(row[col]) and row['neighborhood'] in neigh_medians else\n        region_medians[row['region']]\n        if pd.isna(row[col]) and row['region'] in region_medians else\n        global_median\n        if pd.isna(row[col]) else\n        row[col],\n        axis=1\n    )\n\n    # Step 3: Test set imputation (using train medians only)\n    test_merged[col] = test_merged.apply(\n        lambda row: neigh_medians.get(row['neighborhood'], np.nan)\n        if pd.isna(row[col]) else row[col],\n        axis=1\n    )\n    test_merged[col] = test_merged.apply(\n        lambda row: region_medians.get(row['region'], np.nan)\n        if pd.isna(row[col]) else row[col],\n        axis=1\n    )\n    test_merged[col].fillna(global_median, inplace=True)\n\n    print(f\" Imputed '{col}' using neighborhood ‚Üí region ‚Üí global medians (from training data)\")\n\nFor categorical variables such as foundation type or building condition, we applied single-level mode imputation using the most frequent category within the training data. While this approach is less localized, it provided a simple and stable method for handling missing values in features with low cardinality. ## 3.2 Neighborhood and Region-Level Statistical Features\nTo capture localized pricing dynamics and identify anomalies in property assessments, we engineered a suite of statistical features using the 2018 assessed values as a proxy for prior valuation context. We first computed neighborhood-level metrics including the mean, median, standard deviation, and interquartile range (IQR) of assessed 2018, grouped by neighborhood. Similarly, region-level statistics were computed using the region variable.\nUsing the merged and imputed stats, we computed derived features such as: - assess minus neigh mean: the raw deviation of a property‚Äôs 2018 assessed value from its neighborhood mean - assess ratio neigh mean: a normalized ratio of a property‚Äôs value to its local average - z score assess neigh: a z-score based on neighborhood-level variation - Corresponding region-level counterparts: assess minus region mean, assess ratio region mean, and z score assess region\nThese features helped contextualize each property‚Äôs assessed value relative to other properties within the same neighborhood or region. They proved useful in capturing outliers and potentially undervalued homes that deviated from local valuation patterns.\n\necho: true\noutput: false\ncollapse: true\n# === Step 1: Compute neighborhood-level stats ===\nneigh_stats = train_merged.groupby('neighborhood')['assessed_2018'].agg([\n    ('neigh_assess_mean', 'mean'),\n    ('neigh_assess_median', 'median'),\n    ('neigh_assess_std', 'std'),\n    ('neigh_assess_q1', lambda x: x.quantile(0.25)),\n    ('neigh_assess_q3', lambda x: x.quantile(0.75)),\n]).reset_index()\nneigh_stats['neigh_assess_iqr'] = neigh_stats['neigh_assess_q3'] - neigh_stats['neigh_assess_q1']\n\n# === Step 2: Compute region-level stats ===\nregion_stats = train_merged.groupby('region')['assessed_2018'].agg([\n    ('region_assess_mean', 'mean'),\n    ('region_assess_median', 'median'),\n    ('region_assess_std', 'std'),\n    ('region_assess_q1', lambda x: x.quantile(0.25)),\n    ('region_assess_q3', lambda x: x.quantile(0.75)),\n]).reset_index()\nregion_stats['region_assess_iqr'] = region_stats['region_assess_q3'] - region_stats['region_assess_q1']\n\n# === Step 3: Fallback std maps from training data ===\n# For neighborhood fallback, group region medians of neighborhood std\nneigh_std_by_region = neigh_stats.merge(train_merged[['neighborhood', 'region']], on='neighborhood', how='left') \\\n                                  .groupby('region')['neigh_assess_std'].median()\nglobal_neigh_std = neigh_stats['neigh_assess_std'].median()\n\nregion_std_by_neigh = region_stats.merge(train_merged[['neighborhood', 'region']], on='region', how='left') \\\n                                   .groupby('neighborhood')['region_assess_std'].median()\nglobal_region_std = region_stats['region_assess_std'].median()\n\n# === Step 4: Merge into train/test and compute features ===\nfor df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n    df = df.merge(neigh_stats, on='neighborhood', how='left')\n    df = df.merge(region_stats, on='region', how='left')\n\n    # Fill missing std values via fallback\n    df['neigh_assess_std'] = df['neigh_assess_std'].fillna(\n        df['region'].map(neigh_std_by_region)\n    ).fillna(global_neigh_std)\n\n    df['region_assess_std'] = df['region_assess_std'].fillna(\n        df['neighborhood'].map(region_std_by_neigh)\n    ).fillna(global_region_std)\n\n    # Compute derived features\n    df['assess_minus_neigh_mean'] = df['assessed_2018'] - df['neigh_assess_mean']\n    df['assess_ratio_neigh_mean'] = df['assessed_2018'] / (df['neigh_assess_mean'] + 1e-6)\n    df['z_score_assess_neigh'] = df['assess_minus_neigh_mean'] / (df['neigh_assess_std'] + 1e-6)\n\n    df['assess_minus_region_mean'] = df['assessed_2018'] - df['region_assess_mean']\n    df['assess_ratio_region_mean'] = df['assessed_2018'] / (df['region_assess_mean'] + 1e-6)\n    df['z_score_assess_region'] = df['assess_minus_region_mean'] / (df['region_assess_std'] + 1e-6)\n\n    # Save back\n    if df_name == 'train_merged':\n        train_merged = df\n    else:\n        test_merged = df\n\nprint(\" Completed: Stats merge + std fallback + z-score computation.\")"
  },
  {
    "objectID": "project3.html#frequency-encoding-of-high-cardinality-geographic-variables",
    "href": "project3.html#frequency-encoding-of-high-cardinality-geographic-variables",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.3 Frequency Encoding of High-Cardinality Geographic Variables",
    "text": "3.3 Frequency Encoding of High-Cardinality Geographic Variables\nTo convert high-cardinality categorical variables into numerical features while preserving signal strength, we applied frequency encoding to four key geographic identifiers: neighborhood, region, zone, and subneighborhood.\nThis encoding strategy served two purposes: 1. It allowed the model to retain information about how common or rare a spatial unit was. A frequently occurring neighborhood (i.e., one with high frequency) likely has more properties, which implies greater residential or commercial development in that area. 2. Areas with more properties are also likely to have more consistent and well-understood assessment patterns‚Äîthe government has ‚Äúseen‚Äù more properties there, which may reduce valuation volatility. These areas are more visible or prioritized in municipal processes.\nFinally, frequency encoding avoids the dimensional explosion caused by one-hot encoding, which is especially problematic for variables with high cardinality like neighborhood or subneighborhood.\n\necho: true\noutput: false\ncollapse: true\nfor col in ['neighborhood', 'region','zone','subneighborhood']:\n    if col in train_merged.columns:\n        # Step 1: Compute frequency from training data\n        freq_map = train_merged[col].value_counts(normalize=True)\n\n        # Step 2: Apply to both datasets\n        train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n        test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n\n        print(f\" Frequency encoded: {col} ‚Üí {col}_freq (based on training set)\")\n    else:\n        print(f\" Column '{col}' not found in training set\")"
  },
  {
    "objectID": "project3.html#boolean-and-ordinal-encoding",
    "href": "project3.html#boolean-and-ordinal-encoding",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.4 Boolean and Ordinal Encoding",
    "text": "3.4 Boolean and Ordinal Encoding\nBoolean Encoding: We focused on three boolean variables: has cooling, has heat, and protested. These features were encoded across all five years using binary values (0/1).\nOrdinal Encoding: For ordinal features such as quality, quality description, grade, building condition, and physical condition, we performed domain-informed cleaning and then applied ordinal encoding based on defined category hierarchies. Prior to encoding, raw values were standardized through column-specific replacements. For instance, extreme or ambiguous values like X, None, or overly granular subgrades (e.g., X-, E+) were either mapped to more interpretable categories or treated as missing. Some detailed conditions like Unsound and Very Poor were collapsed into broader categories such as Poor. Unknown values were handled gracefully by assigning an encoded fallback of -1. This process ensured that ordinal information was preserved in a numerically meaningful way, allowing models to leverage the ordered nature of these features without exploding dimensionality as one-hot encoding would.\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import KFold\n\n# Clear specific variables\nfor var in ['ordinal_cols_all', 'bool_cols_all']:\n    if var in locals():\n        del globals()[var]\n\n# === STEP 1: Boolean Encoding (2015‚Äì2019 only) ===\nbool_bases = ['has_cooling', 'has_heat', 'protested']\nbool_cols_all = [f\"{base}_{year}\" for base in bool_bases for year in range(2015, 2020)]\n\nfor col in bool_cols_all:\n    if col in train_merged.columns:\n        mode_val = train_merged[col].mode(dropna=True)[0]\n        train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n        test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n\n# === STEP 2: Ordinal Cleaning and Encoding (2015‚Äì2019 only) ===\nordinal_bases = [\n    'quality', 'quality_description', 'grade',\n    'building_condition', 'physical_condition'\n]\n\nordinal_cols_all = [f\"{base}_{year}\" for base in ordinal_bases for year in range(2015, 2020)]\n\n# Column-specific replacements\nreplacement_maps = {\n    'quality': {'E': 'D', 'F': 'D', 'X': np.nan, 'None': np.nan},\n    'quality_description': {'Poor': 'Very Low', 'None': np.nan},\n    'grade': {'X': 'F', 'X-': 'F', 'X+': 'F', 'E': 'D', 'E-': 'D-', 'E+': 'D+', 'None': np.nan},\n    'building_condition': {'Very Poor': 'Poor', 'Unsound': 'Poor', 'None': np.nan},\n    'physical_condition': {'Very Poor': 'Poor', 'Unsound': 'Poor', 'None': np.nan}\n}\n\n# Ordinal category order\nord_categories = {\n    'quality': ['D', 'C', 'B', 'A'],\n    'quality_description': ['Very Low', 'Low', 'Average', 'Good', 'Excellent', 'Superior'],\n    'grade': ['F', 'D-', 'D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'],\n    'building_condition': ['Poor', 'Fair', 'Average', 'Good', 'Very Good', 'Excellent'],\n    'physical_condition': ['Poor', 'Fair', 'Average', 'Good', 'Very Good', 'Excellent']\n}\n\n# Clean and encode\nfor base in ordinal_bases:\n    for year in range(2015, 2020):\n        col = f\"{base}_{year}\"\n        if col in train_merged.columns:\n            replacements = replacement_maps.get(base, {})\n            train_merged[col] = train_merged[col].replace(replacements)\n            test_merged[col] = test_merged[col].replace(replacements)\n\n            mode_val = train_merged[col].mode(dropna=True)[0]\n            train_merged[col] = train_merged[col].fillna(mode_val)\n            test_merged[col] = test_merged[col].fillna(mode_val)\n\n            encoder = OrdinalEncoder(categories=[ord_categories[base]], handle_unknown='use_encoded_value', unknown_value=-1)\n            train_merged[[col]] = encoder.fit_transform(train_merged[[col]])\n            test_merged[[col]] = encoder.transform(test_merged[[col]])"
  },
  {
    "objectID": "project3.html#target-encoding-of-nominal-categorical-variables-20152019",
    "href": "project3.html#target-encoding-of-nominal-categorical-variables-20152019",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.5 Target Encoding of Nominal Categorical Variables (2015‚Äì2019)",
    "text": "3.5 Target Encoding of Nominal Categorical Variables (2015‚Äì2019)\nFor certain nominal features that lacked ordinal structure but exhibited high cardinality, such as foundation type and exterior walls, we applied target encoding across all years from 2015 to 2019. This encoding replaces each category with a smoothed version of the mean target value (assessed 2018) observed for that category.\nTo avoid overfitting and data leakage, we implemented a 5-fold cross-validated target encoding procedure. For each fold, the mean target value was computed from the training portion and mapped to the validation fold. We used a smoothing parameter of 10 to balance the influence of the global mean versus the category-specific mean, especially for infrequent categories.\nThis method enabled us to capture predictive signal from nominal features without creating high-dimensional one-hot encodings or imposing artificial ordinal structure.\n\necho: true\noutput: false\ncollapse: true\n# === STEP 3: Target Encoding (2015‚Äì2019 only) ===\ndef group_and_target_encode_cv(train_df, test_df, target_name, column, rare_threshold=0.001, smoothing=10, n_splits=5):\n    freq = train_df[column].value_counts(normalize=True)\n    rare_cats = freq[freq &lt; rare_threshold].index\n    train_df[column] = train_df[column].replace(rare_cats, 'Other')\n    test_df[column] = test_df[column].replace(rare_cats, 'Other')\n\n    global_mean = train_df[target_name].mean()\n    oof_encoded = pd.Series(index=train_df.index, dtype='float64')\n\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    for train_idx, val_idx in kf.split(train_df):\n        X_tr, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n        stats = X_tr.groupby(column)[target_name].agg(['mean', 'count'])\n        smooth = (stats['mean'] * stats['count'] + global_mean * smoothing) / (stats['count'] + smoothing)\n        oof_encoded.iloc[val_idx] = X_val[column].map(smooth).fillna(global_mean)\n\n    final_stats = train_df.groupby(column)[target_name].agg(['mean', 'count'])\n    final_smooth = (final_stats['mean'] * final_stats['count'] + global_mean * smoothing) / (final_stats['count'] + smoothing)\n    test_encoded = test_df[column].map(final_smooth).fillna(global_mean)\n\n    return oof_encoded, test_encoded\n\n# Target-encodable nominal columns\ntarget_encodable_bases = ['foundation_type', 'exterior_walls']\ntarget_encodable_cols_all = [f\"{base}_{year}\" for base in target_encodable_bases for year in range(2015, 2020)]\n\n# Apply target encoding\nfor col in target_encodable_cols_all:\n    if col in train_merged.columns:\n        mode_val = train_merged[col].mode(dropna=True)[0]\n        train_merged[col] = train_merged[col].fillna(mode_val)\n        test_merged[col] = test_merged[col].fillna(mode_val)\n\n        train_merged[f'{col}_te'], test_merged[f'{col}_te'] = group_and_target_encode_cv(\n            train_merged, test_merged, target_name='assessed_2018', column=col,\n            rare_threshold=0.001, smoothing=10, n_splits=5\n        )\n\n        train_merged.drop(columns=[col], inplace=True)\n        test_merged.drop(columns=[col], inplace=True)"
  },
  {
    "objectID": "project3.html#quantile-binning-of-features",
    "href": "project3.html#quantile-binning-of-features",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.6 Quantile Binning of Features",
    "text": "3.6 Quantile Binning of Features\nTo enhance robustness and reduce sensitivity to outliers, we converted few continuous features into categorical bins using quantile-based binning. Growth metrics such as land value growth, building value growth, and assessed growth were binned into four quantiles, with thresholds computed only on the training data to prevent information leakage. If quantile binning failed due to low cardinality (e.g., repeated values), we defaulted to equal-width binning. All binned variables were explicitly cast as categorical to ensure compatibility with tree-based models.\nAdditionally, we binned year built final into five quantiles to capture generational differences in construction periods. This replaced raw year values with interpretable ordinal categories. Original continuous features were removed after binning to avoid redundancy and reduce multicollinearity.\n\necho: true\noutput: false\ncollapse: true\n# === Step 1: List your growth features ===\ngrowth_features = ['land_value_growth', 'building_value_growth', 'assessed_growth']\n\n# === Step 2: Binning Function (train-based binning) ===\ndef bin_growth_feature_safe(train_df, test_df, feature, bins=4):\n    try:\n        # Quantile binning on train only\n        train_df[f'{feature}_bin'], bin_edges = pd.qcut(train_df[feature], q=bins, labels=False, retbins=True, duplicates='drop')\n        test_df[f'{feature}_bin'] = pd.cut(test_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n    except ValueError:\n        # Fallback: Equal-width binning\n        min_val = train_df[feature].min()\n        max_val = train_df[feature].max()\n        bin_edges = np.linspace(min_val, max_val, bins + 1)\n        train_df[f'{feature}_bin'] = pd.cut(train_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n        test_df[f'{feature}_bin'] = pd.cut(test_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n\n    # Convert to category\n    train_df[f'{feature}_bin'] = train_df[f'{feature}_bin'].astype('category')\n    test_df[f'{feature}_bin'] = test_df[f'{feature}_bin'].astype('category')\n    return train_df, test_df\n\n# === Step 3: Apply to train_merged and test_merged ===\nfor feature in growth_features:\n    train_merged, test_merged = bin_growth_feature_safe(train_merged, test_merged, feature)\n\n# === Step 4: Bin year_built_final using train-based quantiles ===\ntrain_merged['year_built_bin'], bin_edges = pd.qcut(\n    train_merged['year_built_final'], q=5, retbins=True, labels=False, duplicates='drop'\n)\ntest_merged['year_built_bin'] = pd.cut(\n    test_merged['year_built_final'], bins=bin_edges, labels=False, include_lowest=True\n)\n\n# Convert to category\ntrain_merged['year_built_bin'] = train_merged['year_built_bin'].astype('category')\ntest_merged['year_built_bin'] = test_merged['year_built_bin'].astype('category')\n\n# === Step 5: Drop original continuous columns ===\ncols_to_drop = growth_features + ['year_built_final']\ntrain_merged.drop(columns=cols_to_drop, inplace=True)\ntest_merged.drop(columns=cols_to_drop, inplace=True)\n\nprint(\" Binned growth & year_built features safely with no leakage.\")"
  },
  {
    "objectID": "project3.html#rare-frequency-suppression-in-spatial-encodings",
    "href": "project3.html#rare-frequency-suppression-in-spatial-encodings",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.7 Rare Frequency Suppression in Spatial Encodings",
    "text": "3.7 Rare Frequency Suppression in Spatial Encodings\nFollowing frequency encoding of high-cardinality spatial variables (region, neighborhood, zone, subneighborhood), we applied a rare-value suppression step to mitigate the noise introduced by sparsely represented categories. For each frequency-encoded column, we identified values that occurred in less than 0.1% of the training data and replaced them with a neutral value of zero in both the training and test sets. This was done using thresholds derived solely from the training distribution to prevent data leakage.\nThe intuition behind this strategy is that extremely rare spatial groupings may not provide reliable or generalizable signals to the model. Treating them as a common fallback class (i.e., assigning them a frequency of zero) improves model stability and reduces overfitting to idiosyncratic, low-support locations. This transformation preserves the informativeness of frequent categories while smoothing out sparse tail behavior in the feature space.\n\necho: true\noutput: false\ncollapse: true\n# Define frequency columns and threshold\nfreq_cols = ['region_freq', 'neighborhood_freq', 'zone_freq', 'subneighborhood_freq']\nrare_thresh = 0.001\n\n# Apply rare value replacement for each frequency column\nfor col in freq_cols:\n    if col in train_merged.columns:\n        rare_vals = train_merged[col].value_counts(normalize=True)[lambda x: x &lt; rare_thresh].index\n        train_merged[col] = train_merged[col].replace(rare_vals, 0)\n        test_merged[col] = test_merged[col].replace(rare_vals, 0)\n        print(f\" Replaced rare values in {col} using train_merged threshold &lt; {rare_thresh}\")\n    else:\n        print(f\" Column {col} not found in train_merged ‚Äî skipping.\")"
  },
  {
    "objectID": "project3.html#log-transformation-and-distribution-smoothing-for-ridge-regression",
    "href": "project3.html#log-transformation-and-distribution-smoothing-for-ridge-regression",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.8 Log Transformation and Distribution Smoothing for Ridge Regression",
    "text": "3.8 Log Transformation and Distribution Smoothing for Ridge Regression\nTo satisfy linear model assumptions and reduce skew-related distortion in Ridge regression, we applied a targeted log transformation to select continuous features. Specifically, we identified variables related to building size, land area, and valuation (e.g., building value 2019, land area 2018, neigh assess mean) whose skewness exceeded a threshold of 2.0 in the training set. For these features, we applied a log1p transformation, which effectively stabilized variance, compressed long-tailed distributions, and improved linear fit potential.\nThis transformation was particularly useful for the Ridge regression model, which benefits from normally distributed inputs and is sensitive to extreme values. By selectively applying log1p only to features with high skew, we preserved model interpretability while enhancing numerical stability and predictive performance.\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\n\n# === Step 1: Skew-based Log Transformation (2015‚Äì2019 only) ===\nlog_bases = [\n    'floor_area_total', 'porch_area', 'building_area', 'land_area',\n    'building_value', 'land_value', 'assessed'\n]\nneigh_stat_cols = [\n    'neigh_assess_mean', 'neigh_assess_std', 'neigh_assess_median',\n    'neigh_assess_q1', 'neigh_assess_q3'\n]\n\n# Collect log-transformable columns (2015‚Äì2019 + neighborhood stats)\nlog_transform_cols = [f\"{base}_{year}\" for base in log_bases for year in range(2015, 2020)]\nlog_transform_cols += neigh_stat_cols\n\n# Compute skewness on train and apply log1p only if skew &gt; 2\nfor col in log_transform_cols:\n    if col in train_merged.columns:\n        skew = train_merged[col].skew()\n        if skew &gt; 2:\n            for df in [train_merged, test_merged]:\n                df[f\"log_{col}\"] = np.log1p(df[col])\n            print(f\" Log-transformed: {col} (skew={skew:.2f})\")\n        else:\n            print(f\"‚Ñπ Skipped: {col} (skew={skew:.2f})\")"
  },
  {
    "objectID": "project3.html#adaptive-quantile-clipping-for-tree-based-models",
    "href": "project3.html#adaptive-quantile-clipping-for-tree-based-models",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.9 Adaptive Quantile Clipping for Tree-Based Models",
    "text": "3.9 Adaptive Quantile Clipping for Tree-Based Models\nTo further control the influence of extreme values in tree-based models, we implemented an adaptive quantile clipping strategy informed by skewness severity. Using a precomputed skewness report, we categorized numeric features (excluding binary and target-encoded variables) into two groups: ultra-skewed (skewness &gt; 100) and moderately-skewed (2 &lt; skewness ‚â§ 100). Features were considered only if they had more than ten unique values and were not binary.\nFor ultra-skewed features, we applied clipping at the 0.5th and 99.5th percentiles. For moderately skewed features, we clipped at the 0.1st and 99.9th percentiles. All thresholds were derived solely from the training data and applied to both training and test sets to ensure leakage-free transformations. This clipping procedure helped suppress extreme values that might otherwise dominate decision paths or split criteria in tree-based learners.\nThese transformations were specifically designed for use with XGBoost and LightGBM, where reducing the influence of outliers improves model generalization and enhances interpretability in leaf-based decision structures.\n\necho: true\noutput: false\ncollapse: true\nimport pandas as pd\n\n\n# === Step 1: Categorize features by skew level ===\nultra_skewed = []\nmoderately_skewed = []\n\nfor _, row in skew_df.iterrows():\n    feature = row['feature']\n    skew = row['skewness']\n    \n    if feature not in train_merged.columns:\n        continue\n\n    unique_vals = train_merged[feature].nunique()\n    is_binary = set(train_merged[feature].dropna().unique()).issubset({0, 1})\n\n    if unique_vals &gt; 10 and not is_binary and not feature.endswith('_te'):\n        if skew &gt; 100:\n            ultra_skewed.append(feature)\n        elif 2&lt; skew &lt;= 100:\n            moderately_skewed.append(feature)\n\nprint(f\" {len(ultra_skewed)} ultra-skewed features to clip at 0.995.\")\nprint(f\" {len(moderately_skewed)} moderately-skewed features to clip at 0.999.\")\n\n# === Step 2: Compute quantile clipping bounds ===\nclip_bounds = {}\n\nfor col in ultra_skewed:\n    clip_bounds[col] = (\n        train_merged[col].quantile(0.005),\n        train_merged[col].quantile(0.995)\n    )\n\nfor col in moderately_skewed:\n    clip_bounds[col] = (\n        train_merged[col].quantile(0.001),\n        train_merged[col].quantile(0.999)\n    )\n\n# === Step 3: Apply clipping to both train and test ===\nfor df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n    for col, (lower, upper) in clip_bounds.items():\n        if col in df.columns:\n            df[col] = df[col].clip(lower, upper)\n\nprint(\" Adaptive clipping applied: 0.995 for ultra-skewed, 0.999 for moderately-skewed features.\")"
  },
  {
    "objectID": "project3.html#interaction-features-for-linear-and-nonlinear-models",
    "href": "project3.html#interaction-features-for-linear-and-nonlinear-models",
    "title": "STA 9890 Project: Property Valuation",
    "section": "3.10 Interaction Features for Linear and Nonlinear Models",
    "text": "3.10 Interaction Features for Linear and Nonlinear Models\nTo enrich model expressiveness, we engineered a comprehensive set of interaction features used across both Ridge regression and tree-based models (XGBoost and LightGBM). These included multiplicative and ratio-based terms such as grade quality index, value per age, area x quality, and assess to neigh mean, capturing relationships between physical dimensions, valuation, quality, and neighborhood context. For Ridge regression, these features acted as implicit basis expansions‚Äîeffectively enabling the linear model to capture non-additive effects by introducing new combinations of input features.\nAdditionally, we created a specialized set of log-transformed interaction terms‚Äîsuch as log area x grade, log assess x age, and log value diff‚Äîused exclusively in the Ridge pipeline. These features helped linearize multiplicative relationships and reduce skew, improving fit under Ridge‚Äôs sensitivity to input distribution. Log-based interactions were excluded from tree models, which are inherently robust to skew and insensitive to monotonic transformations like log, as they rely only on the relative ordering of feature values when making splits.\n\necho: true\noutput: false\ncollapse: true\n# === Interaction Features for Ridge Regression ===\ndef add_features(df):\n    df = df.copy()\n\n    # === Ratio features ===\n    df['area_ratio'] = df['building_area_2019'] / (df['land_area_2019'] + 1)\n    df['porch_ratio'] = df['porch_area_2019'] / (df['building_area_2019'] + 1)\n    df['floor_density'] = df['floor_area_total_2019'] / (df['land_area_2019'] + 1)\n    df['log_build_density'] = df['log_building_area_2019'] - df['log_land_area_2019']\n    df['log_land_to_build_ratio'] = df['log_land_area_2019'] - df['log_building_area_2019']\n\n    df['value_ratio'] = df['building_value_2018'] / (df['land_value_2018'] + 1)\n    df['log_value_diff'] = df['log_building_value_2018'] - df['log_land_value_2018']\n    df['value_per_sqft'] = df['building_value_2018'] / (df['building_area_2019'] + 1)\n    df['price_per_sqft'] = df['assessed_2018'] / (df['building_area_2019'] + 1)\n\n    # === Bathroom & room structure ===\n    df['bathroom_score'] = df['full_bath_2019'] + 0.5 * df['half_bath_2019']\n    df['bathroom_density'] = df['bathroom_score'] / (df['total_rooms_2019'] + 1)\n    df['bedroom_ratio'] = df['bedrooms_2019'] / (df['total_rooms_2019'] + 1)\n    df['rooms_per_floor'] = df['total_rooms_2019'] / (df['floors_2019'] + 1)\n\n    # === Core interactions ===\n    df['bedrooms_x_floors'] = df['bedrooms_2019'] * df['floors_2019']\n    df['rooms_x_quality'] = df['total_rooms_2019'] * df['quality_2019']\n    df['log_area_x_grade'] = df['log_building_area_2019'] * df['grade_2019']\n    df['log_assess_x_age'] = df['log_assessed_2018'] * df['building_age']\n    df['assess_spread_neigh'] = df['log_neigh_assess_q3'] - df['log_neigh_assess_q1']\n    df['grade_quality_index'] = df['grade_2019'] * df['quality_2019']\n\n    # === Clean up ===\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    return df\n\n# === Apply to train and test ===\ntrain_merged = add_features(train_merged)\ntest_merged = add_features(test_merged)\n\n\necho: true\noutput: false\ncollapse: true\n# === Interaction Features for Tree models ===\ndef add_features(df):\n    df = df.copy()\n    \n    # === Ratio features ===\n    df['area_ratio'] = df['building_area_2019'] / (df['land_area_2019'] + 1)\n    df['porch_ratio'] = df['porch_area_2019'] / (df['building_area_2019'] + 1)\n    df['floor_density'] = df['floor_area_total_2019'] / (df['land_area_2019'] + 1)\n    \n    df['value_ratio'] = df['building_value_2018'] / (df['land_value_2018'] + 1)\n    df['value_per_sqft'] = df['building_value_2018'] / (df['building_area_2019'] + 1)\n    df['price_per_sqft'] = df['assessed_2018'] / (df['building_area_2019'] + 1)\n\n    # === Bathroom & room structure ===\n    df['bathroom_score'] = df['full_bath_2019'] + 0.5 * df['half_bath_2019']\n    df['bathroom_density'] = df['bathroom_score'] / (df['total_rooms_2019'] + 1)\n    df['bedroom_ratio'] = df['bedrooms_2019'] / (df['total_rooms_2019'] + 1)\n    df['rooms_per_floor'] = df['total_rooms_2019'] / (df['floors_2019'] + 1)\n\n    # === Core interactions ===\n    df['bedrooms_x_floors'] = df['bedrooms_2019'] * df['floors_2019']\n    df['rooms_x_quality'] = df['total_rooms_2019'] * df['quality_2019']\n    df['assess_x_age'] = df['assessed_2018'] * df['building_age']\n    df['grade_quality_index'] = df['grade_2019'] * df['quality_2019']\n\n    # === Selected high-signal interactions ===\n    df['area_x_quality'] = df['building_area_2019'] * df['quality_2019']\n    df['floor_area_x_grade'] = df['floor_area_total_2019'] * df['grade_2019']\n    df['value_to_neigh_median'] = df['building_value_2018'] / (df['neigh_assess_median'] + 1)\n    df['assess_to_neigh_mean'] = df['assessed_2018'] / (df['neigh_assess_mean'] + 1)\n    df['value_per_age'] = df['building_value_2018'] / (df['building_age'] + 1)\n\n    # === Clean up ===\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df = df.fillna(0)\n    \n    return df\n\n# === Apply to train and test sets ===\ntrain_merged = add_features(train_merged)\ntest_merged = add_features(test_merged)"
  },
  {
    "objectID": "project3.html#ridge-regression-with-cross-validation",
    "href": "project3.html#ridge-regression-with-cross-validation",
    "title": "STA 9890 Project: Property Valuation",
    "section": "4.1 Ridge Regression with Cross-Validation",
    "text": "4.1 Ridge Regression with Cross-Validation\nWe implemented a Ridge regression model using RidgeCV to automatically select the regularization strength Œ± through nested cross-validation. A 3-fold outer loop was used for estimating out-of-fold (OOF) performance, while each inner fold evaluated a grid of Œ± values ranging from 10‚Åª¬≥ to 10¬≤ on a logarithmic scale. Input features were standardized within a Pipeline using StandardScaler to ensure scale-invariant regression coefficients. The model selected a different optimal Œ± for each fold, reflecting local variance in validation behavior:\n\nFold 1 RMSE: 42,050.33‚ÄîBest Œ±: 2.1544\nFold 2 RMSE: 41,036.52‚ÄîBest Œ±: 27.8256\nFold 3 RMSE: 40,619.40‚ÄîBest Œ±: 0.5995\n\nThe final out-of-fold RMSE across all folds was 41,239.79, with an average best Œ± of approximately 10.1932. This indicates that moderate regularization consistently improved generalization across different training splits.\nWe saved both OOF predictions and test forecasts as NumPy arrays‚Äîridgecv_oof_preds.npy and ridgecv_test_preds.npy‚Äîfor later use in model ensembling. These stored outputs served as reliable building blocks for downstream blending and stacking strategies.\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\n# === STEP 2: Prepare training/test matrices ===\nX = train_merged.copy()\nX_test = test_merged.copy()\ny = pd.Series(y_train).values # use raw target (not log)\n\n# === STEP 3: RidgeCV pipeline ===\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nalphas = np.logspace(-3, 2, 10)\n\nridge_oof = np.zeros(len(X))\nridge_test_preds = np.zeros(len(X_test))\nbest_alphas = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n    print(f\"\\n Fold {fold+1}/5\")\n\n    X_train, y_train_fold = X.iloc[train_idx], y[train_idx]\n    X_val, y_val = X.iloc[val_idx], y[val_idx]\n\n    model = make_pipeline(\n        StandardScaler(),\n        RidgeCV(alphas=alphas, cv=3, scoring='neg_root_mean_squared_error')\n    )\n\n    model.fit(X_train, y_train_fold)\n    ridge_oof[val_idx] = model.predict(X_val)\n    ridge_test_preds += model.predict(X_test) / kf.get_n_splits()\n\n    best_alpha = model.named_steps['ridgecv'].alpha_\n    best_alphas.append(best_alpha)\n    \n    fold_rmse = root_mean_squared_error(y_val, ridge_oof[val_idx])\n    print(f\"Fold {fold+1} RMSE: {fold_rmse:,.2f} | Best alpha: {best_alpha:.4f}\")\n\n# === STEP 4: Final RMSE ===\nfinal_rmse = root_mean_squared_error(y, ridge_oof)\nprint(f\"\\n Final OOF RMSE (RidgeCV): {final_rmse:,.2f}\")\nprint(f\" Average best alpha across folds: {np.mean(best_alphas):.4f}\")\n\n# === STEP 5: Save predictions ===\nsubmission = pd.DataFrame({\n    \"ACCOUNT\": acct_test.values.ravel(),\n    \"TARGET\": ridge_test_preds\n})\nsubmission.to_csv(\"submission_ridgecv_pipeline.csv\", index=False)\nprint(\"\\n Saved: submission_ridgecv_pipeline.csv\")\n\n# === Optional: Save OOF & test preds for stacking or analysis ===\nnp.save(\"ridgecv_oof_preds.npy\", ridge_oof)\nnp.save(\"ridgecv_test_preds.npy\", ridge_test_preds)\nprint(\" Saved: ridgecv_oof_preds.npy and ridgecv_test_preds.npy\")"
  },
  {
    "objectID": "project3.html#tree-based-models-with-optuna-and-shap-gain-feature-selection",
    "href": "project3.html#tree-based-models-with-optuna-and-shap-gain-feature-selection",
    "title": "STA 9890 Project: Property Valuation",
    "section": "4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection",
    "text": "4.2 Tree-Based Models with Optuna and SHAP-Gain Feature Selection\nTo capture nonlinear interactions and leverage automatic handling of missing values and categorical splits, we trained two gradient boosting models: LightGBM and XGBoost. Both models followed a structured pipeline consisting of hyperparameter optimization using Optuna, followed by SHAP- and gain-based feature selection, and a final retraining on the selected features.\nStep 1: Hyperparameter Tuning with Optuna. For each model, we defined an Optuna objective that trained 3-fold cross-validated models using early stopping. We explored hyperparameter ranges tailored to each algorithm, with LightGBM using a native pruning callback and XGBoost leveraging XGBoostPruningCallback. During tuning, we stored the best out-of-fold (OOF) predictions across all trials to later use in ensembling.\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport lightgbm as lgb\nimport shap\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import root_mean_squared_error\nfrom optuna.integration import LightGBMPruningCallback\nfrom optuna.pruners import SuccessiveHalvingPruner\nfrom lightgbm import log_evaluation, early_stopping\n\n# === STEP 0: Setup Data ===\nX_full = train_merged.copy()\ny_full = pd.Series(y_train)\nX_test = test_merged.copy()\n\n# Detect categorical columns\ncat_cols = X_full.select_dtypes(include=['category', 'object']).columns.tolist()\nfor col in cat_cols:\n    X_full[col] = X_full[col].astype(\"category\")\n    X_test[col] = X_test[col].astype(\"category\")\n\nglobal_oof_preds = np.zeros(len(X_full))\nbest_score = float('inf')\n\n# === STEP 1: Define Optuna Objective ===\ndef objective(trial):\n    global global_oof_preds, best_score\n\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.025, 0.04, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 160, 220),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 7, 11),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 18, 30),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.65, 0.88),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.75),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1.0, 5.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 4.0, log=True),\n        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.15, 0.25),\n        \"verbose\": -1,\n        \"n_jobs\": -1,\n    }\n\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    val_rmse = []\n    oof_preds = np.zeros(len(X_full))\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n        X_train, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n        y_train_fold, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n\n        dtrain = lgb.Dataset(X_train, label=y_train_fold, categorical_feature=cat_cols)\n        dvalid = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_cols)\n\n        model = lgb.train(\n            params,\n            dtrain,\n            valid_sets=[dvalid],\n            num_boost_round=1000,\n            callbacks=[\n                early_stopping(stopping_rounds=100),\n                log_evaluation(period=100),\n                LightGBMPruningCallback(trial, \"rmse\")\n            ]\n        )\n\n        val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n        oof_preds[val_idx] = val_pred\n        val_rmse.append(root_mean_squared_error(y_val, val_pred))\n\n    mean_rmse = np.mean(val_rmse)\n    trial.set_user_attr(\"cv_rmse\", mean_rmse)\n\n    if mean_rmse &lt; best_score:\n        best_score = mean_rmse\n        global_oof_preds[:] = oof_preds\n\n    print(f\" Trial {trial.number} | CV RMSE: {mean_rmse:,.2f}\")\n    return mean_rmse\n\n# === STEP 2: Run Optuna ===\nstudy = optuna.create_study(\n    direction='minimize',\n    study_name='lgbm_study_final_with_shap',\n    storage='sqlite:///lgbm_study_final_with_shap.db',\n    load_if_exists=True,\n    pruner=SuccessiveHalvingPruner(min_resource=100, reduction_factor=2)\n)\nstudy.optimize(objective, n_trials=25, show_progress_bar=True)\n\nprint(\" Best RMSE:\", study.best_value)\nprint(\" Best Parameters:\", study.best_params)\nnp.save(\"oof_preds_lgbm.npy\", global_oof_preds)\nprint(\" Saved: oof_preds_lgbm.npy\")\n\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import root_mean_squared_error\nfrom optuna.integration import XGBoostPruningCallback\nfrom shap import TreeExplainer\n\n# === STEP 0: Prepare Data ===\nX_full = train_merged.copy()\ny_full = pd.Series(y_train)\nX_test = test_merged.copy()\n\nbin_cols = [\n    'building_value_growth_bin',\n    'assessed_growth_bin',\n    'land_value_growth_bin','year_built_bin'\n]\n\nfor col in bin_cols:\n    X_full[col] = X_full[col].cat.codes\n    X_test[col] = X_test[col].cat.codes\n\ncategorical_cols = X_full.select_dtypes(include='object').columns.tolist()\nX_full[categorical_cols] = X_full[categorical_cols].astype('category')\nX_test[categorical_cols] = X_test[categorical_cols].astype('category')\n\n# === Global OOF Tracker ===\nglobal_oof_preds = np.zeros(len(X_full))\nbest_score = float(\"inf\")\n\n# === STEP 1: Optuna Objective Function (No SHAP during tuning) ===\ndef objective(trial):\n    global global_oof_preds, best_score\n\n    params = {\n        \"objective\": \"reg:squarederror\",\n        \"eval_metric\": \"rmse\",\n        \"tree_method\": \"hist\",\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.047, 0.05, log=True),\n        \"max_depth\": 6,\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 11, 12),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.87, 0.89),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.74),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.30, 0.56, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.05, 0.11, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 1.1, 4.3),\n        \"n_estimators\": 1000,\n        \"n_jobs\": -1,\n        \"enable_categorical\": True,\n    }\n\n    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(X_full))\n    fold_rmse = []\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n        X_train, y_train_fold = X_full.iloc[train_idx], y_full.iloc[train_idx]\n        X_val, y_val = X_full.iloc[val_idx], y_full.iloc[val_idx]\n\n        model = XGBRegressor(\n            **params,\n            early_stopping_rounds=100,\n            callbacks=[XGBoostPruningCallback(trial, \"validation_0-rmse\"),\n                       ]\n        )\n        model.fit(X_train, y_train_fold, eval_set=[(X_val, y_val)], verbose=100)\n\n        val_pred = model.predict(X_val)\n        oof_preds[val_idx] = val_pred\n        fold_rmse.append(root_mean_squared_error(y_val, val_pred))\n\n    mean_rmse = np.mean(fold_rmse)\n    trial.set_user_attr(\"cv_rmse\", mean_rmse)\n\n    if mean_rmse &lt; best_score:\n        best_score = mean_rmse\n        global_oof_preds[:] = oof_preds\n\n    print(f\" Trial {trial.number} | CV RMSE: {mean_rmse:,.2f}\")\n    return mean_rmse\n\n# === STEP 2: Run Optuna ===\nstudy = optuna.create_study(\n    direction='minimize',\n    study_name='xgbreg_optuna_final_no_shap',\n    pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=100, reduction_factor=2)\n)\nstudy.optimize(objective, n_trials=25, show_progress_bar=True)\n\nprint(\" Best RMSE:\", study.best_value)\nprint(\" Best Parameters:\", study.best_params)\nnp.save(\"oof_preds_xgbreg.npy\", global_oof_preds)\nprint(\" Saved: oof_preds_xgbreg.npy\")\n\nStep 2: SHAP and Gain-Based Feature Selection. After tuning, we trained new LightGBM and XGBoost models using the best parameters on each fold of the training data. For each fold, we computed SHAP importance values and LightGBM/XGBoost gain importances. We retained features that collectively accounted for 95% of total importance in either SHAP or gain, and constructed a union of these high-signal features across all folds. This union was used to define the final reduced feature space for retraining.\n\necho: true\noutput: false\ncollapse: true\n# === STEP 3: SHAP + GAIN Feature Selection for LGBM ===\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nselected_feature_sets = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n    X_train_raw, y_train_fold = X_full.iloc[train_idx], y_full.iloc[train_idx]\n\n    train_dataset = lgb.Dataset(X_train_raw, label=y_train_fold, categorical_feature=cat_cols)\n    model_temp = lgb.train(\n        study.best_params,\n        train_dataset,\n        num_boost_round=200,\n        valid_sets=[train_dataset],\n        callbacks=[log_evaluation(period=100)] \n    )\n\n    # SHAP importance\n    explainer = shap.TreeExplainer(model_temp)\n    shap_values = explainer.shap_values(X_train_raw)\n    shap_df = pd.DataFrame(np.abs(shap_values), columns=X_train_raw.columns)\n    shap_importance = shap_df.mean().sort_values(ascending=False)\n    shap_cumsum = shap_importance.cumsum() / shap_importance.sum()\n    top_shap = shap_cumsum[shap_cumsum &lt;= 0.95].index.tolist()\n\n    # Gain importance\n    gain_importance = pd.Series(model_temp.feature_importance(importance_type='gain'), index=X_train_raw.columns)\n    gain_sorted = gain_importance.sort_values(ascending=False)\n    gain_cumsum = gain_sorted.cumsum() / gain_sorted.sum()\n    top_gain = gain_cumsum[gain_cumsum &lt;= 0.95].index.tolist()\n\n    selected_features = list(set(top_shap).union(set(top_gain)))\n    selected_feature_sets.append(selected_features)\n\n# === STEP 4: Final Feature Union ===\nfinal_union_features = list(set().union(*selected_feature_sets))\nprint(\" Final Union Feature Count:\", len(final_union_features))\n\n# Filter only those categorical columns that are in final features\nfiltered_cat_cols = [col for col in cat_cols if col in final_union_features]\n\n\necho: true\noutput: false\ncollapse: true\n# === STEP 3: Post-Optuna SHAP + Gain Feature Selection for XGBoost ===\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nselected_feature_sets = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full)):\n    X_train_raw, y_train_fold = X_full.iloc[train_idx], y_full.iloc[train_idx]\n\n    model_temp = XGBRegressor(**study.best_params, n_estimators=200)\n    model_temp.fit(X_train_raw, y_train_fold)\n\n    # === SHAP Importance ===\n    explainer = TreeExplainer(model_temp)\n    shap_values = explainer.shap_values(X_train_raw)\n    shap_df = pd.DataFrame(np.abs(shap_values), columns=X_train_raw.columns)\n    shap_importance = shap_df.mean().sort_values(ascending=False)\n    shap_cumsum = shap_importance.cumsum() / shap_importance.sum()\n    top_shap = shap_cumsum[shap_cumsum &lt;= 0.95].index.tolist()\n\n    # === Gain Importance ===\n    gain_importance = pd.Series(model_temp.feature_importances_, index=X_train_raw.columns)\n    gain_sorted = gain_importance.sort_values(ascending=False)\n    gain_cumsum = gain_sorted.cumsum() / gain_sorted.sum()\n    top_gain = gain_cumsum[gain_cumsum &lt;= 0.95].index.tolist()\n\n    selected_features = list(set(top_shap).union(set(top_gain)))\n    selected_feature_sets.append(selected_features)\n\n# === STEP 4: Final Feature Union ===\nfinal_union_features = list(set().union(*selected_feature_sets))\nprint(\" Final Union Feature Count:\", len(final_union_features))\n\nStep 3: Final Model Training and Inference. Each final model was retrained on the full training set using only the selected features, with early stopping enabled to prevent overfitting. Predictions on the test set were generated using the best iteration count. All model outputs‚Äîincluding OOF predictions and test forecasts‚Äîwere saved for submission and later use in ensemble blending.\n\necho: true\noutput: false\ncollapse: true\n# === STEP 5: Final Model on Selected Features for LGBM ===\nX_full_selected = X_full[final_union_features]\nX_test_selected = X_test[final_union_features]\n\n\nfinal_dataset = lgb.Dataset(X_full_selected, label=y_full, categorical_feature=filtered_cat_cols)\nfinal_model = lgb.train(\n    study.best_params,\n    final_dataset,\n    num_boost_round=1000,\n    valid_sets=[final_dataset],\n    valid_names=[\"train\"],\n    callbacks=[log_evaluation(period=100)]\n)\n\n# === STEP 6: Predict on Test Set ===\ntest_preds = final_model.predict(X_test_selected, num_iteration=final_model.best_iteration)\nnp.save(\"test_preds_lgbm_shap.npy\", test_preds)\nprint(\" Saved: test_preds_lgbm_shap.npy\")\n\n# === STEP 7: Save Submission ===\nsubmission = pd.DataFrame({\n    'ACCOUNT': acct_test.values.ravel(),  # Replace with your ID col\n    'TARGET': test_preds\n})\nsubmission.to_csv(\"submission_lgbm_shap.csv\", index=False)\nprint(\" Submission saved: submission_lgbm_shap.csv\")\n\n\necho: true\noutput: false\ncollapse: true\n# === STEP 5: Final Model on Selected Features for XGBoost ===\nX_full_selected = X_full[final_union_features]\nX_test_selected = X_test[final_union_features]\n\nfinal_model = XGBRegressor(**study.best_params)\nfinal_model.set_params(n_estimators=1000, verbosity=1, early_stopping_rounds=100)\nfinal_model.fit(X_full_selected, y_full, eval_set=[(X_full_selected, y_full)], verbose=100)\n\n# === STEP 6: Predict on Test Set ===\ntest_preds = final_model.predict(X_test_selected)\nnp.save(\"test_preds_xgbreg.npy\", test_preds)\nprint(\" Saved: test_preds_xgbreg.npy\")\n\n# === STEP 7: Create Submission File ===\naccount_ids = acct_test.values.ravel()  # Replace with actual ID column\nsubmission = pd.DataFrame({\n    'ACCOUNT': account_ids,\n    'TARGET': test_preds\n})\nsubmission.to_csv(\"submission_xgbreg.csv\", index=False)\nprint(\" Submission saved: submission_xgbreg.csv\")\n\nThis hybrid approach‚Äîcombining Optuna-based tuning with SHAP-driven interpretability‚Äîallowed us to retain only high-impact features, thereby improving generalization and reducing overfitting without sacrificing performance. The best out-of-fold RMSE achieved was 40,925.29 with XGBoost and 41,641.42 with LightGBM, confirming the robustness of both pipelines."
  },
  {
    "objectID": "project3.html#weighted-model-blending-with-optuna",
    "href": "project3.html#weighted-model-blending-with-optuna",
    "title": "STA 9890 Project: Property Valuation",
    "section": "5.1 Weighted Model Blending with Optuna",
    "text": "5.1 Weighted Model Blending with Optuna\nTo consolidate the strengths of our top-performing base models‚ÄîXGBoost, RidgeCV, and LightGBM‚Äîwe employed a weighted blending strategy optimized using Optuna. This approach directly searched for the optimal linear combination of model predictions that minimized RMSE on a holdout set.\nWe first constructed a meta-training set consisting of out-of-fold (OOF) predictions from each base model. A corresponding test matrix was constructed from each model‚Äôs final test predictions. The blending weights were constrained to be non-negative and normalized to sum to one.\nAn Optuna study was run for 100 trials, where each trial proposed a new set of blending weights and evaluated their performance via RMSE on the holdout split. The final optimized weights were:\n\nXGBoost: w‚ÇÄ = 25.98%\nRidgeCV: w‚ÇÅ = 33.53%\nLightGBM: w‚ÇÇ = 40.49%\n\nThese weights were then used to produce a final blended prediction for the test set. The resulting predictions achieved an RMSE of 36,239.91 on the holdout set‚Äîoutperforming all individual base models and demonstrating the value of combining linear and tree-based perspectives.\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\nimport pandas as pd\nimport optuna\nimport logging\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# === Setup Logging ===\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(\"OptunaBlender\")\noptuna.logging.set_verbosity(optuna.logging.INFO)\n\n# === Load base model predictions ===\noof_xgb = np.load(\"oof_preds_xgbreg.npy\")\nridge_oof = np.load(\"ridgecv_oof_preds.npy\")\noof_lgb=np.load(\"oof_preds_lgbm.npy\")\ntest_xgb = np.load(\"test_preds_xgbreg.npy\")\nridge_test_preds = np.load(\"ridgecv_test_preds.npy\")\ntest_lgb=np.load(\"test_preds_lgbm_shap.npy\")\n# === Targets and prediction stack ===\ny_meta = train['TARGET'].values\nX_base = np.vstack([oof_xgb, ridge_oof,oof_lgb]).T\nX_test_base = np.vstack([test_xgb, ridge_test_preds,test_lgb]).T\n\n# === Holdout split ===\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_base, y_meta, test_size=0.2, random_state=42)\n\n# === Objective Function ===\ndef objective(trial):\n    weights = [trial.suggest_float(f\"w{i}\", 0, 1) for i in range(X_train.shape[1])]\n    weights = np.array(weights)\n    weights /= weights.sum()  # normalize\n\n    preds = X_holdout @ weights\n    rmse = root_mean_squared_error(y_holdout, preds)\n\n    logger.info(f\"Trial {trial.number} | Weights: {np.round(weights, 3).tolist()} | RMSE: {rmse:,.4f}\")\n    return rmse\n\n# === Run Study ===\nlogger.info(\" Starting Optuna optimization for weighted blending...\")\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\n\n# === Best weights ===\nbest_weights = np.array([study.best_trial.params[f\"w{i}\"] for i in range(X_base.shape[1])])\nbest_weights /= best_weights.sum()\nlogger.info(f\" Best weights: {np.round(best_weights, 4)}\")\nlogger.info(f\" Best RMSE: {study.best_value:.4f}\")\n\n# === Final test prediction ===\nmeta_preds = X_test_base @ best_weights\n\n# === Save predictions ===\nnp.save(\"test_preds_optuna_blended.npy\", meta_preds)\naccount_ids = acct_test.values.ravel()\nsubmission = pd.DataFrame({\n    \"ACCOUNT\": account_ids,\n    \"TARGET\": meta_preds\n})\nsubmission.to_csv(\"submission_optuna_blended.csv\", index=False)\nlogger.info(\" Saved: test_preds_optuna_blended.npy and submission_optuna_blended.csv\")"
  },
  {
    "objectID": "project3.html#stacked-ensembling-with-elasticnetcv",
    "href": "project3.html#stacked-ensembling-with-elasticnetcv",
    "title": "STA 9890 Project: Property Valuation",
    "section": "5.2 Stacked Ensembling with ElasticNetCV",
    "text": "5.2 Stacked Ensembling with ElasticNetCV\nTo complement our Optuna-based weighted average ensemble, we implemented a stacked generalization approach using ElasticNetCV as a meta-learner. This method treats out-of-fold (OOF) predictions from the base models‚ÄîXGBoost, RidgeCV, and LightGBM‚Äîas features in a second-level regression model. By learning how to optimally combine base predictions, the meta-model can capture nonlinear inter-model relationships while applying regularization to prevent overfitting.\nMeta-Model Training. We concatenated the OOF predictions into a 3-column meta-feature matrix and used it to fit an ElasticNetCV model wrapped in a StandardScaler pipeline. The meta-model searched over a grid of l1_ratio and alpha values, using 3-fold cross-validation to identify the optimal regularization configuration.\nHoldout Evaluation. For evaluation, we trained the meta-learner on an 80% split and evaluated on a 20% holdout. The resulting RMSE was 36,344.64, closely aligned with the Optuna-weighted blend. The selected alpha was 0.01, indicating strong regularization and robust coefficient shrinkage.\nFinal Test Predictions. The final model was retrained on the full meta-feature set and used to predict the test set. This stacked approach provided a robust and regularized alternative to linear averaging, automatically downweighting weaker models while maintaining interpretability and reproducibility.\nOn the competition leaderboard, the ElasticNetCV ensemble‚Äîcombining Ridge, XGBoost, and LightGBM predictions‚Äîsecured the top rank with a private leaderboard RMSE of 36,021, just 2 points above the public leaderboard score of 36,019. Interestingly, while an Optuna-weighted linear blend achieved a lower public RMSE, it ranked below the ElasticNet ensemble on the final evaluation, underscoring the latter‚Äôs generalization strength.\n\necho: true\noutput: false\ncollapse: true\nimport numpy as np\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import root_mean_squared_error\n\n# === Load OOF + Test Predictions ===\noof_xgb = np.load(\"oof_preds_xgbreg.npy\")\ntest_xgb = np.load(\"test_preds_xgbreg.npy\")\nridge_oof=np.load(\"ridgecv_oof_preds.npy\")\nridge_test_preds=np.load(\"ridgecv_test_preds.npy\")\noof_lgb=np.load(\"oof_preds_lgbm.npy\")\ntest_lgb=np.load(\"test_preds_lgbm_shap.npy\")\n\n# === 3. Combine full meta-input feature set ===\nX_meta = np.hstack([\n    oof_xgb.reshape(-1, 1),\n    ridge_oof.reshape(-1, 1),\n    oof_lgb.reshape(-1,1)\n])\ny_meta = train['TARGET'].values\n\nX_meta_test = np.hstack([\n    test_xgb.reshape(-1, 1),\n    ridge_test_preds.reshape(-1, 1),\n    test_lgb.reshape(-1,1)\n])\n\n# === 4. Train ElasticNetCV meta-learner ===\nmeta_model = make_pipeline(\n    StandardScaler(),\n    ElasticNetCV(\n        l1_ratio=[0.1, 0.5, 0.9, 1],\n        alphas=np.logspace(-4, 2, 100),\n        cv=3,\n        max_iter=5000,\n        n_jobs=-1\n    )\n)\nmeta_model.fit(X_meta, y_meta)\n\n# === 5. Predict and evaluate (optional holdout split) ===\n# You can skip this section if you're blending on full train\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)\nmeta_model.fit(X_train, y_train)\nholdout_preds = meta_model.predict(X_holdout)\nrmse = root_mean_squared_error(y_holdout, holdout_preds)\nprint(f\"ElasticNetCV Blended Meta Holdout RMSE: {rmse:,.2f}\")\nbest_alpha = meta_model.named_steps['elasticnetcv'].alpha_\nprint(f\" Best alpha selected: {best_alpha}\")\n# === 6. Final predictions for test set ===\nmeta_preds = meta_model.predict(X_meta_test)\n\n# === 7. Save blended test predictions ===\nnp.save(\"test_preds_elasticnet_blended.npy\", meta_preds)\naccount_ids = acct_test.values.ravel() \nsubmission = pd.DataFrame({\n    \"ACCOUNT\": account_ids,  # Replace with your actual ID column\n    \"TARGET\": meta_preds\n})\nsubmission.to_csv(\"submission_elasticnet_blended.csv\", index=False)\nprint(\" ElasticNetCV blended stacking submission saved.\")"
  },
  {
    "objectID": "project3.html#top-predictive-features",
    "href": "project3.html#top-predictive-features",
    "title": "STA 9890 Project: Property Valuation",
    "section": "Top Predictive Features",
    "text": "Top Predictive Features\nSHAP analysis revealed that features like assessed 2018, building value 2018, and land value 2018 were primary drivers of the model‚Äôs predictions. Structural attributes such as building area 2019, floor area x grade, and grade 2019 also carried strong explanatory power. These results confirmed the value of engineering ratio and interaction terms that encode economic density, build quality, and age-adjusted valuation. Neighborhood-level variables, especially neigh assess std and frequency encodings, further demonstrated the importance of local context in real estate assessment."
  },
  {
    "objectID": "project3.html#low-impact-features",
    "href": "project3.html#low-impact-features",
    "title": "STA 9890 Project: Property Valuation",
    "section": "Low-Impact Features",
    "text": "Low-Impact Features\nAlthough SHAP-ranked bottom 30 features showed limited average contribution, they were retained via the 95% SHAP + Gain union due to their potential complementary value. Examples include early-year indicators like quality description 2015, fireplaces 2016, and spatial ratios like porch ratio. Their inclusion likely enhanced generalization by supporting edge cases, and their low impact helped confirm that more aggressive feature pruning would have offered little gain. ::: {.columns} ::: {.column width=‚Äú50%‚Äù}  :::\n\n\n\n\nSHAP Top 30\n\n\n\n:::"
  },
  {
    "objectID": "project3.html#what-worked-well",
    "href": "project3.html#what-worked-well",
    "title": "STA 9890 Project: Property Valuation",
    "section": "What Worked Well",
    "text": "What Worked Well\n\nThe SHAP + Gain union was highly effective in denoising the feature space and avoiding overfitting, retaining only high-impact predictors across folds.\nThe ElasticNet ensemble combined linear and non-linear model strengths to capture nuanced patterns in both well-represented and sparse regions.\nOptuna tuning reduced manual trial-and-error and consistently improved generalization across Ridge, LGBM, and XGBoost pipelines."
  },
  {
    "objectID": "project3.html#what-could-improve",
    "href": "project3.html#what-could-improve",
    "title": "STA 9890 Project: Property Valuation",
    "section": "What Could Improve",
    "text": "What Could Improve\n\nWe did not model temporal dependencies across yearly features‚Äîtreating them as time-series sequences or using transformer architectures may better account for evolving valuation dynamics over time.\nGeographical variation was only partially captured using frequency encoding. Future work could explore residual stacking or blended stacking techniques with subgroup-aware features (e.g., neighborhood frequency, protest history) to correct localized error patterns more explicitly.\n\nOverall, the final ElasticNet ensemble‚Äîintegrating Ridge, SHAP-informed LGBM, and Optuna-tuned XGBoost‚Äîdelivered a strong performance on the private leaderboard. Future extensions may benefit from incorporating time-aware modeling, subgroup-specific residual correction, or spatially informed representations to further improve accuracy and fairness in municipal assessment systems."
  }
]