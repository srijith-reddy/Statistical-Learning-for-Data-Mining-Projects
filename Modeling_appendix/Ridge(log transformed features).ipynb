{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975dfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2274541461.py:18: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(folder_path + f'building_details_{year}.csv')\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2274541461.py:30: DtypeWarning: Columns (21,23,25,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(folder_path + 'assessment_history_test.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== Load Data =====\n",
    "folder_path = Path(\"data\")  # Make sure there's a 'data/' folder next to this script\n",
    "\n",
    "building_years = []\n",
    "for year in range(2015, 2020):\n",
    "    df = pd.read_csv(folder_path / f'building_details_{year}.csv')  # FIXED\n",
    "    df['year'] = year\n",
    "    building_years.append(df)\n",
    "\n",
    "building_all = pd.concat(building_years, ignore_index=True)\n",
    "building_all = building_all.drop_duplicates(subset=['acct', 'year'], keep='first')\n",
    "pivoted = building_all.pivot(index='acct', columns='year')\n",
    "pivoted.columns = [f'{col}_{year}' for col, year in pivoted.columns]\n",
    "pivoted = pivoted.reset_index()\n",
    "\n",
    "train = pd.read_csv(folder_path / 'assessment_history_train.csv')  # FIXED\n",
    "test = pd.read_csv(folder_path / 'assessment_history_test.csv')    # FIXED\n",
    "\n",
    "train_merged = train.merge(pivoted, on='acct', how='left')\n",
    "test_merged = test.merge(pivoted, on='acct', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22732988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped 4 columns from train (including protested_2019 if present):\n",
      "['building_value_2019', 'land_value_2019', 'assessed_2019', 'protested_2019']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify all '2019' columns in train\n",
    "train_2019_cols = [col for col in train_merged.columns if '2019' in col]\n",
    "\n",
    "# Step 2: Find which 2019 columns are missing in test\n",
    "train_only_2019_cols = [col for col in train_2019_cols if col not in test_merged.columns]\n",
    "\n",
    "# Step 3: Always drop 'protested_2019' due to leakage risk\n",
    "train_only_2019_cols.append('protested_2019')\n",
    "\n",
    "# Step 4: Drop the identified columns from train\n",
    "train_merged = train_merged.drop(columns=train_only_2019_cols, errors='ignore')\n",
    "\n",
    "# Step 5: Optional logging\n",
    "print(f\"Dropped {len(train_only_2019_cols)} columns from train (including protested_2019 if present):\")\n",
    "print(train_only_2019_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b308d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è 'protested_2019' not found in train_merged\n",
      "‚úÖ Dropped 'protested_2019' from test_merged\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    if 'protested_2019' in df.columns:\n",
    "        df.drop(columns='protested_2019', inplace=True)\n",
    "        print(f\"Dropped 'protested_2019' from {df_name}\")\n",
    "    else:\n",
    "        print(f\"'protested_2019' not found in {df_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ff519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò 2019 Columns in train_merged:\n",
      " - bedrooms_2019\n",
      " - building_area_2019\n",
      " - building_condition_2019\n",
      " - deck_area_2019\n",
      " - elevator_2019\n",
      " - exterior_walls_2019\n",
      " - fireplaces_2019\n",
      " - floor_area_lower_2019\n",
      " - floor_area_primary_2019\n",
      " - floor_area_upper_2019\n",
      " - floors_2019\n",
      " - foundation_type_2019\n",
      " - full_bath_2019\n",
      " - garage_area_2019\n",
      " - grade_2019\n",
      " - half_bath_2019\n",
      " - has_cooling_2019\n",
      " - has_heat_2019\n",
      " - land_area_2019\n",
      " - mobile_home_area_2019\n",
      " - physical_condition_2019\n",
      " - porch_area_2019\n",
      " - quality_2019\n",
      " - quality_description_2019\n",
      " - total_rooms_2019\n",
      " - year_built_2019\n",
      " - year_remodeled_2019\n",
      "\n",
      "üìô 2019 Columns in test_merged:\n",
      " - bedrooms_2019\n",
      " - building_area_2019\n",
      " - building_condition_2019\n",
      " - deck_area_2019\n",
      " - elevator_2019\n",
      " - exterior_walls_2019\n",
      " - fireplaces_2019\n",
      " - floor_area_lower_2019\n",
      " - floor_area_primary_2019\n",
      " - floor_area_upper_2019\n",
      " - floors_2019\n",
      " - foundation_type_2019\n",
      " - full_bath_2019\n",
      " - garage_area_2019\n",
      " - grade_2019\n",
      " - half_bath_2019\n",
      " - has_cooling_2019\n",
      " - has_heat_2019\n",
      " - land_area_2019\n",
      " - mobile_home_area_2019\n",
      " - physical_condition_2019\n",
      " - porch_area_2019\n",
      " - quality_2019\n",
      " - quality_description_2019\n",
      " - total_rooms_2019\n",
      " - year_built_2019\n",
      " - year_remodeled_2019\n"
     ]
    }
   ],
   "source": [
    "## Get all '2019' columns in train and test\n",
    "train_2019_cols = sorted([col for col in train_merged.columns if '2019' in col])\n",
    "test_2019_cols = sorted([col for col in test_merged.columns if '2019' in col])\n",
    "\n",
    "# Print train columns\n",
    "print(\"2019 Columns in train_merged:\")\n",
    "for col in train_2019_cols:\n",
    "    print(f\" - {col}\")\n",
    "\n",
    "print(\"\\n 2019 Columns in test_merged:\")\n",
    "for col in test_2019_cols:\n",
    "    print(f\" - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d9653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All 2019 columns match between train_merged and test_merged.\n"
     ]
    }
   ],
   "source": [
    "# Compare sets\n",
    "train_only_2019 = sorted(list(set(train_2019_cols) - set(test_2019_cols)))\n",
    "test_only_2019 = sorted(list(set(test_2019_cols) - set(train_2019_cols)))\n",
    "\n",
    "# Print comparison result\n",
    "if not train_only_2019 and not test_only_2019:\n",
    "    print(\"\\n All 2019 columns match between train_merged and test_merged.\")\n",
    "else:\n",
    "    print(\"\\n Mismatched 2019 columns found:\")\n",
    "\n",
    "    if train_only_2019:\n",
    "        print(\" In train_merged but not in test_merged:\")\n",
    "        for col in train_only_2019:\n",
    "            print(f\"   - {col}\")\n",
    "\n",
    "    if test_only_2019:\n",
    "        print(\" In test_merged but not in train_merged:\")\n",
    "        for col in test_only_2019:\n",
    "            print(f\"   - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb6dfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (628287, 37)\n",
      "Test shape: (418858, 33)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Store target separately\n",
    "y_train = train_merged[\"TARGET\"].values\n",
    "\n",
    "# Step 4: Confirm sizes\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28236192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top columns with missing data (based on training set only):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>building_area_2015</th>\n",
       "      <td>5.954126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land_area_2015</th>\n",
       "      <td>5.954126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protested_2015</th>\n",
       "      <td>5.954126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assessed_2015</th>\n",
       "      <td>5.954126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_value_2015</th>\n",
       "      <td>5.954126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land_value_2015</th>\n",
       "      <td>5.954126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_description_2015</th>\n",
       "      <td>5.742280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality_2015</th>\n",
       "      <td>5.742280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_remodeled_2015</th>\n",
       "      <td>5.742280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_built_2015</th>\n",
       "      <td>5.742280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building_condition_2015</th>\n",
       "      <td>5.566405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physical_condition_2015</th>\n",
       "      <td>5.566405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_heat_2015</th>\n",
       "      <td>5.566405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_cooling_2015</th>\n",
       "      <td>5.566405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foundation_type_2015</th>\n",
       "      <td>5.566246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exterior_walls_2015</th>\n",
       "      <td>5.566246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade_2015</th>\n",
       "      <td>5.566087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms_2015</th>\n",
       "      <td>5.391485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_bath_2015</th>\n",
       "      <td>5.391485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fireplaces_2015</th>\n",
       "      <td>5.391485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Missing Ratio (%)\n",
       "building_area_2015                 5.954126\n",
       "land_area_2015                     5.954126\n",
       "protested_2015                     5.954126\n",
       "assessed_2015                      5.954126\n",
       "building_value_2015                5.954126\n",
       "land_value_2015                    5.954126\n",
       "quality_description_2015           5.742280\n",
       "quality_2015                       5.742280\n",
       "year_remodeled_2015                5.742280\n",
       "year_built_2015                    5.742280\n",
       "building_condition_2015            5.566405\n",
       "physical_condition_2015            5.566405\n",
       "has_heat_2015                      5.566405\n",
       "has_cooling_2015                   5.566405\n",
       "foundation_type_2015               5.566246\n",
       "exterior_walls_2015                5.566246\n",
       "grade_2015                         5.566087\n",
       "bedrooms_2015                      5.391485\n",
       "full_bath_2015                     5.391485\n",
       "fireplaces_2015                    5.391485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of missing values in each column of TRAIN only\n",
    "train_na = (train_merged.isnull().sum() / train_merged.shape[0]) * 100\n",
    "\n",
    "# Drop columns with no missing values, sort the rest\n",
    "train_na = train_na[train_na > 0].sort_values(ascending=False)\n",
    "\n",
    "# Optional: Show top 200 missing features\n",
    "missing_data = pd.DataFrame({'Missing Ratio (%)': train_na})\n",
    "\n",
    "# Display\n",
    "print(\"Top columns with missing data (based on training set only):\")\n",
    "missing_data.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5543a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created: floor_area_total_2015\n",
      "‚úÖ Created: floor_area_total_2016\n",
      "‚úÖ Created: floor_area_total_2017\n",
      "‚úÖ Created: floor_area_total_2018\n",
      "‚úÖ Created: floor_area_total_2019\n",
      "‚úÖ Created: floor_area_total_2015\n",
      "‚úÖ Created: floor_area_total_2016\n",
      "‚úÖ Created: floor_area_total_2017\n",
      "‚úÖ Created: floor_area_total_2018\n",
      "‚úÖ Created: floor_area_total_2019\n"
     ]
    }
   ],
   "source": [
    "def create_floor_area_totals(df, years):\n",
    "    for y in years:\n",
    "        primary_col = f'floor_area_primary_{y}'\n",
    "        upper_col   = f'floor_area_upper_{y}'\n",
    "        lower_col   = f'floor_area_lower_{y}'\n",
    "        total_col   = f'floor_area_total_{y}'\n",
    "\n",
    "        if all(col in df.columns for col in [primary_col, upper_col, lower_col]):\n",
    "            df[total_col] = (\n",
    "                df[primary_col].fillna(0) +\n",
    "                df[upper_col].fillna(0) +\n",
    "                df[lower_col].fillna(0)\n",
    "            )\n",
    "            print(f\" Created: {total_col}\")\n",
    "        else:\n",
    "            print(f\" Skipping {total_col} ‚Äî one or more components missing.\")\n",
    "    return df\n",
    "\n",
    "# Apply for years 2015‚Äì2019\n",
    "years = ['2015', '2016', '2017', '2018', '2019']\n",
    "train_merged = create_floor_area_totals(train_merged, years)\n",
    "test_merged = create_floor_area_totals(test_merged, years)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e21f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä mobile_home_area_2015 is 0 in 94.42% of training rows\n"
     ]
    }
   ],
   "source": [
    "zero_pct = (train_merged['mobile_home_area_2015'] == 0).mean() * 100\n",
    "print(f\" mobile_home_area_2015 is 0 in {zero_pct:.2f}% of training rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf9005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped columns from train/test: ['mobile_home_area_2015', 'mobile_home_area_2016', 'mobile_home_area_2017', 'mobile_home_area_2018', 'mobile_home_area_2019']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [col for col in train_merged.columns if col.startswith(\"mobile_home_area\")]\n",
    "\n",
    "# Drop from both sets\n",
    "train_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "test_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\" Dropped columns from train/test: {cols_to_drop}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f14b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä deck_area_2015 is 0 in 90.66% of training rows\n"
     ]
    }
   ],
   "source": [
    "zero_pct = (train_merged['deck_area_2015'] == 0).mean() * 100\n",
    "print(f\" deck_area_2015 is 0 in {zero_pct:.2f}% of training rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376042eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped columns from train/test: ['deck_area_2015', 'deck_area_2016', 'deck_area_2017', 'deck_area_2018', 'deck_area_2019']\n"
     ]
    }
   ],
   "source": [
    "# Use training columns to identify which deck_area columns exist\n",
    "cols_to_drop = [col for col in train_merged.columns if col.startswith(\"deck_area\")]\n",
    "\n",
    "# Drop those columns from both datasets\n",
    "train_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "test_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\" Dropped columns from train/test: {cols_to_drop}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf0fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backfilled: garage_area across ['garage_area_2019', 'garage_area_2018', 'garage_area_2017', 'garage_area_2016', 'garage_area_2015']\n",
      "‚úÖ Backfilled: porch_area across ['porch_area_2019', 'porch_area_2018', 'porch_area_2017', 'porch_area_2016', 'porch_area_2015']\n",
      "‚úÖ Backfilled: garage_area across ['garage_area_2019', 'garage_area_2018', 'garage_area_2017', 'garage_area_2016', 'garage_area_2015']\n",
      "‚úÖ Backfilled: porch_area across ['porch_area_2019', 'porch_area_2018', 'porch_area_2017', 'porch_area_2016', 'porch_area_2015']\n"
     ]
    }
   ],
   "source": [
    "def backfill_yearly_features(df, base_features, years):\n",
    "    for feature in base_features:\n",
    "        cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n",
    "        if len(cols) >= 2:\n",
    "            # Backfill all relevant year columns in-place\n",
    "            df[cols] = df[cols].bfill(axis=1)\n",
    "            print(f\" Backfilled: {feature} across {cols}\")\n",
    "        else:\n",
    "            print(f\" Skipping {feature} ‚Äî not enough year columns found.\")\n",
    "    return df\n",
    "\n",
    "# Years in reverse so that bfill works from most recent (2019) to oldest (2015)\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "features = ['garage_area', 'porch_area']\n",
    "\n",
    "# Apply to both train and test\n",
    "train_merged = backfill_yearly_features(train_merged, features, years)\n",
    "test_merged = backfill_yearly_features(test_merged, features, years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f87495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backfilled: floors across ['floors_2019', 'floors_2018', 'floors_2017', 'floors_2016', 'floors_2015']\n",
      "‚úÖ Backfilled: half_bath across ['half_bath_2019', 'half_bath_2018', 'half_bath_2017', 'half_bath_2016', 'half_bath_2015']\n",
      "‚úÖ Backfilled: full_bath across ['full_bath_2019', 'full_bath_2018', 'full_bath_2017', 'full_bath_2016', 'full_bath_2015']\n",
      "‚úÖ Backfilled: total_rooms across ['total_rooms_2019', 'total_rooms_2018', 'total_rooms_2017', 'total_rooms_2016', 'total_rooms_2015']\n",
      "‚úÖ Backfilled: bedrooms across ['bedrooms_2019', 'bedrooms_2018', 'bedrooms_2017', 'bedrooms_2016', 'bedrooms_2015']\n",
      "‚úÖ Backfilled: floors across ['floors_2019', 'floors_2018', 'floors_2017', 'floors_2016', 'floors_2015']\n",
      "‚úÖ Backfilled: half_bath across ['half_bath_2019', 'half_bath_2018', 'half_bath_2017', 'half_bath_2016', 'half_bath_2015']\n",
      "‚úÖ Backfilled: full_bath across ['full_bath_2019', 'full_bath_2018', 'full_bath_2017', 'full_bath_2016', 'full_bath_2015']\n",
      "‚úÖ Backfilled: total_rooms across ['total_rooms_2019', 'total_rooms_2018', 'total_rooms_2017', 'total_rooms_2016', 'total_rooms_2015']\n",
      "‚úÖ Backfilled: bedrooms across ['bedrooms_2019', 'bedrooms_2018', 'bedrooms_2017', 'bedrooms_2016', 'bedrooms_2015']\n"
     ]
    }
   ],
   "source": [
    "def backfill_yearly_features(df, features, years):\n",
    "    for feature in features:\n",
    "        year_cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n",
    "        if len(year_cols) >= 2:\n",
    "            df[year_cols] = df[year_cols].bfill(axis=1)\n",
    "            print(f\" Backfilled: {feature} across {year_cols}\")\n",
    "        else:\n",
    "            print(f\" Skipped: Not enough year columns for '{feature}'\")\n",
    "    return df\n",
    "\n",
    "# Use years in reverse for proper backfill (newest to oldest)\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "features = ['floors', 'half_bath', 'full_bath', 'total_rooms', 'bedrooms']\n",
    "\n",
    "# Apply to both train and test\n",
    "train_merged = backfill_yearly_features(train_merged, features, years)\n",
    "test_merged = backfill_yearly_features(test_merged, features, years)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "461dc946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped elevator-related columns from train/test: ['elevator_2015', 'elevator_2016', 'elevator_2017', 'elevator_2018', 'elevator_2019']\n"
     ]
    }
   ],
   "source": [
    "# Identify elevator-related columns from training set\n",
    "elevator_cols = [col for col in train_merged.columns if col.startswith(\"elevator\")]\n",
    "\n",
    "# Drop from both train and test\n",
    "train_merged.drop(columns=elevator_cols, inplace=True)\n",
    "test_merged.drop(columns=elevator_cols, inplace=True)\n",
    "\n",
    "print(f\"üóëÔ∏è Dropped elevator-related columns from train/test: {elevator_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ccc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backfilled: fireplaces across ['fireplaces_2019', 'fireplaces_2018', 'fireplaces_2017', 'fireplaces_2016', 'fireplaces_2015']\n",
      "‚úÖ Backfilled: quality across ['quality_2019', 'quality_2018', 'quality_2017', 'quality_2016', 'quality_2015']\n",
      "‚úÖ Backfilled: quality_description across ['quality_description_2019', 'quality_description_2018', 'quality_description_2017', 'quality_description_2016', 'quality_description_2015']\n",
      "‚úÖ Created year_built_final from: ['year_built_2019', 'year_built_2018', 'year_built_2017', 'year_built_2016', 'year_built_2015']\n",
      "‚úÖ Backfilled: fireplaces across ['fireplaces_2019', 'fireplaces_2018', 'fireplaces_2017', 'fireplaces_2016', 'fireplaces_2015']\n",
      "‚úÖ Backfilled: quality across ['quality_2019', 'quality_2018', 'quality_2017', 'quality_2016', 'quality_2015']\n",
      "‚úÖ Backfilled: quality_description across ['quality_description_2019', 'quality_description_2018', 'quality_description_2017', 'quality_description_2016', 'quality_description_2015']\n",
      "‚úÖ Created year_built_final from: ['year_built_2019', 'year_built_2018', 'year_built_2017', 'year_built_2016', 'year_built_2015']\n"
     ]
    }
   ],
   "source": [
    "def backfill_year_features(df, features, years):\n",
    "    for feature in features:\n",
    "        cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n",
    "        if len(cols) >= 2:\n",
    "            df[cols] = df[cols].bfill(axis=1)\n",
    "            print(f\" Backfilled: {feature} across {cols}\")\n",
    "        else:\n",
    "            print(f\" Skipped {feature} ‚Äî not enough year-based columns found.\")\n",
    "    return df\n",
    "\n",
    "def create_year_built_final(df):\n",
    "    year_cols = [f\"year_built_{y}\" for y in ['2019', '2018', '2017', '2016', '2015'] if f\"year_built_{y}\" in df.columns]\n",
    "    if year_cols:\n",
    "        df['year_built_final'] = df[year_cols].bfill(axis=1).iloc[:, 0]\n",
    "        print(f\" Created year_built_final from: {year_cols}\")\n",
    "    else:\n",
    "        print(\" Skipped: no year_built_* columns found.\")\n",
    "    return df\n",
    "\n",
    "# Define reverse years for backfill (latest ‚Üí oldest)\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "features_to_backfill = ['fireplaces', 'quality', 'quality_description']\n",
    "\n",
    "# Apply to both train and test\n",
    "train_merged = backfill_year_features(train_merged, features_to_backfill, years)\n",
    "train_merged = create_year_built_final(train_merged)\n",
    "\n",
    "test_merged = backfill_year_features(test_merged, features_to_backfill, years)\n",
    "test_merged = create_year_built_final(test_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b931aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped year_remodeled-related columns from train/test: ['year_remodeled_2015', 'year_remodeled_2016', 'year_remodeled_2017', 'year_remodeled_2018', 'year_remodeled_2019']\n"
     ]
    }
   ],
   "source": [
    "## Identify columns to drop from training data\n",
    "cols_to_drop = [col for col in train_merged.columns if col.startswith(\"year_remodeled\")]\n",
    "\n",
    "# Drop from both train and test\n",
    "train_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "test_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"üóëÔ∏è Dropped year_remodeled-related columns from train/test: {cols_to_drop}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c18cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backfilled: building_condition across ['building_condition_2019', 'building_condition_2018', 'building_condition_2017', 'building_condition_2016', 'building_condition_2015']\n",
      "‚úÖ Backfilled: foundation_type across ['foundation_type_2019', 'foundation_type_2018', 'foundation_type_2017', 'foundation_type_2016', 'foundation_type_2015']\n",
      "‚úÖ Backfilled: grade across ['grade_2019', 'grade_2018', 'grade_2017', 'grade_2016', 'grade_2015']\n",
      "‚úÖ Backfilled: has_cooling across ['has_cooling_2019', 'has_cooling_2018', 'has_cooling_2017', 'has_cooling_2016', 'has_cooling_2015']\n",
      "‚úÖ Backfilled: has_heat across ['has_heat_2019', 'has_heat_2018', 'has_heat_2017', 'has_heat_2016', 'has_heat_2015']\n",
      "‚úÖ Backfilled: physical_condition across ['physical_condition_2019', 'physical_condition_2018', 'physical_condition_2017', 'physical_condition_2016', 'physical_condition_2015']\n",
      "‚úÖ Backfilled: exterior_walls across ['exterior_walls_2019', 'exterior_walls_2018', 'exterior_walls_2017', 'exterior_walls_2016', 'exterior_walls_2015']\n",
      "‚úÖ Backfilled: building_condition across ['building_condition_2019', 'building_condition_2018', 'building_condition_2017', 'building_condition_2016', 'building_condition_2015']\n",
      "‚úÖ Backfilled: foundation_type across ['foundation_type_2019', 'foundation_type_2018', 'foundation_type_2017', 'foundation_type_2016', 'foundation_type_2015']\n",
      "‚úÖ Backfilled: grade across ['grade_2019', 'grade_2018', 'grade_2017', 'grade_2016', 'grade_2015']\n",
      "‚úÖ Backfilled: has_cooling across ['has_cooling_2019', 'has_cooling_2018', 'has_cooling_2017', 'has_cooling_2016', 'has_cooling_2015']\n",
      "‚úÖ Backfilled: has_heat across ['has_heat_2019', 'has_heat_2018', 'has_heat_2017', 'has_heat_2016', 'has_heat_2015']\n",
      "‚úÖ Backfilled: physical_condition across ['physical_condition_2019', 'physical_condition_2018', 'physical_condition_2017', 'physical_condition_2016', 'physical_condition_2015']\n",
      "‚úÖ Backfilled: exterior_walls across ['exterior_walls_2019', 'exterior_walls_2018', 'exterior_walls_2017', 'exterior_walls_2016', 'exterior_walls_2015']\n"
     ]
    }
   ],
   "source": [
    "def backfill_categorical_year_features(df, features, years):\n",
    "    for feature in features:\n",
    "        year_cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n",
    "        if len(year_cols) >= 2:\n",
    "            df[year_cols] = df[year_cols].bfill(axis=1)\n",
    "            print(f\" Backfilled: {feature} across {year_cols}\")\n",
    "        else:\n",
    "            print(f\" Skipped: {feature} ‚Äî not enough year-based columns.\")\n",
    "    return df\n",
    "\n",
    "# Backfill from most recent year to oldest\n",
    "years = ['2019', '2018', '2017', '2016', '2015']\n",
    "features = ['building_condition', 'foundation_type', 'grade', 'has_cooling', \n",
    "            'has_heat', 'physical_condition', 'exterior_walls']\n",
    "\n",
    "# Apply to train and test\n",
    "train_merged = backfill_categorical_year_features(train_merged, features, years)\n",
    "test_merged = backfill_categorical_year_features(test_merged, features, years)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c31525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backfilled: building_area across ['building_area_2019', 'building_area_2018', 'building_area_2017', 'building_area_2016', 'building_area_2015']\n",
      "‚úÖ Backfilled: land_area across ['land_area_2019', 'land_area_2018', 'land_area_2017', 'land_area_2016', 'land_area_2015']\n",
      "‚úÖ Backfilled: building_value across ['building_value_2018', 'building_value_2017', 'building_value_2016', 'building_value_2015']\n",
      "‚úÖ Backfilled: land_value across ['land_value_2018', 'land_value_2017', 'land_value_2016', 'land_value_2015']\n",
      "‚úÖ Backfilled: assessed across ['assessed_2018', 'assessed_2017', 'assessed_2016', 'assessed_2015']\n",
      "‚úÖ Backfilled: building_area across ['building_area_2019', 'building_area_2018', 'building_area_2017', 'building_area_2016', 'building_area_2015']\n",
      "‚úÖ Backfilled: land_area across ['land_area_2019', 'land_area_2018', 'land_area_2017', 'land_area_2016', 'land_area_2015']\n",
      "‚úÖ Backfilled: building_value across ['building_value_2018', 'building_value_2017', 'building_value_2016', 'building_value_2015']\n",
      "‚úÖ Backfilled: land_value across ['land_value_2018', 'land_value_2017', 'land_value_2016', 'land_value_2015']\n",
      "‚úÖ Backfilled: assessed across ['assessed_2018', 'assessed_2017', 'assessed_2016', 'assessed_2015']\n"
     ]
    }
   ],
   "source": [
    "def backfill_year_columns(df, features, years):\n",
    "    for feature in features:\n",
    "        cols = [f\"{feature}_{y}\" for y in years if f\"{feature}_{y}\" in df.columns]\n",
    "        if len(cols) >= 2:\n",
    "            df[cols] = df[cols].bfill(axis=1)\n",
    "            print(f\" Backfilled: {feature} across {cols}\")\n",
    "        else:\n",
    "            print(f\" Skipped: {feature} ‚Äî not enough year-based columns found.\")\n",
    "    return df\n",
    "\n",
    "# Define year ranges\n",
    "area_years = ['2019', '2018', '2017', '2016', '2015']\n",
    "value_years = ['2018', '2017', '2016', '2015']\n",
    "\n",
    "# Define feature groups\n",
    "area_features = ['building_area', 'land_area']\n",
    "value_features = ['building_value', 'land_value', 'assessed']\n",
    "\n",
    "# Apply to train and test\n",
    "train_merged = backfill_year_columns(train_merged, area_features, area_years)\n",
    "train_merged = backfill_year_columns(train_merged, value_features, value_years)\n",
    "\n",
    "test_merged = backfill_year_columns(test_merged, area_features, area_years)\n",
    "test_merged = backfill_year_columns(test_merged, value_features, value_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backfilled: protested across ['protested_2018', 'protested_2017', 'protested_2016', 'protested_2015']\n",
      "‚úÖ Backfilled: protested across ['protested_2018', 'protested_2017', 'protested_2016', 'protested_2015']\n"
     ]
    }
   ],
   "source": [
    "def backfill_protested_columns(df, years):\n",
    "    cols = [f\"protested_{y}\" for y in years if f\"protested_{y}\" in df.columns]\n",
    "    if len(cols) >= 2:\n",
    "        df[cols] = df[cols].bfill(axis=1)\n",
    "        print(f\" Backfilled: protested across {cols}\")\n",
    "    else:\n",
    "        print(\" Skipped protested ‚Äî not enough year-based columns found.\")\n",
    "    return df\n",
    "\n",
    "# Only use pre-2019 years to avoid leakage\n",
    "years = ['2018', '2017', '2016', '2015']\n",
    "\n",
    "# Apply to both datasets\n",
    "train_merged = backfill_protested_columns(train_merged, years)\n",
    "test_merged = backfill_protested_columns(test_merged, years)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38927d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped 'acct' from train_merged\n",
      "üóëÔ∏è Dropped 'acct' from test_merged\n"
     ]
    }
   ],
   "source": [
    "# Save 'acct' from test_merged only\n",
    "acct_test = test_merged[['acct']].copy() if 'acct' in test_merged.columns else None\n",
    "\n",
    "# Drop 'acct' from both train and test\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    if 'acct' in df.columns:\n",
    "        df.drop(columns='acct', inplace=True)\n",
    "        print(f\"üóëÔ∏è Dropped 'acct' from {df_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b46d2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone: 1589 unique values in training set\n",
      "subneighborhood: 6161 unique values in training set\n",
      "neighborhood: 959 unique values in training set\n",
      "region: 87 unique values in training set\n"
     ]
    }
   ],
   "source": [
    "for col in ['zone', 'subneighborhood', 'neighborhood', 'region']:\n",
    "    if col in train_merged.columns:\n",
    "        print(f\"{col}: {train_merged[col].nunique()} unique values in training set\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {col} not found in training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39be55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frequency encoded: neighborhood ‚Üí neighborhood_freq (based on training set)\n",
      "‚úÖ Frequency encoded: region ‚Üí region_freq (based on training set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Frequency encoded: zone ‚Üí zone_freq (based on training set)\n",
      "‚úÖ Frequency encoded: subneighborhood ‚Üí subneighborhood_freq (based on training set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2858503920.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n"
     ]
    }
   ],
   "source": [
    "for col in ['neighborhood', 'region','zone','subneighborhood']:\n",
    "    if col in train_merged.columns:\n",
    "        # Step 1: Compute frequency from training data\n",
    "        freq_map = train_merged[col].value_counts(normalize=True)\n",
    "\n",
    "        # Step 2: Apply to both datasets\n",
    "        train_merged[f'{col}_freq'] = train_merged[col].map(freq_map)\n",
    "        test_merged[f'{col}_freq'] = test_merged[col].map(freq_map)\n",
    "\n",
    "        print(f\" Frequency encoded: {col} ‚Üí {col}_freq (based on training set)\")\n",
    "    else:\n",
    "        print(f\" Column '{col}' not found in training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Percentage of homes with land area change (2015‚Üí2019): 6.42%\n"
     ]
    }
   ],
   "source": [
    "if all(col in train_merged.columns for col in ['land_area_2019', 'land_area_2015']):\n",
    "    growth = train_merged['land_area_2019'] - train_merged['land_area_2015']\n",
    "    changed_pct = (growth != 0).mean() * 100\n",
    "    print(f\" Percentage of homes with land area change (2015‚Üí2019): {changed_pct:.2f}%\")\n",
    "else:\n",
    "    print(\" One or both columns ('land_area_2015', 'land_area_2019') not found in training set\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57919971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Converted 'year_built_final' to string in train_merged\n",
      "üî§ Converted 'year_built_final' to string in test_merged\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    if 'year_built_final' in df.columns:\n",
    "        df['year_built_final'] = df['year_built_final'].astype(str)\n",
    "        print(f\" Converted 'year_built_final' to string in {df_name}\")\n",
    "    else:\n",
    "        print(f\" 'year_built_final' not found in {df_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6711f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Required columns missing in train_merged\n",
      "‚ö†Ô∏è Required columns missing in test_merged\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    if 'floor_area_total_final' in df.columns and 'year_built_final' in df.columns:\n",
    "        df.loc[df['floor_area_total_final'] == 0, 'year_built_final'] = 'None'\n",
    "        print(f\" Set 'year_built_final' to 'None' where 'floor_area_total_final' == 0 in {df_name}\")\n",
    "    else:\n",
    "        print(f\" Required columns missing in {df_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee69535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imputed 'building_value_2015' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_value_2016' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_value_2017' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_value_2018' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_value_2015' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_value_2016' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_value_2017' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_value_2018' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_area_2015' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_area_2016' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_area_2017' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_area_2018' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'building_area_2019' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_area_2015' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_area_2016' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_area_2017' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_area_2018' using group medians (neighborhood ‚Üí region) from training data\n",
      "‚úÖ Imputed 'land_area_2019' using group medians (neighborhood ‚Üí region) from training data\n"
     ]
    }
   ],
   "source": [
    "# Define year ranges per feature type\n",
    "value_years = range(2015, 2019)   # 2015‚Äì2018 for value columns\n",
    "area_years  = range(2015, 2020)   # 2015‚Äì2019 for area columns\n",
    "\n",
    "# Define base columns\n",
    "base_cols_year_map = {\n",
    "    'building_value': value_years,\n",
    "    'land_value': value_years,\n",
    "    'building_area': area_years,\n",
    "    'land_area': area_years,\n",
    "}\n",
    "\n",
    "# Generate full list of columns to impute\n",
    "cols_to_impute = []\n",
    "for base, years in base_cols_year_map.items():\n",
    "    for year in years:\n",
    "        cols_to_impute.append(f\"{base}_{year}\")\n",
    "\n",
    "# Impute using neighborhood ‚Üí region strategy\n",
    "for col in cols_to_impute:\n",
    "    # First, fill by neighborhood (train only)\n",
    "    if 'neighborhood' in train_merged.columns:\n",
    "        medians_by_neigh = train_merged.groupby('neighborhood')[col].median()\n",
    "        train_merged[col] = train_merged.apply(\n",
    "            lambda row: medians_by_neigh[row['neighborhood']] if pd.isna(row[col]) else row[col], axis=1)\n",
    "        test_merged[col] = test_merged.apply(\n",
    "            lambda row: medians_by_neigh.get(row['neighborhood'], np.nan) if pd.isna(row[col]) else row[col], axis=1)\n",
    "\n",
    "    # Then, fill remaining by region (train only)\n",
    "    if 'region' in train_merged.columns:\n",
    "        medians_by_region = train_merged.groupby('region')[col].median()\n",
    "        train_merged[col] = train_merged.apply(\n",
    "            lambda row: medians_by_region[row['region']] if pd.isna(row[col]) else row[col], axis=1)\n",
    "        test_merged[col] = test_merged.apply(\n",
    "            lambda row: medians_by_region.get(row['region'], np.nan) if pd.isna(row[col]) else row[col], axis=1)\n",
    "\n",
    "    print(f\" Imputed '{col}' using group medians (neighborhood ‚Üí region) from training data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774e582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Coerced 'year_built_final' to numeric in train_merged\n",
      "üî¢ Coerced 'year_built_final' to numeric in test_merged\n",
      "‚úÖ Imputed 'year_built_final' using neighborhood ‚Üí region medians from training set\n"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# Step 1: Coerce non-numeric to NaN in both sets\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    if 'year_built_final' in df.columns:\n",
    "        df['year_built_final'] = pd.to_numeric(df['year_built_final'], errors='coerce')\n",
    "        print(f\" Coerced 'year_built_final' to numeric in {df_name}\")\n",
    "\n",
    "# Step 2: Group-based imputation (use training data only)\n",
    "if 'year_built_final' in train_merged.columns and 'neighborhood' in train_merged.columns:\n",
    "    # Neighborhood-based median from train\n",
    "    neigh_medians = train_merged.groupby('neighborhood')['year_built_final'].median()\n",
    "\n",
    "    # Apply to train\n",
    "    train_merged['year_built_final'] = train_merged.apply(\n",
    "        lambda row: neigh_medians[row['neighborhood']] if pd.isna(row['year_built_final']) else row['year_built_final'], axis=1)\n",
    "\n",
    "    # Apply to test\n",
    "    test_merged['year_built_final'] = test_merged.apply(\n",
    "        lambda row: neigh_medians.get(row['neighborhood'], np.nan) if pd.isna(row['year_built_final']) else row['year_built_final'], axis=1)\n",
    "\n",
    "if 'region' in train_merged.columns:\n",
    "    # Region-based fallback median from train\n",
    "    region_medians = train_merged.groupby('region')['year_built_final'].median()\n",
    "\n",
    "    train_merged['year_built_final'] = train_merged.apply(\n",
    "        lambda row: region_medians[row['region']] if pd.isna(row['year_built_final']) else row['year_built_final'], axis=1)\n",
    "\n",
    "    test_merged['year_built_final'] = test_merged.apply(\n",
    "        lambda row: region_medians.get(row['region'], np.nan) if pd.isna(row['year_built_final']) else row['year_built_final'], axis=1)\n",
    "\n",
    "print(\" Imputed 'year_built_final' using neighborhood ‚Üí region medians from training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed53d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/3523755006.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_merged[col].fillna(global_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imputed 'assessed_2015' using neighborhood ‚Üí region ‚Üí global medians (from training data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/3523755006.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_merged[col].fillna(global_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imputed 'assessed_2016' using neighborhood ‚Üí region ‚Üí global medians (from training data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/3523755006.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_merged[col].fillna(global_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imputed 'assessed_2017' using neighborhood ‚Üí region ‚Üí global medians (from training data)\n",
      "‚úÖ Imputed 'assessed_2018' using neighborhood ‚Üí region ‚Üí global medians (from training data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/3523755006.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_merged[col].fillna(global_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# List of all assessed columns to impute\n",
    "assessed_cols = ['assessed_2015', 'assessed_2016', 'assessed_2017', 'assessed_2018']\n",
    "\n",
    "for col in assessed_cols:\n",
    "    if col not in train_merged.columns:\n",
    "        continue\n",
    "\n",
    "    # Step 1: Compute medians from training data only\n",
    "    neigh_medians = train_merged.groupby('neighborhood')[col].median()\n",
    "    region_medians = train_merged.groupby('region')[col].median()\n",
    "    global_median = train_merged[col].median()\n",
    "\n",
    "    # Step 2: Train set imputation\n",
    "    train_merged[col] = train_merged.apply(\n",
    "        lambda row: neigh_medians[row['neighborhood']]\n",
    "        if pd.isna(row[col]) and row['neighborhood'] in neigh_medians else\n",
    "        region_medians[row['region']]\n",
    "        if pd.isna(row[col]) and row['region'] in region_medians else\n",
    "        global_median\n",
    "        if pd.isna(row[col]) else\n",
    "        row[col],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Step 3: Test set imputation (using train medians only)\n",
    "    test_merged[col] = test_merged.apply(\n",
    "        lambda row: neigh_medians.get(row['neighborhood'], np.nan)\n",
    "        if pd.isna(row[col]) else row[col],\n",
    "        axis=1\n",
    "    )\n",
    "    test_merged[col] = test_merged.apply(\n",
    "        lambda row: region_medians.get(row['region'], np.nan)\n",
    "        if pd.isna(row[col]) else row[col],\n",
    "        axis=1\n",
    "    )\n",
    "    test_merged[col].fillna(global_median, inplace=True)\n",
    "\n",
    "    print(f\" Imputed '{col}' using neighborhood ‚Üí region ‚Üí global medians (from training data)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bb99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed: Stats merge + std fallback + z-score computation.\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Compute neighborhood-level stats ===\n",
    "neigh_stats = train_merged.groupby('neighborhood')['assessed_2018'].agg([\n",
    "    ('neigh_assess_mean', 'mean'),\n",
    "    ('neigh_assess_median', 'median'),\n",
    "    ('neigh_assess_std', 'std'),\n",
    "    ('neigh_assess_q1', lambda x: x.quantile(0.25)),\n",
    "    ('neigh_assess_q3', lambda x: x.quantile(0.75)),\n",
    "]).reset_index()\n",
    "neigh_stats['neigh_assess_iqr'] = neigh_stats['neigh_assess_q3'] - neigh_stats['neigh_assess_q1']\n",
    "\n",
    "# === Step 2: Compute region-level stats ===\n",
    "region_stats = train_merged.groupby('region')['assessed_2018'].agg([\n",
    "    ('region_assess_mean', 'mean'),\n",
    "    ('region_assess_median', 'median'),\n",
    "    ('region_assess_std', 'std'),\n",
    "    ('region_assess_q1', lambda x: x.quantile(0.25)),\n",
    "    ('region_assess_q3', lambda x: x.quantile(0.75)),\n",
    "]).reset_index()\n",
    "region_stats['region_assess_iqr'] = region_stats['region_assess_q3'] - region_stats['region_assess_q1']\n",
    "\n",
    "# === Step 3: Fallback std maps from training data ===\n",
    "# For neighborhood fallback, group region medians of neighborhood std\n",
    "neigh_std_by_region = neigh_stats.merge(train_merged[['neighborhood', 'region']], on='neighborhood', how='left') \\\n",
    "                                  .groupby('region')['neigh_assess_std'].median()\n",
    "global_neigh_std = neigh_stats['neigh_assess_std'].median()\n",
    "\n",
    "region_std_by_neigh = region_stats.merge(train_merged[['neighborhood', 'region']], on='region', how='left') \\\n",
    "                                   .groupby('neighborhood')['region_assess_std'].median()\n",
    "global_region_std = region_stats['region_assess_std'].median()\n",
    "\n",
    "# === Step 4: Merge into train/test and compute features ===\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    df = df.merge(neigh_stats, on='neighborhood', how='left')\n",
    "    df = df.merge(region_stats, on='region', how='left')\n",
    "\n",
    "    # Fill missing std values via fallback\n",
    "    df['neigh_assess_std'] = df['neigh_assess_std'].fillna(\n",
    "        df['region'].map(neigh_std_by_region)\n",
    "    ).fillna(global_neigh_std)\n",
    "\n",
    "    df['region_assess_std'] = df['region_assess_std'].fillna(\n",
    "        df['neighborhood'].map(region_std_by_neigh)\n",
    "    ).fillna(global_region_std)\n",
    "\n",
    "    # Compute derived features\n",
    "    df['assess_minus_neigh_mean'] = df['assessed_2018'] - df['neigh_assess_mean']\n",
    "    df['assess_ratio_neigh_mean'] = df['assessed_2018'] / (df['neigh_assess_mean'] + 1e-6)\n",
    "    df['z_score_assess_neigh'] = df['assess_minus_neigh_mean'] / (df['neigh_assess_std'] + 1e-6)\n",
    "\n",
    "    df['assess_minus_region_mean'] = df['assessed_2018'] - df['region_assess_mean']\n",
    "    df['assess_ratio_region_mean'] = df['assessed_2018'] / (df['region_assess_mean'] + 1e-6)\n",
    "    df['z_score_assess_region'] = df['assess_minus_region_mean'] / (df['region_assess_std'] + 1e-6)\n",
    "\n",
    "    # Save back\n",
    "    if df_name == 'train_merged':\n",
    "        train_merged = df\n",
    "    else:\n",
    "        test_merged = df\n",
    "\n",
    "print(\" Completed: Stats merge + std fallback + z-score computation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42769f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped columns from train_merged: ['neighborhood', 'region', 'zone', 'subneighborhood']\n",
      "üóëÔ∏è Dropped columns from test_merged: ['neighborhood', 'region', 'zone', 'subneighborhood']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['neighborhood', 'region','zone','subneighborhood']\n",
    "\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    drop_cols = [col for col in cols_to_drop if col in df.columns]\n",
    "    if drop_cols:\n",
    "        df.drop(columns=drop_cols, inplace=True)\n",
    "        print(f\"üóëÔ∏è Dropped columns from {df_name}: {drop_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b449aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created building_value_growth in train_merged\n",
      "‚úÖ Created land_value_growth in train_merged\n",
      "‚úÖ Created assessed_growth in train_merged\n",
      "‚úÖ Created building_age in train_merged\n",
      "‚úÖ Created building_value_growth in test_merged\n",
      "‚úÖ Created land_value_growth in test_merged\n",
      "‚úÖ Created assessed_growth in test_merged\n",
      "‚úÖ Created building_age in test_merged\n"
     ]
    }
   ],
   "source": [
    "growth_pairs = {\n",
    "    'building_value_growth': ('building_value_2018', 'building_value_2015'),\n",
    "    'land_value_growth':     ('land_value_2018', 'land_value_2015'),\n",
    "    'assessed_growth':       ('assessed_2018', 'assessed_2015')\n",
    "}\n",
    "\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    for new_col, (final_col, base_col) in growth_pairs.items():\n",
    "        if final_col in df.columns and base_col in df.columns:\n",
    "            df[new_col] = df[final_col] - df[base_col]\n",
    "            print(f\" Created {new_col} in {df_name}\")\n",
    "        else:\n",
    "            print(f\" Skipped {new_col} in {df_name}: missing {final_col} or {base_col}\")\n",
    "\n",
    "    # === Building Age ===\n",
    "    if 'year_built_final' in df.columns:\n",
    "        df['year_built_final'] = pd.to_numeric(df['year_built_final'], errors='coerce')  # handle 'Vacant', etc.\n",
    "        df['building_age'] = 2018 - df['year_built_final']\n",
    "        print(f\" Created building_age in {df_name}\")\n",
    "    else:\n",
    "        print(f\" Skipped building_age in {df_name}: missing year_built_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç 'school_dist' has 3 missing values (0.00%) in training set\n"
     ]
    }
   ],
   "source": [
    "if 'school_dist' in train_merged.columns:\n",
    "    missing_pct = train_merged['school_dist'].isna().mean() * 100\n",
    "    missing_count = train_merged['school_dist'].isna().sum()\n",
    "\n",
    "    print(f\" 'school_dist' has {missing_count} missing values ({missing_pct:.2f}%) in training set\")\n",
    "else:\n",
    "    print(\" 'school_dist' not found in training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7945775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filled missing 'school_dist' in train_merged using train median (8.0)\n",
      "‚úÖ Filled missing 'school_dist' in test_merged using train median (8.0)\n"
     ]
    }
   ],
   "source": [
    "if 'school_dist' in train_merged.columns:\n",
    "    school_dist_median = train_merged['school_dist'].median()\n",
    "\n",
    "    for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "        if 'school_dist' in df.columns:\n",
    "            df['school_dist'] = df['school_dist'].fillna(school_dist_median)\n",
    "            print(f\" Filled missing 'school_dist' in {df_name} using train median ({school_dist_median})\")\n",
    "else:\n",
    "    print(\" 'school_dist' not found in train_merged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f776fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç 'year_built_final' has 0 missing values (0.00%) in training set\n"
     ]
    }
   ],
   "source": [
    "if 'year_built_final' in train_merged.columns:\n",
    "    missing_pct = train_merged['year_built_final'].isna().mean() * 100\n",
    "    missing_count = train_merged['year_built_final'].isna().sum()\n",
    "\n",
    "    print(f\" 'year_built_final' has {missing_count} missing values ({missing_pct:.2f}%) in training set\")\n",
    "else:\n",
    "    print(\" 'year_built_final' not found in training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filled structure-dependent missing values in train_merged for 218 rows\n",
      "‚úÖ Filled structure-dependent missing values in test_merged for 150 rows\n"
     ]
    }
   ],
   "source": [
    "# === STEP 0: Define base feature names ===\n",
    "numeric_bases = [\n",
    "    'garage_area', 'porch_area', 'floors', 'half_bath', 'full_bath',\n",
    "    'total_rooms', 'bedrooms', 'fireplaces', 'building_area', 'building_value'\n",
    "]\n",
    "\n",
    "categorical_fill_map = {\n",
    "    'quality': 'None',\n",
    "    'quality_description': 'None',\n",
    "    'building_condition': 'None',\n",
    "    'foundation_type': 'None',\n",
    "    'grade': 'None',\n",
    "    'has_cooling': False,\n",
    "    'has_heat': False,\n",
    "    'physical_condition': 'None',\n",
    "    'exterior_walls': 'None',\n",
    "    'protested': False\n",
    "}\n",
    "\n",
    "# Generate full list of columns (2015‚Äì2019 only, no final columns)\n",
    "numeric_cols_to_zero = [\n",
    "    f'{base}_{year}' for base in numeric_bases for year in range(2015, 2020)\n",
    "] + ['building_value_growth']\n",
    "\n",
    "categorical_cols_to_fill = {\n",
    "    f'{base}_{year}': val\n",
    "    for base, val in categorical_fill_map.items()\n",
    "    for year in range(2015, 2020)\n",
    "}\n",
    "\n",
    "# === STEP 1: Apply imputation if floor_area_total_2019 == 0 ===\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    if 'floor_area_total_2019' in df.columns:\n",
    "        zero_floor_mask = df['floor_area_total_2019'] == 0\n",
    "\n",
    "        # Fill numeric columns with 0\n",
    "        for col in numeric_cols_to_zero:\n",
    "            if col in df.columns:\n",
    "                df.loc[zero_floor_mask, col] = df.loc[zero_floor_mask, col].fillna(0)\n",
    "\n",
    "        # Fill categorical/boolean columns\n",
    "        for col, fill_val in categorical_cols_to_fill.items():\n",
    "            if col in df.columns:\n",
    "                df.loc[zero_floor_mask, col] = df.loc[zero_floor_mask, col].fillna(fill_val)\n",
    "\n",
    "        print(f\" Filled structure-dependent missing values in {df_name} for {zero_floor_mask.sum()} rows\")\n",
    "    else:\n",
    "        print(f\" 'floor_area_total_2019' not found in {df_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5f607f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/2484106990.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Clear specific variables\n",
    "for var in ['ordinal_cols_all', 'bool_cols_all']:\n",
    "    if var in locals():\n",
    "        del globals()[var]\n",
    "\n",
    "# === STEP 1: Boolean Encoding (2015‚Äì2019 only) ===\n",
    "bool_bases = ['has_cooling', 'has_heat', 'protested']\n",
    "bool_cols_all = [f\"{base}_{year}\" for base in bool_bases for year in range(2015, 2020)]\n",
    "\n",
    "for col in bool_cols_all:\n",
    "    if col in train_merged.columns:\n",
    "        mode_val = train_merged[col].mode(dropna=True)[0]\n",
    "        train_merged[col] = train_merged[col].fillna(mode_val).astype(int)\n",
    "        test_merged[col] = test_merged[col].fillna(mode_val).astype(int)\n",
    "\n",
    "# === STEP 2: Ordinal Cleaning and Encoding (2015‚Äì2019 only) ===\n",
    "ordinal_bases = [\n",
    "    'quality', 'quality_description', 'grade',\n",
    "    'building_condition', 'physical_condition'\n",
    "]\n",
    "\n",
    "ordinal_cols_all = [f\"{base}_{year}\" for base in ordinal_bases for year in range(2015, 2020)]\n",
    "\n",
    "# Column-specific replacements\n",
    "replacement_maps = {\n",
    "    'quality': {'E': 'D', 'F': 'D', 'X': np.nan, 'None': np.nan},\n",
    "    'quality_description': {'Poor': 'Very Low', 'None': np.nan},\n",
    "    'grade': {'X': 'F', 'X-': 'F', 'X+': 'F', 'E': 'D', 'E-': 'D-', 'E+': 'D+', 'None': np.nan},\n",
    "    'building_condition': {'Very Poor': 'Poor', 'Unsound': 'Poor', 'None': np.nan},\n",
    "    'physical_condition': {'Very Poor': 'Poor', 'Unsound': 'Poor', 'None': np.nan}\n",
    "}\n",
    "\n",
    "# Ordinal category order\n",
    "ord_categories = {\n",
    "    'quality': ['D', 'C', 'B', 'A'],\n",
    "    'quality_description': ['Very Low', 'Low', 'Average', 'Good', 'Excellent', 'Superior'],\n",
    "    'grade': ['F', 'D-', 'D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'],\n",
    "    'building_condition': ['Poor', 'Fair', 'Average', 'Good', 'Very Good', 'Excellent'],\n",
    "    'physical_condition': ['Poor', 'Fair', 'Average', 'Good', 'Very Good', 'Excellent']\n",
    "}\n",
    "\n",
    "# Clean and encode\n",
    "for base in ordinal_bases:\n",
    "    for year in range(2015, 2020):\n",
    "        col = f\"{base}_{year}\"\n",
    "        if col in train_merged.columns:\n",
    "            replacements = replacement_maps.get(base, {})\n",
    "            train_merged[col] = train_merged[col].replace(replacements)\n",
    "            test_merged[col] = test_merged[col].replace(replacements)\n",
    "\n",
    "            mode_val = train_merged[col].mode(dropna=True)[0]\n",
    "            train_merged[col] = train_merged[col].fillna(mode_val)\n",
    "            test_merged[col] = test_merged[col].fillna(mode_val)\n",
    "\n",
    "            encoder = OrdinalEncoder(categories=[ord_categories[base]], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "            train_merged[[col]] = encoder.fit_transform(train_merged[[col]])\n",
    "            test_merged[[col]] = encoder.transform(test_merged[[col]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: Boolean, Ordinal, and Target Encoding for 2015‚Äì2019 features only.\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: Target Encoding (2015‚Äì2019 only) ===\n",
    "def group_and_target_encode_cv(train_df, test_df, target_name, column, rare_threshold=0.001, smoothing=10, n_splits=5):\n",
    "    freq = train_df[column].value_counts(normalize=True)\n",
    "    rare_cats = freq[freq < rare_threshold].index\n",
    "    train_df[column] = train_df[column].replace(rare_cats, 'Other')\n",
    "    test_df[column] = test_df[column].replace(rare_cats, 'Other')\n",
    "\n",
    "    global_mean = train_df[target_name].mean()\n",
    "    oof_encoded = pd.Series(index=train_df.index, dtype='float64')\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in kf.split(train_df):\n",
    "        X_tr, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]\n",
    "        stats = X_tr.groupby(column)[target_name].agg(['mean', 'count'])\n",
    "        smooth = (stats['mean'] * stats['count'] + global_mean * smoothing) / (stats['count'] + smoothing)\n",
    "        oof_encoded.iloc[val_idx] = X_val[column].map(smooth).fillna(global_mean)\n",
    "\n",
    "    final_stats = train_df.groupby(column)[target_name].agg(['mean', 'count'])\n",
    "    final_smooth = (final_stats['mean'] * final_stats['count'] + global_mean * smoothing) / (final_stats['count'] + smoothing)\n",
    "    test_encoded = test_df[column].map(final_smooth).fillna(global_mean)\n",
    "\n",
    "    return oof_encoded, test_encoded\n",
    "\n",
    "# Target-encodable nominal columns\n",
    "target_encodable_bases = ['foundation_type', 'exterior_walls']\n",
    "target_encodable_cols_all = [f\"{base}_{year}\" for base in target_encodable_bases for year in range(2015, 2020)]\n",
    "\n",
    "# Apply target encoding\n",
    "for col in target_encodable_cols_all:\n",
    "    if col in train_merged.columns:\n",
    "        mode_val = train_merged[col].mode(dropna=True)[0]\n",
    "        train_merged[col] = train_merged[col].fillna(mode_val)\n",
    "        test_merged[col] = test_merged[col].fillna(mode_val)\n",
    "\n",
    "        train_merged[f'{col}_te'], test_merged[f'{col}_te'] = group_and_target_encode_cv(\n",
    "            train_merged, test_merged, target_name='assessed_2018', column=col,\n",
    "            rare_threshold=0.001, smoothing=10, n_splits=5\n",
    "        )\n",
    "\n",
    "        train_merged.drop(columns=[col], inplace=True)\n",
    "        test_merged.drop(columns=[col], inplace=True)\n",
    "\n",
    "print(\" Done: Boolean, Ordinal, and Target Encoding for 2015‚Äì2019 features only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filled and flagged missing values in train_merged for: []\n",
      "‚úÖ Filled and flagged missing values in test_merged for: []\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get growth columns from training set\n",
    "growth_cols = [col for col in train_merged.columns if '_growth' in col]\n",
    "\n",
    "# Step 2: Compute medians from train_merged only\n",
    "growth_medians = {col: train_merged[col].median() for col in growth_cols}\n",
    "\n",
    "# Step 3: Apply to both train and test\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    for col in growth_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_missing'] = df[col].isna().astype(int)\n",
    "            df[col].fillna(growth_medians[col], inplace=True)\n",
    "    print(f\" Filled and flagged missing values in {df_name} for: {growth_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d65ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Binned growth & year_built features safely with no leakage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Step 1: List your growth features ===\n",
    "growth_features = ['land_value_growth', 'building_value_growth', 'assessed_growth']\n",
    "\n",
    "# === Step 2: Binning Function (train-based binning) ===\n",
    "def bin_growth_feature_safe(train_df, test_df, feature, bins=4):\n",
    "    try:\n",
    "        # Quantile binning on train only\n",
    "        train_df[f'{feature}_bin'], bin_edges = pd.qcut(train_df[feature], q=bins, labels=False, retbins=True, duplicates='drop')\n",
    "        test_df[f'{feature}_bin'] = pd.cut(test_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n",
    "    except ValueError:\n",
    "        # Fallback: Equal-width binning\n",
    "        min_val = train_df[feature].min()\n",
    "        max_val = train_df[feature].max()\n",
    "        bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        train_df[f'{feature}_bin'] = pd.cut(train_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n",
    "        test_df[f'{feature}_bin'] = pd.cut(test_df[feature], bins=bin_edges, labels=False, include_lowest=True)\n",
    "\n",
    "    # Convert to category\n",
    "    train_df[f'{feature}_bin'] = train_df[f'{feature}_bin'].astype('category')\n",
    "    test_df[f'{feature}_bin'] = test_df[f'{feature}_bin'].astype('category')\n",
    "    return train_df, test_df\n",
    "\n",
    "# === Step 3: Apply to train_merged and test_merged ===\n",
    "for feature in growth_features:\n",
    "    train_merged, test_merged = bin_growth_feature_safe(train_merged, test_merged, feature)\n",
    "\n",
    "# === Step 4: Bin year_built_final using train-based quantiles ===\n",
    "train_merged['year_built_bin'], bin_edges = pd.qcut(\n",
    "    train_merged['year_built_final'], q=5, retbins=True, labels=False, duplicates='drop'\n",
    ")\n",
    "test_merged['year_built_bin'] = pd.cut(\n",
    "    test_merged['year_built_final'], bins=bin_edges, labels=False, include_lowest=True\n",
    ")\n",
    "\n",
    "# Convert to category\n",
    "train_merged['year_built_bin'] = train_merged['year_built_bin'].astype('category')\n",
    "test_merged['year_built_bin'] = test_merged['year_built_bin'].astype('category')\n",
    "\n",
    "# === Step 5: Drop original continuous columns ===\n",
    "cols_to_drop = growth_features + ['year_built_final']\n",
    "train_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "test_merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(\" Binned growth & year_built features safely with no leakage.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e6a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Column 'quality_description_final' not found in train_merged\n",
      "‚ö†Ô∏è Column 'foundation_type_final_te' not found in train_merged\n",
      "‚ö†Ô∏è Column 'physical_condition_final' not found in train_merged\n",
      "‚ö†Ô∏è Column 'exterior_walls_final_te' not found in train_merged\n",
      "\n",
      "‚ö†Ô∏è Rare categories in 'region_freq' (less than 0.1% of training data):\n",
      "region_freq\n",
      "0.000963    0.000963\n",
      "0.000641    0.000641\n",
      "0.000347    0.000347\n",
      "0.000159    0.000159\n",
      "0.000083    0.000083\n",
      "0.000010    0.000019\n",
      "0.000002    0.000019\n",
      "0.000008    0.000016\n",
      "0.000005    0.000005\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "‚ö†Ô∏è Rare categories in 'neighborhood_freq' (less than 0.1% of training data):\n",
      "neighborhood_freq\n",
      "0.000500    0.001000\n",
      "0.000996    0.000996\n",
      "0.000988    0.000988\n",
      "0.000984    0.000984\n",
      "0.000492    0.000984\n",
      "              ...   \n",
      "0.000008    0.000024\n",
      "0.000021    0.000021\n",
      "0.000014    0.000014\n",
      "0.000002    0.000006\n",
      "0.000003    0.000006\n",
      "Name: proportion, Length: 321, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rare_threshold = 0.001  # 0.1%\n",
    "\n",
    "cat_cols = [\n",
    "    \"quality_description_final\",\n",
    "    \"foundation_type_final_te\",\n",
    "    \"physical_condition_final\",\n",
    "    \"exterior_walls_final_te\",\n",
    "    \"region_freq\",\n",
    "    \"neighborhood_freq\"\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in train_merged.columns:\n",
    "        freq = train_merged[col].value_counts(normalize=True)\n",
    "        rare = freq[freq < rare_threshold]\n",
    "        if not rare.empty:\n",
    "            print(f\"\\n Rare categories in '{col}' (less than 0.1% of training data):\\n{rare}\")\n",
    "    else:\n",
    "        print(f\" Column '{col}' not found in train_merged\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76763d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Replaced rare values in region_freq using train_merged threshold < 0.001\n",
      "‚úÖ Replaced rare values in neighborhood_freq using train_merged threshold < 0.001\n",
      "‚úÖ Replaced rare values in zone_freq using train_merged threshold < 0.001\n",
      "‚úÖ Replaced rare values in subneighborhood_freq using train_merged threshold < 0.001\n"
     ]
    }
   ],
   "source": [
    "# Define frequency columns and threshold\n",
    "freq_cols = ['region_freq', 'neighborhood_freq', 'zone_freq', 'subneighborhood_freq']\n",
    "rare_thresh = 0.001\n",
    "\n",
    "# Apply rare value replacement for each frequency column\n",
    "for col in freq_cols:\n",
    "    if col in train_merged.columns:\n",
    "        rare_vals = train_merged[col].value_counts(normalize=True)[lambda x: x < rare_thresh].index\n",
    "        train_merged[col] = train_merged[col].replace(rare_vals, 0)\n",
    "        test_merged[col] = test_merged[col].replace(rare_vals, 0)\n",
    "        print(f\" Replaced rare values in {col} using train_merged threshold < {rare_thresh}\")\n",
    "    else:\n",
    "        print(f\" Column {col} not found in train_merged ‚Äî skipping.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Log-transformed: floor_area_total_2015 (skew=2.22)\n",
      "üìâ Log-transformed: floor_area_total_2016 (skew=2.37)\n",
      "üìâ Log-transformed: floor_area_total_2017 (skew=2.62)\n",
      "üìâ Log-transformed: floor_area_total_2018 (skew=2.83)\n",
      "üìâ Log-transformed: floor_area_total_2019 (skew=3.06)\n",
      "üìâ Log-transformed: porch_area_2015 (skew=4.95)\n",
      "üìâ Log-transformed: porch_area_2016 (skew=4.87)\n",
      "üìâ Log-transformed: porch_area_2017 (skew=4.84)\n",
      "üìâ Log-transformed: porch_area_2018 (skew=4.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Log-transformed: porch_area_2019 (skew=4.67)\n",
      "üìâ Log-transformed: building_area_2015 (skew=2.75)\n",
      "üìâ Log-transformed: building_area_2016 (skew=2.76)\n",
      "üìâ Log-transformed: building_area_2017 (skew=2.82)\n",
      "üìâ Log-transformed: building_area_2018 (skew=2.81)\n",
      "üìâ Log-transformed: building_area_2019 (skew=2.83)\n",
      "üìâ Log-transformed: land_area_2015 (skew=656.66)\n",
      "üìâ Log-transformed: land_area_2016 (skew=560.38)\n",
      "üìâ Log-transformed: land_area_2017 (skew=792.64)\n",
      "üìâ Log-transformed: land_area_2018 (skew=721.01)\n",
      "üìâ Log-transformed: land_area_2019 (skew=501.85)\n",
      "üìâ Log-transformed: building_value_2015 (skew=7.90)\n",
      "üìâ Log-transformed: building_value_2016 (skew=7.79)\n",
      "üìâ Log-transformed: building_value_2017 (skew=7.58)\n",
      "üìâ Log-transformed: building_value_2018 (skew=7.61)\n",
      "üìâ Log-transformed: land_value_2015 (skew=16.10)\n",
      "üìâ Log-transformed: land_value_2016 (skew=15.40)\n",
      "üìâ Log-transformed: land_value_2017 (skew=15.71)\n",
      "üìâ Log-transformed: land_value_2018 (skew=15.18)\n",
      "üìâ Log-transformed: assessed_2015 (skew=10.40)\n",
      "üìâ Log-transformed: assessed_2016 (skew=10.45)\n",
      "üìâ Log-transformed: assessed_2017 (skew=10.48)\n",
      "üìâ Log-transformed: assessed_2018 (skew=10.47)\n",
      "üìâ Log-transformed: neigh_assess_mean (skew=5.73)\n",
      "üìâ Log-transformed: neigh_assess_std (skew=8.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n",
      "/var/folders/sl/hc9dmfps60l7lq3bh_fp8ny40000gn/T/ipykernel_32068/4250956199.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"log_{col}\"] = np.log1p(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Log-transformed: neigh_assess_median (skew=5.25)\n",
      "üìâ Log-transformed: neigh_assess_q1 (skew=4.89)\n",
      "üìâ Log-transformed: neigh_assess_q3 (skew=5.84)\n",
      "‚úÇÔ∏è Clipped: floors_2015 to [1.00, 4.00]\n",
      "‚úÇÔ∏è Clipped: floors_2016 to [1.00, 4.00]\n",
      "‚úÇÔ∏è Clipped: floors_2017 to [1.00, 4.00]\n",
      "‚úÇÔ∏è Clipped: floors_2018 to [1.00, 4.00]\n",
      "‚úÇÔ∏è Clipped: floors_2019 to [1.00, 4.00]\n",
      "‚úÇÔ∏è Clipped: full_bath_2015 to [1.00, 6.00]\n",
      "‚úÇÔ∏è Clipped: full_bath_2016 to [1.00, 6.00]\n",
      "‚úÇÔ∏è Clipped: full_bath_2017 to [1.00, 6.00]\n",
      "‚úÇÔ∏è Clipped: full_bath_2018 to [1.00, 7.00]\n",
      "‚úÇÔ∏è Clipped: full_bath_2019 to [1.00, 7.00]\n",
      "‚úÇÔ∏è Clipped: total_rooms_2015 to [2.00, 17.00]\n",
      "‚úÇÔ∏è Clipped: total_rooms_2016 to [2.00, 17.00]\n",
      "‚úÇÔ∏è Clipped: total_rooms_2017 to [2.00, 17.00]\n",
      "‚úÇÔ∏è Clipped: total_rooms_2018 to [3.00, 17.00]\n",
      "‚úÇÔ∏è Clipped: total_rooms_2019 to [3.00, 17.00]\n",
      "‚úÇÔ∏è Clipped: bedrooms_2015 to [1.00, 8.00]\n",
      "‚úÇÔ∏è Clipped: bedrooms_2016 to [1.00, 8.00]\n",
      "‚úÇÔ∏è Clipped: bedrooms_2017 to [1.00, 8.00]\n",
      "‚úÇÔ∏è Clipped: bedrooms_2018 to [1.00, 8.00]\n",
      "‚úÇÔ∏è Clipped: bedrooms_2019 to [1.00, 8.00]\n",
      "\n",
      "‚úÖ Finished: Skew-aware log transformation + 0.001‚Äì0.999 percentile clipping.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === Step 1: Skew-based Log Transformation (2015‚Äì2019 only) ===\n",
    "log_bases = [\n",
    "    'floor_area_total', 'porch_area', 'building_area', 'land_area',\n",
    "    'building_value', 'land_value', 'assessed'\n",
    "]\n",
    "neigh_stat_cols = [\n",
    "    'neigh_assess_mean', 'neigh_assess_std', 'neigh_assess_median',\n",
    "    'neigh_assess_q1', 'neigh_assess_q3'\n",
    "]\n",
    "\n",
    "# Collect log-transformable columns (2015‚Äì2019 + neighborhood stats)\n",
    "log_transform_cols = [f\"{base}_{year}\" for base in log_bases for year in range(2015, 2020)]\n",
    "log_transform_cols += neigh_stat_cols\n",
    "\n",
    "# Compute skewness on train and apply log1p only if skew > 2\n",
    "for col in log_transform_cols:\n",
    "    if col in train_merged.columns:\n",
    "        skew = train_merged[col].skew()\n",
    "        if skew > 2:\n",
    "            for df in [train_merged, test_merged]:\n",
    "                df[f\"log_{col}\"] = np.log1p(df[col])\n",
    "            print(f\" Log-transformed: {col} (skew={skew:.2f})\")\n",
    "        else:\n",
    "            print(f\"‚Ñπ Skipped: {col} (skew={skew:.2f})\")\n",
    "\n",
    "# === Step 2: Percentile Clipping at 0.1%‚Äì99.9% ===\n",
    "clip_bases = ['floors', 'full_bath', 'total_rooms', 'bedrooms']\n",
    "clip_cols = [f\"{base}_{year}\" for base in clip_bases for year in range(2015, 2020)]\n",
    "clip_cols += ['neigh_count']  # add any other specific columns if needed\n",
    "\n",
    "# Compute clipping bounds from train\n",
    "clip_bounds = {\n",
    "    col: (\n",
    "        train_merged[col].quantile(0.001),\n",
    "        train_merged[col].quantile(0.999)\n",
    "    )\n",
    "    for col in clip_cols if col in train_merged.columns\n",
    "}\n",
    "\n",
    "# Apply clipping to both train and test\n",
    "for col, (lower, upper) in clip_bounds.items():\n",
    "    for df in [train_merged, test_merged]:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.clip(df[col], lower, upper)\n",
    "    print(f\" Clipped: {col} to [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "print(\"\\n Finished: Skew-aware log transformation + 0.001‚Äì0.999 percentile clipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5b0ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Ratio features ===\n",
    "    df['area_ratio'] = df['building_area_2019'] / (df['land_area_2019'] + 1)\n",
    "    df['porch_ratio'] = df['porch_area_2019'] / (df['building_area_2019'] + 1)\n",
    "    df['floor_density'] = df['floor_area_total_2019'] / (df['land_area_2019'] + 1)\n",
    "    df['log_build_density'] = df['log_building_area_2019'] - df['log_land_area_2019']\n",
    "    df['log_land_to_build_ratio'] = df['log_land_area_2019'] - df['log_building_area_2019']\n",
    "\n",
    "    df['value_ratio'] = df['building_value_2018'] / (df['land_value_2018'] + 1)\n",
    "    df['log_value_diff'] = df['log_building_value_2018'] - df['log_land_value_2018']\n",
    "    df['value_per_sqft'] = df['building_value_2018'] / (df['building_area_2019'] + 1)\n",
    "    df['price_per_sqft'] = df['assessed_2018'] / (df['building_area_2019'] + 1)\n",
    "\n",
    "    # === Bathroom & room structure ===\n",
    "    df['bathroom_score'] = df['full_bath_2019'] + 0.5 * df['half_bath_2019']\n",
    "    df['bathroom_density'] = df['bathroom_score'] / (df['total_rooms_2019'] + 1)\n",
    "    df['bedroom_ratio'] = df['bedrooms_2019'] / (df['total_rooms_2019'] + 1)\n",
    "    df['rooms_per_floor'] = df['total_rooms_2019'] / (df['floors_2019'] + 1)\n",
    "\n",
    "    # === Core interactions ===\n",
    "    df['bedrooms_x_floors'] = df['bedrooms_2019'] * df['floors_2019']\n",
    "    df['rooms_x_quality'] = df['total_rooms_2019'] * df['quality_2019']\n",
    "    df['log_area_x_grade'] = df['log_building_area_2019'] * df['grade_2019']\n",
    "    df['log_assess_x_age'] = df['log_assessed_2018'] * df['building_age']\n",
    "    df['assess_spread_neigh'] = df['log_neigh_assess_q3'] - df['log_neigh_assess_q1']\n",
    "    df['grade_quality_index'] = df['grade_2019'] * df['quality_2019']\n",
    "\n",
    "    # === Clean up ===\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "# === Apply to train and test ===\n",
    "train_merged = add_features(train_merged)\n",
    "test_merged = add_features(test_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Features with skewness > 2:\n",
      "land_area_2017             792.639441\n",
      "land_area_2018             721.010870\n",
      "land_area_2015             656.661667\n",
      "land_area_2016             560.378106\n",
      "value_ratio                502.848761\n",
      "                              ...    \n",
      "floor_area_primary_2018      2.664604\n",
      "floor_area_total_2017        2.621085\n",
      "floor_area_primary_2017      2.414200\n",
      "floor_area_total_2016        2.372924\n",
      "floor_area_total_2015        2.224773\n",
      "Length: 76, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select numeric columns only\n",
    "numeric_features = train_merged.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate skewness\n",
    "skewness = numeric_features.skew(numeric_only=True)\n",
    "\n",
    "# Filter and sort features with skewness > 30\n",
    "highly_skewed = skewness[skewness > 2].sort_values(ascending=False)\n",
    "\n",
    "print(\" Features with skewness > 2:\")\n",
    "print(highly_skewed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73623d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped extremely skewed columns related to value growth missingness.\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop due to extreme skewness\n",
    "cols_to_drop = ['building_value_growth_missing', 'land_value_growth_missing','assessed_growth_missing']\n",
    "\n",
    "# Drop from both train and test\n",
    "train_merged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "test_merged.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"Dropped extremely skewed columns related to value growth missingness.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46be7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Log-transformed and dropped raw versions of highly skewed ratio features.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Features to log-transform due to extreme skew\n",
    "skewed_cols = ['price_per_sqft', 'value_per_sqft', 'porch_ratio', 'value_ratio','land_area_2015','land_area_2016','land_area_2017','land_area_2018','land_area_2019']\n",
    "\n",
    "for df_name, df in [('train_merged', train_merged), ('test_merged', test_merged)]:\n",
    "    for col in skewed_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'log_{col}'] = np.log1p(df[col])\n",
    "            df.drop(columns=col, inplace=True)\n",
    "\n",
    "print(\"Log-transformed and dropped raw versions of highly skewed ratio features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "019c5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TARGET\" in train_merged.columns:\n",
    "    train_merged.drop(\"TARGET\", axis=1,inplace=True)\n",
    "else:\n",
    "    print(\"TARGET not found in columns:\", train_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7839d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped year_built_2015 to year_built_2018 from both train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Define columns to drop\n",
    "year_built_cols = [f'year_built_{year}' for year in range(2015, 2020)]\n",
    "\n",
    "# Drop if columns exist\n",
    "train_merged.drop(columns=[col for col in year_built_cols if col in train_merged.columns], inplace=True)\n",
    "test_merged.drop(columns=[col for col in year_built_cols if col in test_merged.columns], inplace=True)\n",
    "\n",
    "print(\"Dropped year_built_2015 to year_built_2018 from both train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550aa48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columns in train but not in test:\n",
      "[]\n",
      "\n",
      "‚ùå Columns in test but not in train:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "train_cols = set(train_merged.columns)\n",
    "test_cols = set(test_merged.columns)\n",
    "\n",
    "missing_in_test = train_cols - test_cols\n",
    "missing_in_train = test_cols - train_cols\n",
    "\n",
    "print(\" Columns in train but not in test:\")\n",
    "print(sorted(missing_in_test))\n",
    "\n",
    "print(\"\\n Columns in test but not in train:\")\n",
    "print(sorted(missing_in_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2348f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if column sets match\n",
    "print(set(train_merged.columns) == set(test_merged.columns))  # Should be True\n",
    "\n",
    "# Check if column order matches\n",
    "print(list(train_merged.columns) == list(test_merged.columns))  # Must also be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e466ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1/5\n",
      "üìâ Fold 1 RMSE: 42,050.33 | Best alpha: 2.1544\n",
      "\n",
      "üìÇ Fold 2/5\n",
      "üìâ Fold 2 RMSE: 41,036.52 | Best alpha: 27.8256\n",
      "\n",
      "üìÇ Fold 3/5\n",
      "üìâ Fold 3 RMSE: 40,619.40 | Best alpha: 0.5995\n",
      "\n",
      "üèÅ Final OOF RMSE (RidgeCV): 41,239.79\n",
      "üîç Average best alpha across folds: 10.1932\n",
      "\n",
      "üì§ Saved: submission_ridgecv_pipeline.csv\n",
      "üì¶ Saved: ridgecv_oof_preds.npy and ridgecv_test_preds.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# === STEP 2: Prepare training/test matrices ===\n",
    "X = train_merged.copy()\n",
    "X_test = test_merged.copy()\n",
    "y = pd.Series(y_train).values # use raw target (not log)\n",
    "\n",
    "# === STEP 3: RidgeCV pipeline ===\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "alphas = np.logspace(-3, 2, 10)\n",
    "\n",
    "ridge_oof = np.zeros(len(X))\n",
    "ridge_test_preds = np.zeros(len(X_test))\n",
    "best_alphas = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n Fold {fold+1}/5\")\n",
    "\n",
    "    X_train, y_train_fold = X.iloc[train_idx], y[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        RidgeCV(alphas=alphas, cv=3, scoring='neg_root_mean_squared_error')\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train_fold)\n",
    "    ridge_oof[val_idx] = model.predict(X_val)\n",
    "    ridge_test_preds += model.predict(X_test) / kf.get_n_splits()\n",
    "\n",
    "    best_alpha = model.named_steps['ridgecv'].alpha_\n",
    "    best_alphas.append(best_alpha)\n",
    "    \n",
    "    fold_rmse = root_mean_squared_error(y_val, ridge_oof[val_idx])\n",
    "    print(f\"Fold {fold+1} RMSE: {fold_rmse:,.2f} | Best alpha: {best_alpha:.4f}\")\n",
    "\n",
    "# === STEP 4: Final RMSE ===\n",
    "final_rmse = root_mean_squared_error(y, ridge_oof)\n",
    "print(f\"\\n Final OOF RMSE (RidgeCV): {final_rmse:,.2f}\")\n",
    "print(f\" Average best alpha across folds: {np.mean(best_alphas):.4f}\")\n",
    "\n",
    "# === STEP 5: Save predictions ===\n",
    "submission = pd.DataFrame({\n",
    "    \"ACCOUNT\": acct_test.values.ravel(),\n",
    "    \"TARGET\": ridge_test_preds\n",
    "})\n",
    "submission.to_csv(\"submission_ridgecv_pipeline.csv\", index=False)\n",
    "print(\"\\n Saved: submission_ridgecv_pipeline.csv\")\n",
    "\n",
    "# === Optional: Save OOF & test preds for stacking or analysis ===\n",
    "np.save(\"ridgecv_oof_preds.npy\", ridge_oof)\n",
    "np.save(\"ridgecv_test_preds.npy\", ridge_test_preds)\n",
    "print(\" Saved: ridgecv_oof_preds.npy and ridgecv_test_preds.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
